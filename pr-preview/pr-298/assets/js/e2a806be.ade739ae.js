"use strict";(self.webpackChunkrts_docs=self.webpackChunkrts_docs||[]).push([["6261"],{1050(e,n,r){r.r(n),r.d(n,{metadata:()=>i,default:()=>d,frontMatter:()=>o,contentTitle:()=>s,toc:()=>l,assets:()=>a});var i=JSON.parse('{"id":"hpc/ml_ai_hpc/LLM inference/llm_inferenceoverview","title":"Overview","description":"This directory provides two primary pathways for deploying and running Large Language Models (LLMs) on the NYU Torch cluster: Hugging Face Transformers (for research/experimentation) and vLLM (for high-performance serving).","source":"@site/docs/hpc/08_ml_ai_hpc/08_LLM inference/01_llm_inferenceoverview.md","sourceDirName":"hpc/08_ml_ai_hpc/08_LLM inference","slug":"/hpc/ml_ai_hpc/LLM inference/llm_inferenceoverview","permalink":"/pr-preview/pr-298/docs/hpc/ml_ai_hpc/LLM inference/llm_inferenceoverview","draft":false,"unlisted":false,"editUrl":"https://github.com/NYU-RTS/rts-docs/blob/main/docs/hpc/08_ml_ai_hpc/08_LLM inference/01_llm_inferenceoverview.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"hpcSidebar","previous":{"title":"Fine tune LLMs on HPC","permalink":"/pr-preview/pr-298/docs/hpc/ml_ai_hpc/llm_fine_tuning"},"next":{"title":"Basic LLM Inference with Hugging Face transformers","permalink":"/pr-preview/pr-298/docs/hpc/ml_ai_hpc/LLM inference/run_hf_model"}}'),t=r(62615),c=r(30416);let o={},s="Overview",a={},l=[{value:"1. Basic Inference (Hugging Face)",id:"1-basic-inference-hugging-face",level:2},{value:"2. High-Performance Serving (vLLM)",id:"2-high-performance-serving-vllm",level:2}];function h(e){let n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",p:"p",strong:"strong",...(0,c.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"overview",children:"Overview"})}),"\n",(0,t.jsx)(n.p,{children:"This directory provides two primary pathways for deploying and running Large Language Models (LLMs) on the NYU Torch cluster: Hugging Face Transformers (for research/experimentation) and vLLM (for high-performance serving)."}),"\n",(0,t.jsx)(n.h2,{id:"1-basic-inference-hugging-face",children:"1. Basic Inference (Hugging Face)"}),"\n",(0,t.jsxs)(n.p,{children:["This method is ideal for feature extraction, embeddings, or small-scale batch processing. Persistence is achieved by setting up the environment within an ",(0,t.jsx)(n.code,{children:"ext3"})," overlay that can be mounted to an Apptainer container."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Workflow:"}),"\nEnvironment: Launch an Apptainer container with a read/write overlay.\nPersistence: Install ",(0,t.jsx)(n.code,{children:"conda"})," and libraries directly into ",(0,t.jsx)(n.code,{children:"/ext3"}),".\nExecution: Use AutoModel to load weights and perform a forward pass.\nKey File: ",(0,t.jsx)(n.code,{children:"huggingface.py"}),"\nIdeal for: Extracting ",(0,t.jsx)(n.code,{children:"last_hidden_state"})," (embeddings) or sentiment classification."]}),"\n",(0,t.jsx)(n.h2,{id:"2-high-performance-serving-vllm",children:"2. High-Performance Serving (vLLM)"}),"\n",(0,t.jsxs)(n.p,{children:["vLLM is the recommended tool for production-level throughput and low-latency inference. It utilizes ",(0,t.jsx)(n.code,{children:"PagedAttention"})," to manage memory efficiently. Please find our guide on ",(0,t.jsx)(n.a,{href:"/pr-preview/pr-298/docs/hpc/ml_ai_hpc/LLM%20inference/vLLM",children:"deploying LLMs with vLLM on Torch here"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why vLLM?"}),"\nSpeed: higher throughput than standard backends on Torch.\nCompatibility: Drop-in replacement for OpenAI API."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Deployment Options:"}),"\nOnline: Use ",(0,t.jsx)(n.code,{children:"vllm serve"})," to start an HTTP server accessible via curl or OpenAI clients.\nOffline: Use the LLM class within Python for processing large datasets without a server."]})]})}function d(e={}){let{wrapper:n}={...(0,c.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},30416(e,n,r){r.d(n,{R:()=>o,x:()=>s});var i=r(59471);let t={},c=i.createContext(t);function o(e){let n=i.useContext(c);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(c.Provider,{value:n},e.children)}}}]);