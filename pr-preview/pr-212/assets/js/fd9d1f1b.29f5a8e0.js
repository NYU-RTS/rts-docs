"use strict";(self.webpackChunkrts_docs_dev=self.webpackChunkrts_docs_dev||[]).push([["8819"],{23330:function(e,t,i){i.r(t),i.d(t,{metadata:()=>n,default:()=>p,frontMatter:()=>r,contentTitle:()=>l,toc:()=>c,assets:()=>a});var n=JSON.parse('{"id":"hpc/submitting_jobs/slurm_submitting_jobs","title":"Submitting Jobs on Torch","description":"If you are new to using HPC resources and would like to learn about the principles of using the SLURM scheduler for submitting batch jobs, please refer to this section. This section focuses on the specifics of the Torch cluster and assumes familiarity with the tutorial.","source":"@site/docs/hpc/05_submitting_jobs/01_slurm_submitting_jobs.md","sourceDirName":"hpc/05_submitting_jobs","slug":"/hpc/submitting_jobs/slurm_submitting_jobs","permalink":"/pr-preview/pr-212/docs/hpc/submitting_jobs/slurm_submitting_jobs","draft":false,"unlisted":false,"editUrl":"https://github.com/NYU-RTS/rts-docs/blob/main/docs/hpc/05_submitting_jobs/01_slurm_submitting_jobs.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"hpcSidebar","previous":{"title":"Working with Datasets","permalink":"/pr-preview/pr-212/docs/hpc/datasets/working_with_datasets"},"next":{"title":"Slurm: Command reference","permalink":"/pr-preview/pr-212/docs/hpc/submitting_jobs/slurm_main_commands"}}'),o=i(47259),s=i(55511);let r={},l="Submitting Jobs on Torch",a={},c=[{value:"Partition Management",id:"partition-management",level:2},{value:"Job Submission",id:"job-submission",level:2},{value:"New Slurm configuration introduces preemption partitions enabling:",id:"new-slurm-configuration-introduces-preemption-partitions-enabling",level:2},{value:"How to Enable Pre-Emption",id:"how-to-enable-pre-emption",level:2}];function u(e){let t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"submitting-jobs-on-torch",children:"Submitting Jobs on Torch"})}),"\n",(0,o.jsx)(t.admonition,{title:"Beginner tutorial available",type:"tip",children:(0,o.jsxs)(t.p,{children:["If you are new to using HPC resources and would like to learn about the principles of using the ",(0,o.jsx)(t.code,{children:"SLURM"})," scheduler for submitting batch jobs, please refer to ",(0,o.jsx)(t.a,{href:"/pr-preview/pr-212/docs/hpc/tutorial_intro_hpc/scheduler_fundamentals",children:"this section"}),". This section focuses on the specifics of the Torch cluster and assumes familiarity with the tutorial."]})}),"\n",(0,o.jsx)(t.h2,{id:"partition-management",children:"Partition Management"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:"Partitions control stakeholder resource access."}),"\n",(0,o.jsx)(t.li,{children:"No physical nodes are tied to partitions \u2014 instead, equivalent compute resources are allocated via partition QoS."}),"\n",(0,o.jsx)(t.li,{children:"Jobs within the same partition cannot exceed their assigned resources (QOSGrpGRES)."}),"\n",(0,o.jsx)(t.li,{children:"User GPU Quotas: Each user has a total GPU quota of 24 GPUs for jobs with wall time < 48 hours (QOSMaxGRESPerUser)."}),"\n"]}),"\n",(0,o.jsx)(t.h2,{id:"job-submission",children:"Job Submission"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:"Only request the compute resources (e.g., GPUs, CPUs, memory)."}),"\n",(0,o.jsx)(t.li,{children:"Scheduler will automatically dispatch jobs to all accessible GPU partitions that match resource requests."}),"\n"]}),"\n",(0,o.jsx)(t.admonition,{title:"Partitions",type:"tip",children:(0,o.jsx)(t.p,{children:"Do not specify partitions manually."})}),"\n",(0,o.jsx)(t.admonition,{title:"Low GPU Utilization Policy",type:"warning",children:(0,o.jsx)(t.p,{children:"Jobs with low GPU utilization will be automatically cancelled.\nThe exact threshold is TBD, but enforcement will be very aggressive."})}),"\n",(0,o.jsx)(t.h2,{id:"new-slurm-configuration-introduces-preemption-partitions-enabling",children:"New Slurm configuration introduces preemption partitions enabling:"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:"Non-stakeholders to temporarily use stakeholder resources"}),"\n",(0,o.jsx)(t.li,{children:"One stakeholder group to temporarily use another group\u2019s resources"}),"\n",(0,o.jsx)(t.li,{children:"Stakeholders retain normal access to their own resources."}),"\n",(0,o.jsx)(t.li,{children:"If non-stakeholders (or other stakeholders) are using them, their jobs may be preempted (cancelled) once stakeholders submit new jobs."}),"\n",(0,o.jsx)(t.li,{children:"Public users are allowed to use stakeholder resources only with preemption partitions."}),"\n"]}),"\n",(0,o.jsx)(t.admonition,{title:"[Pre-Emption Policy]",type:"info",children:(0,o.jsx)(t.p,{children:"Jobs become eligible for pre-emption after 1 hour of runtime.\nJobs will not be cancelled within the first hour."})}),"\n",(0,o.jsx)(t.h2,{id:"how-to-enable-pre-emption",children:"How to Enable Pre-Emption"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:'To allow jobs in both normal and pre-emption partitions:\n#SBATCH --comment="preemption=yes;requeue=true"\nJobs in stakeholder partitions will not be cancelled, but those in pre-emption partitions may be.\nCancelled jobs will be requeued automatically with requeue=true'}),"\n",(0,o.jsx)(t.li,{children:'To use only pre-emption partitions:\n#SBATCH --comment="preemption=yes;preemption_partitions_only=yes;requeue=true"\nJobs with preemption partitions only may be allowed to use more resources'}),"\n"]}),"\n",(0,o.jsx)(t.p,{children:"Pre-Emption Order\nStakeholder jobs have highest priority\n\u2192 Can pre-]empt GPU jobs from public users or other stakeholders\nGPU jobs can preempt CPU-only jobs running on GPU nodes\nPartition Assignment Order\nPI stakeholder partitions\nSchool stakeholder partitions\nIT public partitions\nPre-emption partitions\nApplies separately to both GPU types: L40S first, then H200"}),"\n",(0,o.jsx)(t.p,{children:'Enable GPU MPS\nUse GPU Multi-Process Service (MPS) to improve overall GPU utilization, allows multiple GPU jobs to share a single GPU concurrently\n#SBATCH --comment="gpu_mps=yes"\nMount Memory as Disk\nCreate a small RAM disk for fast I/O operations\n#SBATCH --comment="ram_disk=1GB"\nCombine Features\nExample with pre-emption, GPU MPS, and RAM disk enabled:\n#SBATCH --comment="preemption=yes;preemption_partitions_only=yes;requeue=true;gpu_mps=yes;ram_disk=1GB"'})]})}function p(e={}){let{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(u,{...e})}):u(e)}},55511:function(e,t,i){i.d(t,{R:()=>r,x:()=>l});var n=i(96363);let o={},s=n.createContext(o);function r(e){let t=n.useContext(s);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),n.createElement(s.Provider,{value:t},e.children)}}}]);