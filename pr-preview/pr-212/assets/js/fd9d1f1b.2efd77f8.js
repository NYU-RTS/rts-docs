"use strict";(self.webpackChunkrts_docs_dev=self.webpackChunkrts_docs_dev||[]).push([["8819"],{23330:function(e,t,s){s.r(t),s.d(t,{metadata:()=>i,default:()=>d,frontMatter:()=>n,contentTitle:()=>l,toc:()=>a,assets:()=>c});var i=JSON.parse('{"id":"hpc/submitting_jobs/slurm_submitting_jobs","title":"Submitting Jobs on Torch","description":"If you are new to using HPC resources and would like to learn about the principles of using the SLURM scheduler for submitting batch jobs, please refer to this section. This section focuses on the specifics of the Torch cluster and assumes familiarity with the tutorial.","source":"@site/docs/hpc/05_submitting_jobs/01_slurm_submitting_jobs.md","sourceDirName":"hpc/05_submitting_jobs","slug":"/hpc/submitting_jobs/slurm_submitting_jobs","permalink":"/pr-preview/pr-212/docs/hpc/submitting_jobs/slurm_submitting_jobs","draft":false,"unlisted":false,"editUrl":"https://github.com/NYU-RTS/rts-docs/blob/main/docs/hpc/05_submitting_jobs/01_slurm_submitting_jobs.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"hpcSidebar","previous":{"title":"Working with Datasets","permalink":"/pr-preview/pr-212/docs/hpc/datasets/working_with_datasets"},"next":{"title":"Slurm: Command reference","permalink":"/pr-preview/pr-212/docs/hpc/submitting_jobs/slurm_main_commands"}}'),o=s(47259),r=s(55511);let n={},l="Submitting Jobs on Torch",c={},a=[{value:"Partitions on Torch",id:"partitions-on-torch",level:2},{value:"Resource limits and restrictions",id:"resource-limits-and-restrictions",level:3},{value:"Job Submission on Torch",id:"job-submission-on-torch",level:2},{value:"Preemptible jobs on Torch",id:"preemptible-jobs-on-torch",level:2},{value:"Enable GPU MPS",id:"enable-gpu-mps",level:2}];function h(e){let t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"submitting-jobs-on-torch",children:"Submitting Jobs on Torch"})}),"\n",(0,o.jsx)(t.admonition,{title:"Beginner tutorial available",type:"tip",children:(0,o.jsxs)(t.p,{children:["If you are new to using HPC resources and would like to learn about the principles of using the ",(0,o.jsx)(t.code,{children:"SLURM"})," scheduler for submitting batch jobs, please refer to ",(0,o.jsx)(t.a,{href:"/pr-preview/pr-212/docs/hpc/tutorial_intro_hpc/scheduler_fundamentals",children:"this section"}),". This section focuses on the specifics of the Torch cluster and assumes familiarity with the tutorial."]})}),"\n",(0,o.jsx)(t.h2,{id:"partitions-on-torch",children:"Partitions on Torch"}),"\n",(0,o.jsxs)(t.p,{children:[(0,o.jsx)(t.code,{children:"SLURM"})," partitions on Torch control stakeholder resource access. No physical nodes are tied to partitions \u2014 instead, equivalent compute resources are allocated via partition ",(0,o.jsx)(t.code,{children:"QoS"}),(0,o.jsx)(t.a,{href:"https://slurm.schedmd.com/qos.html",children:"QualityOfService"}),"."]}),"\n",(0,o.jsx)(t.admonition,{title:"Partitions",type:"tip",children:(0,o.jsx)(t.p,{children:"Do not specify partitions manually, except for preemption which is described later."})}),"\n",(0,o.jsx)(t.h3,{id:"resource-limits-and-restrictions",children:"Resource limits and restrictions"}),"\n",(0,o.jsxs)(t.p,{children:["Jobs within the same partition cannot exceed their assigned resources (",(0,o.jsx)(t.code,{children:"QOSGrpGRES"}),"). User GPU Quotas: Each user has a total GPU quota of 24 GPUs for jobs with wall time < 48 hours (",(0,o.jsx)(t.code,{children:"QOSMaxGRESPerUser"}),")."]}),"\n",(0,o.jsx)(t.p,{children:"Non-stakeholders to temporarily use stakeholder resources (a stakeholder group to temporarily use another group\u2019s resources). Stakeholders retain normal access to their own resources. If non-stakeholders (or other stakeholders) are using them, their jobs may be preempted (canceled) once stakeholders submit new jobs. Public users are allowed to use stakeholder resources only with preemption partitions."}),"\n",(0,o.jsx)(t.h2,{id:"job-submission-on-torch",children:"Job Submission on Torch"}),"\n",(0,o.jsxs)(t.p,{children:["As stated in the tuturial, always only request the compute resources (e.g., GPUs, CPUs, memory). The ",(0,o.jsx)(t.code,{children:"SLURM"})," scheduler will automatically dispatch jobs to all accessible GPU partitions that match resource requests."]}),"\n",(0,o.jsx)(t.admonition,{title:"Low GPU Utilization Policy",type:"danger",children:(0,o.jsx)(t.p,{children:"Jobs with low GPU utilization will be automatically canceled. The exact threshold is TBD, but enforcement will be very aggressive."})}),"\n",(0,o.jsx)(t.h2,{id:"preemptible-jobs-on-torch",children:"Preemptible jobs on Torch"}),"\n",(0,o.jsx)(t.p,{children:'On Torch, users may run "preemptible" jobs on stakeholder resources that their group does not own. This allows the stakeholder resources to be utilized by non-stakeholders which may otherwise be idle. To make the best use of these resources, you are encouraged to adopt checkpoint/restart to allow for resumption of the workload in subsequent jobs.'}),"\n",(0,o.jsx)(t.admonition,{title:"Preemption Policy",type:"warning",children:(0,o.jsx)(t.p,{children:"Jobs become eligible for preemption after 1 hour of runtime. Jobs will not be canceled within the first hour."})}),"\n",(0,o.jsx)(t.p,{children:"The preemption order is:"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:"Stakeholder jobs (highest priority), these can preempt GPU jobs from public users or other stakeholders"}),"\n",(0,o.jsx)(t.li,{children:"GPU jobs can preempt CPU-only jobs running on GPU nodes"}),"\n",(0,o.jsx)(t.li,{children:"Partition Assignment Order"}),"\n",(0,o.jsx)(t.li,{children:"PI stakeholder partitions"}),"\n",(0,o.jsx)(t.li,{children:"School stakeholder partitions"}),"\n",(0,o.jsx)(t.li,{children:"IT public partitions"}),"\n",(0,o.jsx)(t.li,{children:"Preemption partitions\nApplies separately to both GPU types: L40S first, then H200"}),"\n"]}),"\n",(0,o.jsx)(t.p,{children:"To allow jobs in both normal and preemption partitions:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'#SBATCH --comment="preemption=yes;requeue=true"\n'})}),"\n",(0,o.jsxs)(t.p,{children:["Jobs in stakeholder partitions will not be canceled, but those in preemption partitions may be. Canceled jobs will be re-queued automatically with ",(0,o.jsx)(t.code,{children:"requeue=true"}),".  To use only preemption partitions:"]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'#SBATCH --comment="preemption=yes;preemption_partitions_only=yes;requeue=true"\n'})}),"\n",(0,o.jsx)(t.p,{children:"Jobs with preemption partitions only might be allowed to use more resources"}),"\n",(0,o.jsx)(t.h2,{id:"enable-gpu-mps",children:"Enable GPU MPS"}),"\n",(0,o.jsx)(t.p,{children:"Use GPU Multi-Process Service (MPS) to improve overall GPU utilization, as this allows multiple GPU jobs to share a single GPU concurrently by:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'#SBATCH --comment="gpu_mps=yes"\n'})}),"\n",(0,o.jsx)(t.p,{children:"Mount Memory as Disk\nCreate a small RAM disk for fast I/O operations"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'#SBATCH --comment="ram_disk=1GB"\n'})}),"\n",(0,o.jsx)(t.p,{children:"Example with preemption, GPU MPS, and RAM disk enabled:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'#SBATCH --comment="preemption=yes;preemption_partitions_only=yes;requeue=true;gpu_mps=yes;ram_disk=1GB"\n'})})]})}function d(e={}){let{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},55511:function(e,t,s){s.d(t,{R:()=>n,x:()=>l});var i=s(96363);let o={},r=i.createContext(o);function n(e){let t=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:n(e.components),i.createElement(r.Provider,{value:t},e.children)}}}]);