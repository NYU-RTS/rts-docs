{"searchDocs":[{"title":"o4-mini is now available!","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/blog/2025-05-02-o4-mini/","content":"We are pleased to announce that o4-mini is now available via Pythia. Here is an overview of the model: https://platform.openai.com/docs/models/o4-mini When compared to o3-mini, o4-mini adds support for images as inputs. It costs the same as o3-mini for regular usage, though the price is lower if you use cached inputs. Thus, this model will now be the default reasoning model we provide and the use of o3-mini is now deprecated.","keywords":"","version":null},{"title":"LLM Catalogue","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/genai/external_llms/catalogue/","content":"","keywords":"","version":"Next"},{"title":"OpenAI​","type":1,"pageTitle":"LLM Catalogue","url":"/rts-docs-dev/pr-preview/pr-115/docs/genai/external_llms/catalogue/#openai","content":" gpt-4o-minigpt-4oo4-minio3-mini (deprecated)text-embedding-3-small  ","version":"Next","tagName":"h2"},{"title":"VertexAI​","type":1,"pageTitle":"LLM Catalogue","url":"/rts-docs-dev/pr-preview/pr-115/docs/genai/external_llms/catalogue/#vertexai","content":" Gemini-2.5-flash-preview-04-17Gemini-2.0 models (flash, flash-lite)Gemini-1.5 models (flash, pro) (deprecated)  For a comprehensive list, please refer to the VertexAI documentation. ","version":"Next","tagName":"h2"},{"title":"Portkey","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/genai/external_llms/llm_access/","content":"","keywords":"","version":"Next"},{"title":"Onboarding​","type":1,"pageTitle":"Portkey","url":"/rts-docs-dev/pr-preview/pr-115/docs/genai/external_llms/llm_access/#onboarding","content":" Send an email to genai-support@nyu.edu to start the onboarding process.  ","version":"Next","tagName":"h2"},{"title":"Getting started with Portkey​","type":1,"pageTitle":"Portkey","url":"/rts-docs-dev/pr-preview/pr-115/docs/genai/external_llms/llm_access/#getting-started-with-portkey","content":" As part of the onboarding process, you would have received an invite which gives you access to a workspace. We will also add virtual keys for LLMs to your workspace as part of the onboarding process. Once you've accepted it, head over to https://app.portkey.ai/ and select the sign-in with Single Sign-On option and proceed with your NYU email address.  Access to Portkey is only permitted via NYU VPN You need to be connected to the NYU VPN to access the Portkey LLM gateway. If you are not, your requests will timeout and result in connection errors.  You will now be able to create an API key for yourself by access the API Keys item on the left sidebar. With an API key and a virtual key at your disposal, you can now run the following script:  from portkey_ai import Portkey portkey = Portkey( base_url=&quot;https://ai-gateway.apps.cloud.rt.nyu.edu/v1/&quot;, api_key=&quot;&quot;, # Replace with your Portkey API key virtual_key=&quot;&quot;, # Replace with your virtual key ) completion = portkey.chat.completions.create( messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are not a helpful assistant&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Say this is a test&quot;}, ], model=&quot;gemini-2.0-flash-lite&quot;, ) print(completion)   Once the script is executed, you can head back to app.portkey.ai to view the logs for the call! ","version":"Next","tagName":"h2"},{"title":"Research Technology Cloud","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/cloud/intro/","content":"Research Technology Cloud","keywords":"","version":"Next"},{"title":"OpenWebUI","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/genai/external_llms/playground/","content":"OpenWebUI We are working on providing an NYU hosted instance of OpenWebUI. More details about this will be provided soon.","keywords":"","version":"Next"},{"title":"Pythia","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/genai/getting_started/intro/","content":"Pythia Non-research workflows If you're looking to harness Generative AI for administrative or classroom use, please reach out to genai-support@nyu.edu Welcome to Pythia, the generative AI platform for research workflows. As part of the Pythia platform, the following capabilities are offered: Access to externally hosted LLMsHPC resources for fine tuning LLMsMilvus vector database Personal use If you want to access NYU provided LLMs for personal use, proceed to https://gemini.google.com/app with your NYU credentials.","keywords":"","version":"Next"},{"title":"Fine tuning","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/genai/llm_fine_tuning/intro/","content":"Fine tuning You can use HPC for this. But for most cases, RAG might suffice. Please look into harnessing RAG before attempting to fine-tune a model.","keywords":"","version":"Next"},{"title":"Temperature","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/genai/how_to_guides/temperature/","content":"Temperature Generating text (or images) from LLMs is inherently probabilistic. However, as an end user you have many parameters at your disposal to tweak the behavior of LLMs. Of these, temperature is the most commonly used. Broadly, it controls the randonmess of the generated text. A lower temperature produces more deterministic outputs, while a higher temperature produces more random &quot;creative&quot; output. For a more comprehensive explanation on this topic, refer to the following: How to generate text: using different decoding methods for language generation with TransformersWhat is LLM Temperature? tip The effect of temperature is probabalistic, so you might need to run the script repeatedly to obtain a representative sample of generated text from the LLM. Here's a script to test the effect of temperature: from portkey_ai import Portkey portkey = Portkey( base_url=&quot;https://ai-gateway.apps.cloud.rt.nyu.edu/v1/&quot;, api_key=&quot;&quot;, # Replace with your Portkey API key virtual_key=&quot;&quot;, # Replace with your virtual key ) completion = portkey.chat.completions.create( messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are not a helpful assistant&quot;}, { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Complete the following sentence:The sun is shining and the sky is&quot;, }, ], model=&quot;gemini-2.5-flash-preview-04-17&quot;, temperature=2.0, #tweak this parameter! ) print(completion) At the temperature of 2.0, you might get an output along along the lines of: &quot;listening to old radio static.&quot;&quot;... a really peculiar shade of chartreuse today.&quot;&quot;The sun is shining and the sky is, I assume, still above.&quot;&quot;...containing numerous molecules of gas.&quot; alongside the more common response of blue which is likely the only response you'd get a lower temperature (like 0.1). note Reasoning models do not support the temperature parameter. Instead, you should look into tweaking the thinking budget parameter for reasoning models.","keywords":"","version":"Next"},{"title":"Vector Databases","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/genai/vector_databases/intro/","content":"Vector Databases What is it? How is it different from a regular database?","keywords":"","version":"Next"},{"title":"Connecting to the HPC Cluster","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/connecting_to_hpc/","content":"","keywords":"","version":"Next"},{"title":"Remote Connections with the NYU VPN & HPC Gateway Server​","type":1,"pageTitle":"Connecting to the HPC Cluster","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/connecting_to_hpc/#remote-connections-with-the-nyu-vpn--hpc-gateway-server","content":" If you are connecting from a remote location that is not on the NYU network (your home for example), you have two options:  VPN Option: Set up your computer to use the NYU VPN. Once you've created a VPN connection, you can proceed as if you were connected to the NYU net Gateway Option: Go through our gateway servers (example below). Gateways are designed to support only a very minimal set of commands and their only purpose is to let users connect HPC systems without needing to first connect to the VPN  You do not need to use the NYU VPN or gateways if you are connected to the NYU network (wired connection in your office or WiFi) or if you have VPN connection initiated. In this case you can ssh directly to the clusters.  ","version":"Next","tagName":"h2"},{"title":"Command Line Interface (Use Terminal)​","type":1,"pageTitle":"Connecting to the HPC Cluster","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/connecting_to_hpc/#command-line-interface-use-terminal","content":" ","version":"Next","tagName":"h2"},{"title":"Mac & Linux Access​","type":1,"pageTitle":"Connecting to the HPC Cluster","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/connecting_to_hpc/#mac--linux-access","content":" To connect to the gateway servers, simply open a terminal application and enter the following command:  ssh &lt;NetID&gt;@gw.hpc.nyu.edu   After typing in your password you will be logged in to the cluster. Once this connection is established, you can make one more hop and connect to one of the HPC clusters:  # this will connect you to Greene HPC cluster ssh &lt;NetID&gt;@greene.hpc.nyu.edu   ","version":"Next","tagName":"h3"},{"title":"Windows CMD​","type":1,"pageTitle":"Connecting to the HPC Cluster","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/connecting_to_hpc/#windows-cmd","content":" Windows 11 users have several options. First, the CMD program should contain an ssh client, allowing you to log into Greene or Hudson the same way as with a Linux terminal.  ","version":"Next","tagName":"h3"},{"title":"Windows WSL2​","type":1,"pageTitle":"Connecting to the HPC Cluster","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/connecting_to_hpc/#windows-wsl2","content":" If you run Windows 10, you can install WSL, and then install Ubuntu or other Linux distribution (for example, from Microsoft Store). You will have a fully functional Ubuntu with terminal and can connect to cluster using instructions provided above for Linux/Mac users.  Instructions on WSL installation can be found here: https://docs.microsoft.com/en-us/windows/wsl/install-win10  tip One of many options to get terminal that support tabs, etc. is to install 'Windows Terminal' from Microsoft Store.If you are using WSL 2 (Windows subsystem for Linux), you may not be able to access internet when Cisco AnyConnect VPN, installed from exe file, is activated. A potential solution: uninstall Cisco AnyConnect and install AnyConnect using Microsoft Store, and then setup new VPN connection using settings described on IT webpage.  ","version":"Next","tagName":"h3"},{"title":"Setting up SSH Keys​","type":1,"pageTitle":"Connecting to the HPC Cluster","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/connecting_to_hpc/#setting-up-ssh-keys","content":" Instead of typing your password every time you need to log in, you can also specify an ssh key.  Only do that on the computer you trust Generate ssh key pair (terminal in Linux/Mac or cmd/WSL in Windows):https://www.ssh.com/ssh/keygen/ Note the path to ssh key files. Don't share key files with anybody - anybody with this key file can login to your account Log into cluster using regular login/password and then add the content of generated public key file (the one with .pub) to $HOME/.ssh/authorized_keys on cluster Next time you will log into cluster no password will be required  For additional recommendations on how to configure your SSH sessions, see the [ssh configuring and x11 forwarding page].  ","version":"Next","tagName":"h3"},{"title":"PuTTY (Only for Windows)​","type":1,"pageTitle":"Connecting to the HPC Cluster","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/connecting_to_hpc/#putty-only-for-windows","content":" There are many SSH clients for Windows OS, but we recommend using PuTTY SSH if you have not already. Once it is installed, launch PuTTY and configure new session &quot;Session&quot; category as in the screenshot below:    Here we are instructing PuTTY to connect to host gw.hpc.nyu.edu on port 22 using SSH protocol (note, that this interface allows you to save this connection configuration for future). Just like for Linux and Mac users, if you are connecting from the outside of NYU network, you need to go through the gateway servers.  Once you click &quot;Open&quot;, a terminal window with prompt for password will pop up. Enter your NetID password and you should be authorized on the gateway server. Gateways are designed to support only a very minimal set of commands and their only purpose it to let users access HPC systems. Once you are there type in an ssh command that will let you connect to Greene cluster :  # Greene Login ssh greene.hpc.nyu.edu   A new command line interface window will open up that prompts you for your password on the gateway server, from there you can connect to Greene by entering the following:  ssh greene.hpc.nyu.edu  ","version":"Next","tagName":"h2"},{"title":"Open OnDemand (Web-based Graphical User Interface)","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/ood/","content":"","keywords":"","version":"Next"},{"title":"Access the Shell​","type":1,"pageTitle":"Open OnDemand (Web-based Graphical User Interface)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/ood/#access-the-shell","content":" Under the clusters menu you can select the Greene Shell Access option to access the Linux shell. No local SSH client is required.    Interactive Applications  GUI based applications are accessible without the need for port or X11 forwarding. Select the Interactive Apps menu, select the desired application, and submit the job based on required resources and options.    ","version":"Next","tagName":"h2"},{"title":"Troubleshooting Connections to Open OnDemand​","type":1,"pageTitle":"Open OnDemand (Web-based Graphical User Interface)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/ood/#troubleshooting-connections-to-open-ondemand","content":" A common issue that can occur is receiving an error that the Open OnDemand page cannot be reached. Sometimes this can indicate that the service is down, but often this is an issue with the the local browser cache. You can test this by opening a private browser window and seeing if https://ood.hpc.nyu.edu will load. If it does, try deleting the cache for https://ood.hpc.nyu.edu in your browser history to resolve this issue.  In Chrome, this can be done by navigating to this page in your settings:  chrome://settings/content/all?searchSubpage=ood.hpc.nyu.edu&amp;search   The link above will automatically search for the Open OnDemand site data and cookies. You can then simply click on the trashcan icon to delete the site cache.    Once done, try navigating again to https://ood.hpc.nyu.edu and the site should load. For other issues please email hpc@nyu.edu. ","version":"Next","tagName":"h2"},{"title":"Using Containers on HPC","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/containers/containers/","content":"Using Containers on HPC","keywords":"","version":"Next"},{"title":"Apptainer/Singularity","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/containers/intro/","content":"Apptainer/Singularity","keywords":"","version":"Next"},{"title":"SSH Tunneling and X11 Forwarding","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/","content":"","keywords":"","version":"Next"},{"title":"Avoiding Man in the Middle Warning.​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#avoiding-man-in-the-middle-warning","content":" If you see this warning:  warning @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed.   Do not be alarmed - this is an issue that occurs because the cluster has multiple login nodes (log-1, log-2, and log-3) that greene.hpc.nyu.edu resolves to.  To avoid this warning, you can add these lines to your SSH configuration file. Open ~/.ssh/config and place the following lines in it:  tip Host greene.hpc.nyu.edu dtn.hpc.nyu.edu gw.hpc.nyu.edu StrictHostKeyChecking no ServerAliveInterval 60 ForwardAgent yes StrictHostKeyChecking no UserKnownHostsFile /dev/null LogLevel ERROR   The above will also fix SSH timeout errors by extending the ServerAliveInterval argument.  ","version":"Next","tagName":"h2"},{"title":"SSH Tunneling (Mac, Linux)​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#ssh-tunneling-mac-linux","content":" Setting up your workstation for SSH tunneling will make logging in and transferring files significantly easier, and installing and running an X server will allow you to use graphical software on the HPC clusters. X server is a software package that draws on your local screen windows created on a remote computer such as on the remote HPC.  Linux users have X set up already. Mac users can download and install XQuartz.  ","version":"Next","tagName":"h2"},{"title":"Set up a reusable tunnel​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#set-up-a-reusable-tunnel","content":" To avoid repeatedly setting up a tunnel, you can write the details of the tunnel into your SSH configuration file. Using your favorite editor, open the file ~/.ssh/config and place the following lines in it:  # first we create the tunnel, with instructions to pass incoming # packets on ports 8027 and 8028 through it and to specific locations Host hpcgwtunnel HostName gw.hpc.nyu.edu ForwardX11 no StrictHostKeyChecking no LocalForward 8027 greene.hpc.nyu.edu:22 UserKnownHostsFile /dev/null User &lt;Your NetID&gt; # next we create an alias for incoming packets on the port # The alias corresponds to where the tunnel forwards these packets Host greene HostName localhost Port 8027 ForwardX11 yes StrictHostKeyChecking no UserKnownHostsFile /dev/null LogLevel ERROR User &lt;Your NetID&gt;   Create this file/directory In case you don't have it. Make sure that &quot;.ssh&quot; directory has correct permissions (it should be &quot;700&quot; or &quot;drwx------&quot;). If needed, set permissions with:  chmod 700 ~/.ssh   You may also need to setup permissions on your local computer:  chmod 700 $HOME chmod 700 $HOME/.ssh ## to be safe, all files inside ~/.ssh should be set 600 chmod 600 ~/.ssh/*   ","version":"Next","tagName":"h3"},{"title":"Start the tunnel​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#start-the-tunnel","content":" To create the tunnel, ssh to it with the following command:  ssh hpcgwtunnel   tip You must leave this window open for the tunnel to remain open. It is best to start a new terminal window for subsequent logins.  ","version":"Next","tagName":"h3"},{"title":"Log in via the tunnel​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#log-in-via-the-tunnel","content":" Open a new terminal window and use ssh to log in to the cluster, as shown below.  ssh greene   Note that you must use the short name defined above in your .ssh/config file, not the fully qualified domain name:  Creating a once-off tunnel.  Alternatively, you can set up a once-off tunnel without editing .ssh/config by running the following command:  ssh -L 8027:greene:22 NetID@gw.hpc.nyu.edu # to set up a tunnel ssh -Y -p 8027 NetID@localhost   This is the equivalent to running &quot;ssh hpcgwtunnel&quot; in the reusable tunnel instructions, but the port forwarding is specified on the command line.  ","version":"Next","tagName":"h3"},{"title":"Tunneling (Windows)​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#tunneling-windows","content":" ","version":"Next","tagName":"h2"},{"title":"Creating the tunnel​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#creating-the-tunnel","content":" First open Putty and prepare to log in to gw.hpc.nyu.edu. If you saved your session during that process, you can load it by selecting from the &quot;Saved Sessions&quot; box and hitting &quot;Load&quot;. Don't hit &quot;Open&quot; yet! Under &quot;Connection&quot; -&gt; &quot;SSH&quot;, just below &quot;X11&quot;, select &quot;Tunnels Write &quot;8026&quot; (the port number) in the &quot;Source port&quot; box, and &quot;greene.hpc.nyu.edu:22&quot; (the machine you wish to tunnel to - 22 is the port that ssh listens on) in the &quot;Destination&quot; box Click &quot;Add&quot;. You can repeat step 3 with a different port number and a different destination. If you do this you will create multiple tunnels, one to each destination Before hitting &quot;Open&quot;, go back to the &quot;Sessions&quot; page, give the session a name (&quot;hpcgw_tunnel&quot;) and hit &quot;Save&quot;. Then next time you need not do all this again, just load the saved session Hit &quot;Open&quot; to login in to gw.hpc.nyu.edu and create the tunnel. A terminal window will appear, asking for your login name (NYU NetID) and password (NYU password). Windows may also ask you to allow certain connections through its firewall - this is so you can ssh to port 8026 on your workstation - the entrance to the tunnel  note You can add other NYU hosts to the tunnel by adding a new source port and destination and clicking &quot;Add&quot;. For example, you could add &quot;Source port = 8025&quot; and &quot;Destination = EXAMPLE.hpc.nyu.edu:22&quot;, then press &quot;Add&quot;. You would then perform Step 2 (below) twice - once for greene on port 8026 and once for an example server on port 8025.  Using your SSH tunnel: To log in via the tunnel, first the tunnel must be open. If you've just completed Step 1, it will be open and you can jump down to &quot;Step 2: Logging in via your SSH tunnel&quot;. If you completed Step 1 yesterday, and now want to re-use the tunnel you created, first start the tunnel:  Starting the tunnel: During a session, you need only do this once - as long as the tunnel is open, new connections will go over it.  Start Putty.exe (again, if necessary), and load the session you saved in settings during procedure above Hit &quot;Open&quot;, and log in to the bastion host with your NYU NetID and password. This will create the tunnel.  ","version":"Next","tagName":"h3"},{"title":"Logging in via your SSH tunnel​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#logging-in-via-your-ssh-tunnel","content":" Start the second Putty.exe. In the &quot;Host Name&quot; box, write &quot;localhost&quot; and in the &quot;Port&quot; box, write &quot;8026&quot; (or whichever port number you specified when you set up the tunnel in the procedure above). We use &quot;localhost&quot; because the entrance of the tunnel is actually on this workstation, at port 8026 Go to &quot;Connections&quot; -&gt; &quot;SSH&quot; -&gt; &quot;X11&quot; and check &quot;Enable X11 forwarding&quot; Optionally, give this session a name (in &quot;Saved Sessions&quot;) and hit &quot;Save&quot; to save it. Then next time instead of steps 1 and 2 you can simply load this saved session Hit &quot;Open&quot;. You will again get a terminal window asking for your login (NYU NetID) and password (NYU password). You are now logged in to the HPC cluster!  ","version":"Next","tagName":"h3"},{"title":"X11 Forwarding​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#x11-forwarding","content":" In rare cases when you need to interact with GUI applications on HPC clusters, you need to enable X11 forwarding for your SSH connection. Mac and Linux users will need to run the ssh commands described above with an additional flag:  ssh -Y &lt;NYU_NetID&gt;@greene.hpc.nyu.edu   However, Mac users need to install XQuartz, since X-server is no longer shipped with the macOS.  Windows users will also need to install X server software. We recommend two options out there. We recommend installing Xming. Start Xming application and configure PuTTY to support X11 forwarding: ","version":"Next","tagName":"h2"},{"title":"Squash File System and Singularity","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/containers/squash_file_system_and_singularity/","content":"","keywords":"","version":"Next"},{"title":"Working with Datasets​","type":1,"pageTitle":"Squash File System and Singularity","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/containers/squash_file_system_and_singularity/#working-with-datasets","content":" Writable ext3 overlay images have conda environments installed inside, Singularity can work with squashFS for fixed datasets, such as the coco datasets.  /scratch/work/public/ml-datasets/coco/coco-2014.sqf /scratch/work/public/ml-datasets/coco/coco-2015.sqf /scratch/work/public/ml-datasets/coco/coco-2017.sqf singularity exec \\ --overlay /scratch/wang/zzz/pytorch1.8.0-cuda11.1.ext3:ro \\ --overlay /scratch/work/public/ml-datasets/coco/coco-2014.sqf:ro \\ --overlay /scratch/work/public/ml-datasets/coco/coco-2015.sqf:ro \\ --overlay /scratch/work/public/ml-datasets/coco/coco-2017.sqf:ro \\ /scratch/work/public/singularity/cuda11.1-cudnn8-devel-ubuntu18.04.sif /bin/bash   If you have many tiny files as fixed datasets, please make squashFS files to work with Singularity. Here is an example  Make a temporary folder in /state/partition1, it is a folder in local hard drive on each computer node  mkdir -p /state/partition1/sw77 cd /state/partition1/sw77   Unzip files there, for example  tar -vxzf /scratch/work/public/examples/squashfs/imagenet-example.tar.gz   Change access permissions in case we'll share files with others  find imagenet-example -type d -exec chmod 755 {} \\; find imagenet-example -type f -exec chmod 644 {} \\;   Convert to a single squashFS file on host  mksquashfs imagenet-example imagenet-example.sqf -keep-as-directory   For more details on working with squashFS, please see this page from the SquashFS documentation.  Copy this file to /scratch  cp -rp /state/partition1/sw77/imagenet-example.sqf /scratch/sw77/.   To test, files are in /imagenet-example inside Singularity container  singularity exec --overlay /scratch/sw77/imagenet-example.sqf:ro /scratch/work/public/singularity/ubuntu-20.04.1.sif /bin/bash Singularity&gt; find /imagenet-example | wc -l 1303 Singularity&gt; find /state/partition1/sw77/imagenet-example | wc -l 1303   To delete the tempoary folder on host  rm -rf /state/partition1/sw77  ","version":"Next","tagName":"h2"},{"title":"Working with Datasets","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/datasets/working_with_datasets/","content":"Working with Datasets Please see the Squash File System and Singularity page in the Containers section for details about working with datasets on the clusters.","keywords":"","version":"Next"},{"title":"Datasets Available","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/datasets/intro/","content":"","keywords":"","version":"Next"},{"title":"General​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/datasets/intro/#general","content":" The HPC team makes available a number of public sets that are commonly used in analysis jobs. The data sets are available Read-Only under  /scratch/work/public/ml-datasets//vast/work/public/ml-datasets/  We recommend to use version stored at /vast (when available) to have better read performance  note For some of the datasets users must provide a signed usage agreement before accessing  ","version":"Next","tagName":"h2"},{"title":"Format​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/datasets/intro/#format","content":" Many datasets are available in the form of '.sqf' file, which can be used with Singularity. For example, in order to use coco dataset, one can run the following commands  $ singularity exec \\ --overlay /&lt;path&gt;/pytorch1.8.0-cuda11.1.ext3:ro \\ --overlay /vast/work/public/ml-datasets/coco/coco-2014.sqf:ro \\ --overlay /vast/work/public/ml-datasets/coco/coco-2015.sqf:ro \\ --overlay /vast/work/public/ml-datasets/coco/coco-2017.sqf:ro \\ /scratch/work/public/singularity/cuda11.1-cudnn8-devel-ubuntu18.04.sif /bin/bash $ singularity exec \\ --overlay /&lt;path&gt;/pytorch1.8.0-cuda11.1.ext3:ro \\ --overlay /vast/work/public/ml-datasets/coco/coco-2014.sqf:ro \\ --overlay /vast/work/public/ml-datasets/coco/coco-2015.sqf:ro \\ --overlay /vast/work/public/ml-datasets/coco/coco-2017.sqf:ro \\ /scratch/work/public/singularity/cuda11.1-cudnn8-devel-ubuntu18.04.sif find /coco | wc -l 532896   ","version":"Next","tagName":"h2"},{"title":"Data Sets​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/datasets/intro/#data-sets","content":" ","version":"Next","tagName":"h2"},{"title":"COCO Dataset​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/datasets/intro/#coco-dataset","content":" About data set: https://cocodataset.org/  Common Objects in Context (COCO) is a large-scale object detection, segmentation, and captioning dataset.  Dataset is available under/scratch  /scratch/work/public/ml-datasets/coco/coco-2014.sqf/scratch/work/public/ml-datasets/coco/coco-2015.sqf/scratch/work/public/ml-datasets/coco/coco-2017.sqf  /vast  /vast/work/public/ml-datasets/coco/coco-2014.sqf/vast/work/public/ml-datasets/coco/coco-2015.sqf/vast/work/public/ml-datasets/coco/coco-2017.sqf  ","version":"Next","tagName":"h3"},{"title":"ImageNet and ILSVRC​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/datasets/intro/#imagenet-and-ilsvrc","content":" About data set: ImageNet (image-net.org)  ImageNet is an image dataset organized according to the WordNet hierarchy (Miller, 1995). Each concept in WordNet, possibly described by multiple words or word phrases, is called a “synonym set” or “synset”. ImageNet populates 21,841 synsets of WordNet with an average of 650 manually verified and full resolution images. As a result, ImageNet contains 14,197,122 annotated images organized by the semantic hierarchy of WordNet (as of August 2014). ImageNet is larger in scale and diversity than the other image classification datasets (https://arxiv.org/abs/1409.0575).  note WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept (https://wordnet.princeton.edu/)  ILSVRC (subset of ImageNet)​  ILSVRC uses a subset of ImageNet images for training the algorithms and some of ImageNet’s image collection protocols for annotating additional images for testing the algorithms (https://arxiv.org/abs/1409.0575). The name comes from 'ImageNet Large Scale Visual Recognition Challenge (ILSVRC)'. Competition was moved to Kaggle (http://image-net.org/challenges/LSVRC/2017/)  What is included (https://arxiv.org/abs/1409.0575).  1000 object classesapproximately 1.2 million training images50 thousand validation images100 thousand test imagesSize of data is about 150 GB (for train and validation)  Dataset is available under  /scratch/work/public/ml-datasets/imagenet/vast/work/public/ml-datasets/imagenet  Get access to Data​  New York University does not own this dataset.  Please open the ImageNet site, find the terms of use (http://image-net.org/download), copy them, replace the needed parts with your name, send us an email including the terms with your name - thereby confirming you agree to the these terms. Once you do this, we can grant you access to the copy of the dataset on the cluster.  ","version":"Next","tagName":"h3"},{"title":"Millions Songs​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/datasets/intro/#millions-songs","content":" About data set: https://labrosa.ee.columbia.edu/millionsong/  Dataset is available under  /scratch/work/public/MillionSongDataset/vast/work/public/ml-datasets/millionsongdataset/  ","version":"Next","tagName":"h3"},{"title":"Twitter Decahose​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/datasets/intro/#twitter-decahose","content":" About data set: https://developer.twitter.com/en/docs/twitter-api/enterprise/decahose-api/overview/decahose  NYU has a subscription to Twitter Decahose - 10% random sample of the realtime Twitter Firehose through a streaming connection  Data are stored in GCP cloud (BigQuery) and on HPC clusters Greene and Peel (Parquet format).  Please contact Megan Brown at The Center for Social Media &amp; Politics to get access to data and learn the tools available to work with it.  On cluster dataset is available under (given that you have permissions)  /scratch/work/twitter_decahose/  ","version":"Next","tagName":"h3"},{"title":"ProQuest Congressional Record​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/datasets/intro/#proquest-congressional-record","content":" About data set: ProQuest Congressional Record  The ProQuest Congressional Record text-as-data collection consists of machine-readable files capturing the full text and a small number of metadata fields for a full run of the Congressional Record between 1789 and 2005. Metadata fields include the date of publication, subjects (for issues for which such information exists in the ProQuest system), and URLs linking the full text to the canonical online record for that issue on the ProQuest Congressional platform. A total of 31,952 issues are available.  Dataset is available under:  /scratch/work/public/proquest/  ","version":"Next","tagName":"h3"},{"title":"C4​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/datasets/intro/#c4","content":" About data set: c4 | TensorFlow Datasets  A colossal, cleaned version of Common Crawl's web crawl corpus. Based on Common Crawl dataset: https://commoncrawl.org  Dataset is available under  /scratch/work/public/ml-datasets/c4/vast/work/public/ml-datasets/c4  ","version":"Next","tagName":"h3"},{"title":"GQA​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/datasets/intro/#gqa","content":" About data set: GQA: Visual Reasoning in the Real World (stanford.edu)  Question Answering on Image Scene Graphs  Dataset is available under  /scratch/work/public/ml-datasets/gqa/vast/work/public/ml-datasets/gqa  ","version":"Next","tagName":"h3"},{"title":"MJSynth​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/datasets/intro/#mjsynth","content":" About data set: Visual Geometry Group - University of Oxford  This is synthetically generated dataset which found to be sufficient for training text recognition on real-world images  This dataset consists of 9 million images covering 90k English words, and includes the training, validation and test splits used in the author's work (archived dataset is about 10 GB)  Dataset is available under  /vast/work/public/ml-datasets/mjsynth  ","version":"Next","tagName":"h3"},{"title":"open-images-dataset​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/datasets/intro/#open-images-dataset","content":" About data set: Open Images Dataset – opensource.google  A dataset of ~9 million varied images with rich annotations  The images are very diverse and often contain complex scenes with several objects (8.4 per image on average). It contains image-level labels annotations, object bounding boxes, object segmentations, visual relationships, localized narratives, and more  Dataset is available under  /scratch/work/public/ml-datasets/open-images-dataset/vast/work/public/ml-datasets/open-images-dataset  ","version":"Next","tagName":"h3"},{"title":"Pile​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/datasets/intro/#pile","content":" About data set: The Pile (eleuther.ai)  The Pile is a 825 GiB diverse, open source language modeling data set that consists of 22 smaller, high-quality datasets combined together.  Dataset is available under  /scratch/work/public/ml-datasets/pile/vast/work/public/ml-datasets/pile  ","version":"Next","tagName":"h3"},{"title":"Waymo open dataset​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/datasets/intro/#waymo-open-dataset","content":" About data set: Open Dataset – Waymo  The field of machine learning is changing rapidly. Waymo is in a unique position to contribute to the research community with some of the largest and most diverse autonomous driving datasets ever released.  Dataset is available under  /vast/work/public/ml-datasets/waymo_open_dataset_scene_flow/vast/work/public/ml-datasets/waymo_open_dataset_v_1_2_0_individual_files/vast/work/public/ml-datasets/waymo_open_dataset_v_1_3_2_individual_files/vast/work/public/ml-datasets/waymo_open_dataset_v_1_4_1_individual_files ","version":"Next","tagName":"h3"},{"title":"How to submit an Allocation Request","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/getting_started/coldfront_requesting_an_allocation_request/","content":"How to submit an Allocation Request If you would ike to raise an allocation request, please first login to coldfront page and you'll see &quot;Project&quot; navigation bar. Click the &quot;Project&quot; navigation bar and click &quot;Projects&quot; to choose which project you would like to raise the allocation request for. Now, you'll see a list of projects and if you click a project, you'll see this project detail page. If you scroll down a bit, you'll see &quot;+Request Resource Allocation&quot; button. After clicking &quot;+Request Resource Allocation&quot;, you'll see a list of resources you can request for. Basically, you'll see general &quot;Univeristy HPC&quot; which is default for all and school-wise resources (e.g., Tandon(Genric)) based on a school that your project belongs to. Please select a resource and fill in justification to complete the allocation reqeust process. Now your allocation request is created! You'll see an allocation request with &quot;New&quot; status. If you have any difficulties or questions, please contact us at hpc@nyu.edu.","keywords":"","version":"Next"},{"title":"How to approve an Allocation Request","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/getting_started/approving_an_allocation_request/","content":"How to approve an Allocation Request If you are an approver for your school, first login to coldfront page and you'll see &quot;staff&quot; navigation bar. Click the &quot;staff&quot; navigation bar and click &quot;Allocation Requests&quot; to see a list of requests waiting for your approval. Now, you'll see a list of allocation requests. Click &quot;Details&quot; button for the request that you would like to approve. You should click &quot;Details&quot; instead of just &quot;Approve&quot; button if you'd like to set more details. After clicking &quot;Details&quot;, now you'll see Allocation information part to fill in with your approval. Here, you could set start date, end date, description, etc. If you're done with adding the information, you could click &quot;Update&quot; button to update the allocation request with the information you put. After clicking &quot;Update&quot;, the allocation request is approved, and the allocation is activated now! Slurm account name is also available on the allocation attributes if you need it. If you have any difficulties or questions, please contact us at hpc@nyu.edu.","keywords":"","version":"Next"},{"title":"HPC Accounts for Sponsored External Collaborators","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/getting_started/hpc_accounts_external_collaborators/","content":"HPC Accounts for Sponsored External Collaborators External (non-NYU) collaborators can access, with proper sponsorship, the NYU HPC Environment. The first step is to sponsor a collaborator for an NYU netid (if they do not have one already). A department administrator or the faculty sponsor must submit the Affiliate Management Form (the link is only accessible over VPN, or within NYU-Net).Once a netid for the external collaborator is created, the collaborator must submit the Request for an NYU HPC account. tip The collaborator must setup VPN access to be able to access the HPC account request form.The collaborator must enter in the account request form the Netid of the sponsoring NYU Full time facultyThe collaborator should select &quot;External Collaborator&quot; as Affiliation, when filing the HPC account request form. Once the HPC request is submitted, the sponsoring faculty will receive an email with a link to approve (or deny) the HPC account request for the external collaborator. The account approval link can only be accessed over VPN. note Once the sponsoring faculty approves the account request, the HPC account is created within one hour. Once the HPC account is created, the external collaborator can access HPC resources as described here. note As with all sponsored accounts, HPC accounts for external collaborators are valid for a period of 12 months, at which point a renewal process is required to continue access to the NYU HPC environment.","keywords":"","version":"Next"},{"title":"Start here!","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/getting_started/intro/","content":"Start here! Welcome to the Torch HPC documentation! If you do not have an HPC account, please proceed to the next section that explains how you may be able to get one. If you are an active user, you can proceed to one of the categories on the left.","keywords":"","version":"Next"},{"title":"How to approve an HPC Account Request","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/getting_started/walkthrough_approve_hpc_account_request/","content":"How to approve an HPC Account Request When someone nominates you as their HPC sponsor, you should be notified by email. You can also log into IIQ at any time, and if you have a request awaiting your approval, it will appear in your &quot;Actions Items&quot; box, as per the following screenshot: Another way to get to pending approvals is to click on the line item in the “Latest Approvals” section which will lead directly to the approval page. For new HPC Account Requests, the page will look like this: Here, the Approve or Deny button should be clicked, then confirmed, in order to complete the request. If you have any difficulties or questions, please contact us at hpc@nyu.edu.","keywords":"","version":"Next"},{"title":"Getting and Renewing an Account","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/getting_started/getting_and_renewing_an_account/","content":"","keywords":"","version":"Next"},{"title":"Who is eligible for an NYU HPC account ?​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/getting_started/getting_and_renewing_an_account/#who-is-eligible-for-an-nyu-hpc-account-","content":" NYU HPC clusters and related resources are available to full-time NYU faculty and to all NYU staff and, students with sponsorship from a full-time NYU faculty.  ","version":"Next","tagName":"h2"},{"title":"Getting a new account on the NYU HPC clusters​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/getting_started/getting_and_renewing_an_account/#getting-a-new-account-on-the-nyu-hpc-clusters","content":" To request an NYU HPC account please log in to NYU Identity Management service and follow the link to &quot;Request HPC account&quot;. We have a walkthrough of how to [request an account through IIQ]. If you are a student, alumni or an external collaborator you need an NYU faculty sponsor.  ","version":"Next","tagName":"h2"},{"title":"Renewing HPC account​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/getting_started/getting_and_renewing_an_account/#renewing-hpc-account","content":" Each year, non-faculty users must renew their HPC account by filling in the account renewal form from the NYU Identity Management service. See Renewing your HPC account with IIQ for a walkthrough of the process.  ","version":"Next","tagName":"h2"},{"title":"Information for faculty who sponsor HPC users​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/getting_started/getting_and_renewing_an_account/#information-for-faculty-who-sponsor-hpc-users","content":" All full-time NYU faculty members (other than NYU Med School) are eligible to become sponsors and in turn can sponsor:  NYU Degree program students Scholars visiting NYU NYU Research staff NYU School of Medicine faculty, staff and students Other NYU staff/affiliates with a NetID Non-NYU researchers with whom they are actively collaborating  If you need to sponsor an HPC account for an external collaborator (for example, for an NYU alumnus), please, request a &quot;research affiliate&quot; affiliation for your collaborator. You can find the instructions at https://start.nyu.edu/.  You can request a NetID for your student(s) or collaborator(s) at https://start.nyu.edu/pwm/public/. The request form has additional information about affiliates.  HPC faculty sponsors are expected to:  Approve/disapprove sponsored users' association with you Approve/disapprove the purpose for which user is requesting an account on NYU HPC resources Agree to supervise the sponsored individual, to the extent necessary, to ensure proper use of the NYU HPC resource and compliance with all applicable policies Respond promptly to account-related requests from HPC staff  Each year, your sponosred users must renew their account. You will need to approve the renewal by logging into the NYU Identity Management service. We have a walkthrogh of the approval process here  ","version":"Next","tagName":"h2"},{"title":"Bulk HPC Accounts for Courses​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/getting_started/getting_and_renewing_an_account/#bulk-hpc-accounts-for-courses","content":" HPC bulk accounts request is disabled for HPC sponsors.  If you would like to use JupyterHub for your classes, please don't submit the form below, read [Jupyter Hub page] instead (the link to an intake form is also there) Please fill out this request form for the course, we'll create HPC accounts for the class per request Note that accounts created for courses last until the end of the semester, rather than a full year.  ","version":"Next","tagName":"h2"},{"title":"Getting an account with one of NYU partners​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/getting_started/getting_and_renewing_an_account/#getting-an-account-with-one-of-nyu-partners","content":" NYU partners ([look for the list here]) with many state and national facilities with a variety of HPC systems and expertise. [Contact us] for assistance setting up a collaboration with any of these.  ","version":"Next","tagName":"h2"},{"title":"Non-NYU Researchers​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/getting_started/getting_and_renewing_an_account/#non-nyu-researchers","content":" If you are part of collaboration with NYU researcher you need to obtain an affiliate status before applying for an NYU HPC account. A full-time NYU faculty member must sponsor a non-NYU collaborator for an affiliate status.  Please see instructions for affiliate management (NYU NetID login is required to follow the link). Please read instructions about sponsoring external collaborators here.  ","version":"Next","tagName":"h2"},{"title":"Access to cluster after Graduation​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/getting_started/getting_and_renewing_an_account/#access-to-cluster-after-graduation","content":" If you will still work on a project with an NYU researchers after graduation - refer to the section above for &quot;Non-NYU Researchers&quot;  If you are not part of a collaboration, your access to cluster will end together with NetID becoming non-active. Please copy all your data cluster (if you need any) before that time.  ","version":"Next","tagName":"h2"},{"title":"VPN on a Linux machine​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/getting_started/getting_and_renewing_an_account/#vpn-on-a-linux-machine","content":" note In order to request a new HPC account or renew an expired one, you need to be connected to the NYU VPN if you are working remotely, Please see instructions on how to install and use the NYU VPN. Linux clients are not officially supported, however we were able to successfully use openVPN client. Here are installation and connection instructions for a debian linux distribution with apt pacakge manager: apt-get install openconnect sudo openconnect -b vpn.nyu.edu When prompted follow the instructions and provide your netID, password, and authenticate with ('push', 'phone1' or 'sms') This method was tested on few Linux distributions and settings however is not guaranteeed to work in future. ","version":"Next","tagName":"h2"},{"title":"Renewing your HPC Account with IIQ","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/getting_started/walkthrough_renew_hpc_account_iiq/","content":"Renewing your HPC Account with IIQ Login to the URL given below, using your netid/password, to create or manage HPC Account Requests: https://identity.it.nyu.edu/ (NYU VPN is required) Upon logging in, an end user’s landing page will look like this If the menu does not appear, select the &quot;burger&quot; menu on the top left hand corner: The burger menu will show an &quot;Update/Renew HPC Account&quot; option - select this. Next complete the form as instructed. Please note that all accounts require the sponsorship of a full-time NYU faculty member. The user’s name will be pre-populated, and the forms required fields must be completed (sponsor, reason for request, consent to terms of use). After clicking “Submit” the chosen sponsor will be notified of the request and provisioning will only occur after approval. NOTE: If your HPC Account is due for renewal you will get an update on your dashboard which will suggest you to fill out a form given in the &quot;Latest form&quot; widget for renewing your account If you are not a full-time NYU faculty member, you will need an NYU faculty member to sponsor your application. This is probably your thesis supervisor, or NYU collaborator. Hit submit, and the request will go to your sponsor to approve (if applicable), and your account will be created, usually within a day of being approved. You will be returned to the dashboard, and now you should see your request in the &quot;Pending Approvals&quot; tables. If after a few days you still do not have an account, check with your sponsor - they may have missed a step in the approval process. If you are still stuck, contact us at hpc@nyu.edu for assistance.","keywords":"","version":"Next"},{"title":"Machine Learning on HPC","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/ml_ai_hpc/intro/","content":"Machine Learning on HPC","keywords":"","version":"Next"},{"title":"Fine tune LLMs on HPC","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/ml_ai_hpc/llm_fine_tuning/","content":"Fine tune LLMs on HPC LoRA fine-tuning?","keywords":"","version":"Next"},{"title":"PyTorch on HPC","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/ml_ai_hpc/pytorch_on_hpc/","content":"PyTorch on HPC Distribtued training, inference, etc","keywords":"","version":"Next"},{"title":"LLMs on HPC","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/ml_ai_hpc/llm_on_hpc/","content":"LLMs on HPC How to run LLMs on HPC.","keywords":"","version":"Next"},{"title":"Greene Spec Sheet","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/spec_sheet/","content":"","keywords":"","version":"Next"},{"title":"Hardware Specs​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/spec_sheet/#hardware-specs","content":" Please find Greene's hardware specification in detail at the google sheets here:  tip Hover a mouse over a cell with a black triangle to see more details_    ","version":"Next","tagName":"h2"},{"title":"Mounted Storage Systems​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/spec_sheet/#mounted-storage-systems","content":" Please find the details on Greene's available storage offerings at the google sheets here:    ","version":"Next","tagName":"h2"},{"title":"General Parallel File System (GPFS)​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/spec_sheet/#general-parallel-file-system-gpfs","content":" The NYU HPC Clusters are served by a General Parallel File System (GPFS) storage cluster. GPFS is a high-performance clustered file system software developed by IBM that provides concurrent high-speed file access to applications executing on multiple nodes of clusters.  The cluster storage runs on Lenovo Distributed Storage Solution DSS-G hardware:  2x DSS-G 202 116 Solid State Drives (SSDs)464 TB raw storage 2x DSS-G 240 668 Hard Disk Drives (HDDs)9.1 PB raw storage  ","version":"Next","tagName":"h2"},{"title":"GPFS Performance​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/spec_sheet/#gpfs-performance","content":" \tRead Bandwidth\t78 GB per second reads Write Bandwidth\t42 GB per second writes I/O Performance\t~650k Input/Output operations per second (IOPS)  ","version":"Next","tagName":"h3"},{"title":"Flash Tier Storage (VAST)​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/spec_sheet/#flash-tier-storage-vast","content":" An all flash file system, using VAST Flash storage is now available on Greene. Flash storage is optimal for computational workloads with high I/O rates. For example, if you have jobs to run with huge number of tiny files, VAST may be a good candidate.  Please contact the team hpc@nyu.edu for more information.  NVMe Interface778 TB Total StorageAvailable to all users as read onlyWrite access available to approved users only  ","version":"Next","tagName":"h2"},{"title":"Research Project Space (RPS)​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/spec_sheet/#research-project-space-rps","content":" Research Project Space (RPS) volumes provide working spaces for sharing data and work amongst project or lab members for long term research needs.RPS directories are available on the Greene HPC cluster.RPS is backed up. There is no file purging policy on RPS.There is a cost per TB per year, and inodes per year for RPS volumes.  Please find more inforamtion at [Research Project Space page].  ","version":"Next","tagName":"h2"},{"title":"Data Transfer Nodes (gDTN)​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/spec_sheet/#data-transfer-nodes-gdtn","content":" \tNode Type\tLenovo SR630 Number of Nodes\t2 CPUs\t2x Intel Xeon Gold 6244 8C 150W 3.6 GHz Processor. Memory\t192 GB (total) - 12x 16GB DDR4, 2933 MHz Local Disk\t1x 1.92 TB SSD Infiniband Interconnect\t1x Mellanox ConnectX-6 HDR100/100GbE VPI 1-Port x16 PCIe 3.0 HCA Ethernet Connectivity to the NYU High-Speed Research Network (HSRN)\t200 Gbit - 1x Mellanox ConnectX-5 EDR IB/100GbE VPI Dual-Port x16 PCIe 3.0 HCA ","version":"Next","tagName":"h2"},{"title":"Available storage systems","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/available_storage_systems/","content":"","keywords":"","version":"Next"},{"title":"GPFS​","type":1,"pageTitle":"Available storage systems","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/available_storage_systems/#gpfs","content":" General Parallel File System (GPFS) storage cluster is a high-performance clustered file system developed by IBM that provides concurrent high-speed file access to applications executing on multiple nodes of clusters.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Available storage systems","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/available_storage_systems/#configuration","content":" The NYU HPC cluster storage runs on Lenovo Distributed Storage Solution DSS-G hardware:  2x DSS-G 202 116 Solid State Drives (SSDs)464TB raw storage 2x DSS-G 240 668 Hard Disk Drives (HDDs)9.1PB raw storage  ","version":"Next","tagName":"h3"},{"title":"Performance​","type":1,"pageTitle":"Available storage systems","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/available_storage_systems/#performance","content":" Read Speed: 78 GB per second read speedsWrite Speed: 42 GB per second write speedsI/O Performance: up to 650k input/output operations per second (IOPS)  ","version":"Next","tagName":"h3"},{"title":"Flash Tier Storage (VAST)​","type":1,"pageTitle":"Available storage systems","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/available_storage_systems/#flash-tier-storage-vast","content":" An all flash file system, using VAST Flash storage, is now available on Greene. Flash storage is optimal for computational workloads with high I/O rates. For example, If you have jobs to run with huge amount of tiny files, VAST may be a good candidate. If you and your lab members are interested, please reach out to hpc@nyu.edu for more information.  NVMe interfaceTotal size: 778 TB  note /vast is available for all users to read and available to approved users to write data.  ","version":"Next","tagName":"h2"},{"title":"Research Project Space (RPS)​","type":1,"pageTitle":"Available storage systems","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/available_storage_systems/#research-project-space-rps","content":" Research Project Space (RPS) volumes provide working spaces for sharing data and code amongst project or lab members.  RPS directories are available on the Greene HPC cluster.There is no old-file purging policy on RPS.RPS is backed up.There is a cost per TB per year and inodes per year for RPS volumes.  Please see Research Project Space for more information. ","version":"Next","tagName":"h2"},{"title":"Best Practices on HPC Storage","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/best_practices/","content":"","keywords":"","version":"Next"},{"title":"User Quota Limits and the myquota command​","type":1,"pageTitle":"Best Practices on HPC Storage","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/best_practices/#user-quota-limits-and-the-myquota-command","content":" All users have quote limits set on HPC fie systems. There are several types of quota limits, such as limits on the amount of disk space (disk quota), number of files (inode quota) etc. The default user quota limits on HPC file systems are listed on our Data Management page.  warning One of the common issues users report is running out of inodes in their home directory. This usually occurs during software installation, for example installing conda environment under their home directory. Running out of quota causes a variety of issues such as running user jobs being interrupted or users being unable to finish the installation of packages under their home directory.  Users can check their current utilization of quota using the myquota command. The myquota command provides a report of the current quota limits on mounted file systems, the user's quota utilization, as well as the percentage of quota utilization.  In the following example the user who executes the myquota command is out of inodes in their home directory. The user inode quota limit on the /home file system 30.0K inodes and the user has 33000 inodes, thus 110% of the inode quota limit.  $ myquota Hostname: log-1 at Sun Mar 21 21:59:08 EDT 2021 Filesystem Environment Backed up? Allocation Current Usage Space Variable /Flushed? Space / Files Space(%) / Files(%) /home $HOME Yes/No 50.0GB/30.0K 8.96GB(17.91%)/33000(110.00%) /scratch $SCRATCH No/Yes 5.0TB/1.0M 811.09GB(15.84%)/2437(0.24%) /archive $ARCHIVE Yes/No 2.0TB/20.0K 0.00GB(0.00%)/1(0.00%) /vast $VAST No/Yes 2.0TB/5.0M 0.00GB(0.00%)/1(0.00%)   Users can find out the number of inodes (files) used per subdirectory under their home directory ($HOME), by running the following commands:  $cd $HOME $ for d in $(find $(pwd) -maxdepth 1 -mindepth 1 -type d | sort -u); do n_files=$(find $d | wc -l); echo $d $n_files; done /home/netid/.cache 1507 /home/netid/.conda 2 /home/netid/.config 2 /home/netid/.ipython 11 /home/netid/.jupyter 2 /home/netid/.keras 2 /home/netid/.local 24185 /home/netid/.nv 2 /home/netid/.sacrebleu 46 /home/netid/.singularity 1 /home/netid/.ssh 5 /home/netid/.vscode-server 7216   ","version":"Next","tagName":"h2"},{"title":"Large number of small files​","type":1,"pageTitle":"Best Practices on HPC Storage","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/best_practices/#large-number-of-small-files","content":" In case your dataset or workflow requires to use large number of small files, this can create a bottleneck due to read/write rates.  Please refer to our page on working with a large number of files to learn about some of the options we recommend to consider.  ","version":"Next","tagName":"h2"},{"title":"Installing Python packages​","type":1,"pageTitle":"Best Practices on HPC Storage","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/best_practices/#installing-python-packages","content":" warning Your home directory has a relatively small number of inodes. If you create a conda or python environment in you home directory, this can eat up all the inodes.  Please review the Package Management section of the Greene Software Page. ","version":"Next","tagName":"h2"},{"title":"Globus","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/globus/","content":"","keywords":"","version":"Next"},{"title":"Transferring data between endpoints​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/globus/#transferring-data-between-endpoints","content":" ","version":"Next","tagName":"h2"},{"title":"Endpoint​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/globus/#endpoint","content":" A globus Endpoint is a data transfer location, a location where data can be moved to or from using Globus transfer, sync and sharing service. An endpoint can either be a personal endpoint (on a user’s personal computer) or a server endpoint (located on a server, for use by multiple users). Please see Data Transfer With Globus for details.  ","version":"Next","tagName":"h3"},{"title":"Collection​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/globus/#collection","content":" A collection is a named set of files (or blobs), hierarchically organized in folders.  ","version":"Next","tagName":"h3"},{"title":"Data Sharing​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/globus/#data-sharing","content":" Please see How to Share Data Using Globus for details from Globus.  Instructions for NYU​  The first step in transferring data is to get a Globus account at https://www.globus.org/. Click on &quot;Log in&quot; at upper right corner. Select &quot;New York University&quot; from the pull-down menu and click on &quot;Continue&quot;.    Enter your NYU NetID and password in the familiar screen, and hit &quot;LOGIN&quot; then go through the Multi-Factor Authentication.    The &quot;File Manager&quot; panel should come up as the following image. In order to be able to transfer files, you will need to specify two Collections. A collection is defined on top of an endpoint. We can search for a collection using an endpoint name. The Server Endpoint on the NYU HPC storage is nyu#greene .        ","version":"Next","tagName":"h3"},{"title":"Server and Personal Endpoints​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/globus/#server-and-personal-endpoints","content":" note The NYU HPC Server Endpoint: nyu#greene  Globus Connect Server is already installed on the NYU HPC cluster creating a Server Endpoint named nyu#greene, that is available to authorized users (users with a valid HPC account) using Globus. If you want to move data to or from your computer and the NYU HPC cluster, you need to install Globus Connect Personal on your computer, thus creating a Personal Endpoint on your computer.  ","version":"Next","tagName":"h2"},{"title":"Moving data between Server Endpoints​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/globus/#moving-data-between-server-endpoints","content":" If you plan to transfer data between Server Endpoints, such as between the NYU server endpoint nyu#greene and a server endpoint at another institution, you do not need to install Globus Connect Personal on your computer.  Creating a Personal Endpoint on your computer​  This needs to be done only once on your personal computer.  After clicking &quot;Transfer or Sync to...&quot;, click &quot;Search&quot; on the upper right side. Then follow the link &quot;Install Globus Connect Personal&quot;.  More information about Globus Connect Personal and download links for Linux, Mac and Windows can be found at: https://www.globus.org/globus-connect-personal    ","version":"Next","tagName":"h3"},{"title":"Transfer files between your Personal Endpoint and NYU nyu#greene​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/globus/#transfer-files-between-your-personal-endpoint-and-nyu-nyugreene","content":" To transfer files you need to specify two collections (endpoints). Specify one of them as Greene scratch directory, or Greene archive directory or Greene home directory. The other endpoint is the one created for your personal computer (e.g. My Mac Laptop) if it is involved in the transfer. When you first use the Greene directory collection, authentication/consent is required for the Globus web app to manage collections on this endpoint on your behalf.    When writing to your Greene archive directory, please pay attention that there is a default inode limit of 20K per user.  When the second Endpoint is chosen to be your personal computer, your computer home directory content will show up. Now select directory and files (you may select multiple files when clicking on file names while pressing down &quot;shift&quot; key), click one of the two blue Start buttons to indicate the transfer direction. After clicking the blue Start button, you should see a message indicating a transfer request has been submitted successfully, and a transfer ID is generated. Globus file transfer service takes care of the actual copying.  When the transfer is done, you should receive an email notification. Click &quot;ACTIVITY&quot; on the Globus portal, select the transfer you want to check, a finished transfer should look like the following:    ","version":"Next","tagName":"h3"},{"title":"Small file download from web browsers​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/globus/#small-file-download-from-web-browsers","content":" Globus support HTTPS access to data. To download a small file from your web browser, select a file and right-click your mouse, then click 'Download' at the popup menu.    Additional info can be found at File Management How-Tos.  Please contact hpc@nyu.edu if you have any questions. Good luck! ","version":"Next","tagName":"h3"},{"title":"Data Transfers","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/data_transfers/","content":"","keywords":"","version":"Next"},{"title":"Data-Transfer nodes​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/data_transfers/#data-transfer-nodes","content":" Attached to the NYU HPC cluster Greene, the Greene Data Transfer Node (gDTN) are nodes optimized for transferring data between cluster file systems (e.g. scratch) and other endpoints outside the NYU HPC clusters, including user laptops and desktops. The gDTNs have 100-Gb/s Ethernet connections to the High Speed Research Network (HSRN) and are connected to the HDR Infiniband fabric of the HPC clusters. More information on the hardware characteristics is available at Greene spec sheet.  ","version":"Next","tagName":"h2"},{"title":"Data Transfer Node Access​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/data_transfers/#data-transfer-node-access","content":" The HPC cluster filesystems include /home, /scratch, /archive and the HPC Research Project Space are available on the gDTN.  The Data-Transfer Node (DTN) can be accessed in a variety of ways  From NYU-net and the High Speed Research Network: use SSH to the DTN hostname gdtn.hpc.nyu.eduFrom the Greene cluster (e.g., the login nodes): the hostname can be shortened to gdtn  tip For example, to log in to a DTN from the Greene cluster, to carry out some copy operation, and to log back out, you can use a command sequence like: ssh gdtn rsync ... logout   Via specific tools like Globus  ","version":"Next","tagName":"h3"},{"title":"Tools for Data Transfer​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/data_transfers/#tools-for-data-transfer","content":" ","version":"Next","tagName":"h2"},{"title":"Linux & Mac Tools​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/data_transfers/#linux--mac-tools","content":" scp and rsync​  warning Please use Data Transfer Nodes (DTNs) with these tools. While one can transfer data while on login nodes, it is considered a bad practice because it can degrate the node's performance.  Sometimes these two tools are convenient for transferring small files. Using the DTNs does not require to set up an SSH tunnel; use the hostname dtn.hpc.nyu.edu for one-step copying. See below for examples of commands invoked on the command line on a laptop running a Unix-like operating system:  scp HMLHWBGX7_n01_HK16.fastq.gz jdoe55@dtn.hpc.nyu.edu:/scratch/jdoe55/ rsync -av HMLHWBGX7_n01_HK16.fastq.gz jdoe55@dtn.hpc.nyu.edu:/scratch/jdoe55/   In particular, rsync can also be used on the DTNs to copy directories recursively between filesystems, e.g. (assuming that you are logged in to a DTN),  rsync -av /scratch/username/project1 /rw/sharename/   where username would be your user name, project1 a directory to be copied to the Research Workspace, and sharename the name of a share on the Research Workspace (either your NetID or the name of a project you're a member of).  ","version":"Next","tagName":"h3"},{"title":"Windows Tools​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/data_transfers/#windows-tools","content":" File Transfer Clients​  Windows 10 machines may have the Linux Subsystem installed, which will allow for the use of Linux tools, as listed above, but generally it is recommended to use a client such as WinSCP or FileZilla to transfer data. Additionally, Windows users may also take advantage of Globus to transfer files.  ","version":"Next","tagName":"h3"},{"title":"Tunneling​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/data_transfers/#tunneling","content":" Read the detailed instructions for setting up tunnels.  ","version":"Next","tagName":"h3"},{"title":"Globus​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/data_transfers/#globus","content":" Globus is the recommended tool to use for large-volume data transfers. It features automatic performance tuning and automatic retries in cases of file-transfer failures. Data-transfer tasks can be submitted via a web portal. The Globus service will take care of the rest, to make sure files are copied efficiently, reliably, and securely. Globus is also a tool for you to share data with collaborators, for whom you only need to provide the email addresses.  The Globus endpoint for Greene is available at nyu#greene. The endpoint nyu#prince has been retired.  Detailed instructions available at Globus  ","version":"Next","tagName":"h3"},{"title":"rclone​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/data_transfers/#rclone","content":" rclone - rsync for cloud storage, is a command line program to sync files and directories to and from cloud storage systems such as Google Drive, Amazon Drive, S3, B2 etc. rclone is available on DTNs.  Please see the documentation for how to use it.  ","version":"Next","tagName":"h3"},{"title":"Open OnDemand (OOD)​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/data_transfers/#open-ondemand-ood","content":" One can use Open OnDemand (OOD) interface to upload data.  warning Please only use OOD for small data transfers! Please use Data-Transfer Nodes (DTNs) for moving large data.  ","version":"Next","tagName":"h3"},{"title":"FDT​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/data_transfers/#fdt","content":" FDT stands for &quot;Fast Data Transfer&quot;. It is a command line application written in Java. With the plugin mechanism, FDT allows users to load user-defined classes for Pre- and Post-Processing of file transfers. Users can start their own server processes. If you have use cases for FDT, visit the download page to get fdt.jar to start. Please contact hpc@nyu.edu for any questions. ","version":"Next","tagName":"h3"},{"title":"Open OnDemand (OOD) with Conda/Singularity","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/ood/open_on_demand/","content":"","keywords":"","version":"Next"},{"title":"OOD + Singularity + conda​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/ood/open_on_demand/#ood--singularity--conda","content":" This page describes how to use your Singularity with conda environment in Open OnDemand (OOD) GUI at Greene.  ","version":"Next","tagName":"h2"},{"title":"Log Into Greene via the Terminal​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/ood/open_on_demand/#log-into-greene-via-the-terminal","content":" The following commands must be run from the terminal. Information on accessing via the terminal can be found at the Connecting to the HPC page.  ","version":"Next","tagName":"h3"},{"title":"Preinstallation Warning​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/ood/open_on_demand/#preinstallation-warning","content":" warning If you have initialized Conda in your base environment, your prompt on Greene may show something like: (base) [NETID@log-1 ~]$ then you must first comment out or remove this portion of your ~/.bashrc file: # &gt;&gt;&gt; conda initialize &gt;&gt;&gt; # !! Contents within this block are managed by 'conda init' !! __conda_setup=&quot;$('/share/apps/anaconda3/2020.07/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)&quot; if [ $? -eq 0 ]; then eval &quot;$__conda_setup&quot; else if [ -f &quot;/share/apps/anaconda3/2020.07/etc/profile.d/conda.sh&quot; ]; then . &quot;/share/apps/anaconda3/2020.07/etc/profile.d/conda.sh&quot; else export PATH=&quot;/share/apps/anaconda3/2020.07/bin:$PATH&quot; fi fi unset __conda_setup # &lt;&lt;&lt; conda initialize &lt;&lt;&lt; The above code automatically makes your environment look for the default shared installation of Conda on the cluster and will sabotage any attempts to install packages to a Singularity environment. Once removed or commented out, log out and back into the cluster for a fresh environment.  ","version":"Next","tagName":"h3"},{"title":"Prepare Overlay File​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/ood/open_on_demand/#prepare-overlay-file","content":" mkdir /scratch/$USER/my_env cd /scratch/$USER/my_env cp -rp /scratch/work/public/overlay-fs-ext3/overlay-15GB-500K.ext3.gz . gunzip overlay-15GB-500K.ext3.gz   Above we used the overlay file &quot;overlay-15GB-500K.ext3.gz&quot; which will contain all of the installed packages. There are more optional overlay files. You can find instructions on the following pages: Singularity with Conda, Squash File System and Singularity.  ","version":"Next","tagName":"h3"},{"title":"Launch Singularity Environment for Installation​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/ood/open_on_demand/#launch-singularity-environment-for-installation","content":" singularity exec --overlay /scratch/$USER/my_env/overlay-15GB-500K.ext3:rw /scratch/work/public/singularity/cuda12.3.2-cudnn9.0.0-ubuntu-22.04.4.sif /bin/bash   Above we used the Singularity OS image &quot;cuda12.3.2-cudnn9.0.0-ubuntu-22.04.4.sif &quot; which provides the base operating system environment for the conda environment. There are other Singularity OS images available at /scratch/work/public/singularity  Launching Singularity with the --overlay flag mounts the overlay file to a new directory: /ext3 - you will notice that when not using Singularity /ext3 is not available. Be sure that you have the Singularity prompt (Singularity&gt;) and that /ext3 is available before the next step:  Singularity&gt; ls -lah /ext3 total 8.5K drwxrwxr-x. 2 root root 4.0K Oct 19 10:01 . drwx------. 29 root root 8.0K Oct 19 10:01 ..   ","version":"Next","tagName":"h3"},{"title":"Install Miniforge to Overlay File​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/ood/open_on_demand/#install-miniforge-to-overlay-file","content":" wget --no-check-certificate https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh sh Miniforge3-Linux-x86_64.sh -b -p /ext3/miniforge3   Next, create a wrapper script at /ext3/env.sh  touch /ext3/env.sh echo '#!/bin/bash' &gt;&gt; /ext3/env.sh echo 'unset -f which' &gt;&gt; /ext3/env.sh echo 'source /ext3/miniforge3/etc/profile.d/conda.sh' &gt;&gt; /ext3/env.sh echo 'export PATH=/ext3/miniforge3/bin:$PATH' &gt;&gt; /ext3/env.sh echo 'export PYTHONPATH=/ext3/miniforge3/bin:$PATH' &gt;&gt; /ext3/env.sh   Your /ext3/env.sh file should now contain the following:  #!/bin/bash unset -f which source /ext3/miniforge3/etc/profile.d/conda.sh export PATH=/ext3/miniforge3/bin:$PATH export PYTHONPATH=/ext3/miniforge3/bin:$PATH   The wrapper script will activate your conda environment, to which you will be installing your packages and dependencies.  Next, activate your conda environment with the following:  source /ext3/env.sh   ","version":"Next","tagName":"h3"},{"title":"Install Packages to Miniforge Environment​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/ood/open_on_demand/#install-packages-to-miniforge-environment","content":" Now that your environment is activated, you can update and install packages  conda config --remove channels defaults conda update -n base conda -y conda clean --all --yes conda install pip --yes conda install ipykernel --yes # Note: ipykernel is required to run as a kernel in the Open OnDemand Jupyter Notebooks   To confirm that your environment is appropriately referencing your Miniforge installation, try out the following:  unset which which conda # output: /ext3/miniforge3/bin/conda which python # output: /ext3/miniforge3/bin/python python --version # output: Python 3.8.5 which pip # output: /ext3/miniforge3/bin/pip   Now use either conda install or pip to install your required python packages to the Miniforge environment.  To install larger packages, like Tensorflow, you must first start an interactive job with adequate compute and memory resources to install packages. The login nodes restrict memory to 2GB per user, which may cause some large packages to crash.  srun --cpus-per-task=2 --mem=10GB --time=04:00:00 --pty /bin/bash # wait to be assigned a node singularity exec --overlay /scratch/$USER/my_env/overlay-15GB-500K.ext3:rw /scratch/work/public/singularity/cuda12.3.2-cudnn9.0.0-ubuntu-22.04.4.sif /bin/bash source /ext3/env.sh # activate the environment   After it is running, you’ll be redirected to a compute node. From there, run singularity to setup on conda environment, same as you were doing on login node.  ","version":"Next","tagName":"h3"},{"title":"Configure iPython Kernels​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/ood/open_on_demand/#configure-ipython-kernels","content":" To create a kernel named my_env copy the template files to your home directory.  mkdir -p ~/.local/share/jupyter/kernels cd ~/.local/share/jupyter/kernels cp -R /share/apps/mypy/src/kernel_template ./my_env # this should be the name of your Singularity env cd ./my_env ls #kernel.json logo-32x32.png logo-64x64.png python # files in the ~/.local/share/jupyter/kernels directory   To set the conda environment, edit the file named 'python' in /.local/share/jupyter/kernels/my_env/.  The python file is a wrapper script that the Jupyter notebook will use to launch your Singularity container and attach it to the notebook.  At the bottom of the file we have the template singularity command.  singularity exec $nv \\ --overlay /scratch/$USER/my_env/overlay-15GB-500K.ext3:ro \\ /scratch/work/public/singularity/cuda12.3.2-cudnn9.0.0-ubuntu-22.04.4.sif \\ /bin/bash -c &quot;source /ext3/env.sh; $cmd $args&quot;   warning If you used a different overlay (/scratch/$USER/my_env/overlay-15GB-500K.ext3 shown above) or .sif file (/scratch/work/public/singularity/cuda12.3.2-cudnn9.0.0-ubuntu-22.04.4.sif shown above), you MUST change those lines in the command above to the files you used.  Edit the default kernel.json file by setting PYTHON_LOCATION and KERNEL_DISPLAY_NAME using a text editor like nano/vim.  { &quot;argv&quot;: [ &quot;PYTHON_LOCATION&quot;, &quot;-m&quot;, &quot;ipykernel_launcher&quot;, &quot;-f&quot;, &quot;{connection_file}&quot; ], &quot;display_name&quot;: &quot;KERNEL_DISPLAY_NAME&quot;, &quot;language&quot;: &quot;python&quot; }   to  { &quot;argv&quot;: [ &quot;/home/&lt;Your NetID&gt;/.local/share/jupyter/kernels/my_env/python&quot;, &quot;-m&quot;, &quot;ipykernel_launcher&quot;, &quot;-f&quot;, &quot;{connection_file}&quot; ], &quot;display_name&quot;: &quot;my_env&quot;, &quot;language&quot;: &quot;python&quot; }   Update the &quot;&lt;Your NetID&gt;&quot; to your own NetID without the &quot;&lt;&gt;&quot; symbols.  ","version":"Next","tagName":"h3"},{"title":"Launch an Open OnDemand Jupyter Notebook​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/ood/open_on_demand/#launch-an-open-ondemand-jupyter-notebook","content":" https://ood.hpc.nyu.edu   ","version":"Next","tagName":"h3"},{"title":"HPC Storage","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/intro_and_data_management/","content":"","keywords":"","version":"Next"},{"title":"Highlights​","type":1,"pageTitle":"HPC Storage","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/intro_and_data_management/#highlights","content":" 9.5 PB Total GPFS Storage Up to 78 GB per second read speedsUp to 650k input/output operations per second (IOPS) Research Project Space (RPS): RPS volumes provide working spaces for sharing data and code amongst project or lab members  ","version":"Next","tagName":"h2"},{"title":"Introduction to HPC Data Management​","type":1,"pageTitle":"HPC Storage","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/intro_and_data_management/#introduction-to-hpc-data-management","content":" The NYU HPC Environment provides access to a number of file systems to better serve the needs of researchers managing data during the various stages of the research data lifecycle (data capture, analysis, archiving, etc.). Each HPC file system comes with different features, policies, and availability.  In addition, a number of data management tools are available that enable data transfers and data sharing, recommended best practices, and various scenarios and use cases of managing data in the HPC Environment.  Multiple public data sets are available to all users of the HPC environment, such as a subset of The Cancer Genome Atlas (TCGA), the Million Song Database, ImageNet, and Reference Genomes.  Below is a list of file systems with their characteristics and a summary table. Reviewing the list of available file systems and the various Scenarios/Use cases that are presented below, can help select the right file systems for a research project. As always, if you have any questions about data storage in the HPC environment, you can request a consultation with the HPC team by sending email to hpc@nyu.edu.  ","version":"Next","tagName":"h2"},{"title":"Data Security Warning​","type":1,"pageTitle":"HPC Storage","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/intro_and_data_management/#data-security-warning","content":" warning Moderate Risk Data - HPC Approved​ The HPC Environment has been approved for storing and analyzing Moderate Risk research data, as defined in the NYU Electronic Data and System Risk Classification Policy.High Risk research data, such as those that include Personal Identifiable Information (PII) or electronic Protected Health Information (ePHI) or Controlled Unclassified Information (CUI) should NOT be stored in the HPC Environment. note only the Office of Sponsored Projects (OSP) and Global Office of Information Security (GOIS) are empowered to classify the risk categories of data. tip High Risk Data - Secure Research Data Environments (SRDE) Approved​ Because the HPC system is not approved for High Risk data, we recommend using an approved system like the Secure Research Data Environments (SRDE).  ","version":"Next","tagName":"h3"},{"title":"Data Storage options in the HPC Environment​","type":1,"pageTitle":"HPC Storage","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/intro_and_data_management/#data-storage-options-in-the-hpc-environment","content":" User Home Directories​  Every individual user has a home directory (under /home/$USER, environment variable $HOME) for permanently storing code and important configuration files. Home Directories provide limited storage space (50 GB) and inodes (files) 30,000 per user. Users can check their quota utilization using the myquota command.  User home directories are backed up daily and old files under $HOME are not purged.  The User home directories are available on all HPC clusters (Greene) and on every cluster node (login nodes, compute nodes) as well as and Data Transfer Node (gDTN).  warning Avoid changing file and directory permissions in your home directory to allow other users to access files.  User Home Directories are not ideal for sharing files and folders with other users. HPC Scratch or Research Project Space (RPS) are better file systems for sharing data.  warning One of the common issues that users report regarding their home directories is running out of inodes, i.e. the number of files stored under their home exceeds the inode limit, which by default is set to 30,000 files. This typically occurs when users install software under their home directories, for example, when working with Conda and Julia environments, that involve many small files.  tip To find out the current space and inode quota utilization and the distribution of files under your home directory, please see: Understanding user quota limits and the myquota command.Working with Conda environments: To avoid running out of inode limits in home directories, the HPC team recommends setting up conda environments with Singularity overlay images  HPC Scratch​  The HPC scratch file system is the HPC file system where most of the users store research data needed during the analysis phase of their research projects. The scratch file system provides temporary storage for datasets needed for running jobs.  Files stored in the HPC scratch file system are subject to the HPC Scratch old file purging policy: Files on the /scratch file system that have not been accessed for 60 or more days will be purged.  Every user has a dedicated scratch directory (/scratch/$USER) with 5 TB disk quota and 1,000,000 inodes (files) limit per user.  The scratch file system is available on all nodes (compute, login, etc.) on Greene as well as Data Transfer Node (gDTN).  warning There are No Back ups of the scratch file system. Files that were deleted accidentally or removed due to storage system failures CAN NOT be recovered.  tip Since there are no back ups of HPC Scratch file system, users should not put important source code, scripts, libraries, executables in /scratch. These important files should be stored in file systems that are backed up, such as /home or Research Project Space (RPS). Code can also be stored in a git repository.Old file purging policy on HPC Scratch: All files on the HPC Scratch file system that have not been accessed for more than 60 days will be removed. It is a policy violation to use scripts to change the file access time. Any user found to be violating this policy will have their HPC account locked. A second violation may result in your HPC account being turned off.To find out the user's current disk space and inode quota utilization and the distribution of files under your scratch directory, please see: Understanding user quota Limits and the myquota command.Once a research project completes, users should archive their important files in the HPC Archive file system.  HPC Vast​  The HPC Vast all-flash file system is the HPC file system where users store research data needed during the analysis phase of their research projects, particuarly for high I/O data that can bottleneck on the scratch file system. The Vast file system provides temporary storage for datasets needed for running jobs.  Files stored in the HPC vast file system are subject to the HPC Vast old file purging policy: Files on the /vast file system that have not been accessed for 60 or more days will be purged.  Every user has a dedicated vast directory (/vast/$USER) with 2 TB disk quota and 5,000,000 inodes (files) limit per user.  The vast file system is available on all nodes (compute, login, etc.) on Greene as well as Data Transfer Node (gDTN).  warning There are No Back ups of the vastsc file system. Files that were deleted accidentally or removed due to storage system failures CAN NOT be recovered.  tip Since there are no back ups of HPC Vast file system, users should not put important source code, scripts, libraries, executables in /vast. These important files should be stored in file systems that are backed up, such as /home or Research Project Space (RPS). Code can also be stored in a git repository.Old file purging policy on HPC Vast: All files on the HPC Vast file system that have not been accessed for more than 60 days will be removed. It is a policy violation to use scripts to change the file access time. Any user found to be violating this policy will have their HPC account locked. A second violation may result in your HPC account being turned off.To find out the user's current disk space and inode quota utilization and the distribution of files under your vast directory, please see: Understanding user quota Limits and the myquota command.Once a research project completes, users should archive their important files in the HPC Archive file system.  HPC Research Project Space​  The HPC Research Project Space (RPS) provides data storage space for research projects that is easily shared amongst collaborators, backed up, and not subject to the old file purging policy. HPC RPS was introduced to ease data management in the HPC environment and eliminate the need of having to frequently copying files between Scratch and Archive file systems by having all projects files under one area. These benefits of the HPC RPS come at a cost. The cost is determined by the allocated disk space and the number of files (inodes).  For detailed information about RPS see: HPC Research Project Space  HPC Work​  The HPC team makes available a number of public datasets that are commonly used in analysis jobs. The data sets are available Read-Only under /scratch/work/public.  For some of the datasets users must provide a signed usage agreement before accessing.  Public datasets available on the HPC clusters can be viewed on the Datasets page.  HPC Archive​  Once the Analysis stage of the research data lifecycle has completed, HPC users should tar their data and code into a single tar.gz file and then copy the file to their archive directory (/archive/$USER). The HPC Archive file system is not accessible by running jobs; it is suitable for long-term data storage. Each user has access to a default disk quota of 2TB and 20,000 inode (files) limit. The rather low limit on the number of inodes per user is intentional. The archive file system is available only on login nodes of Greene. The archive file system is backed up daily.  Here is an example tar command that combines the data in a directory named my_run_dir under $SCRATCH and outputs the tar file in the user's $ARCHIVE:  # to archive `$SCRATCH/my_run_dir` tar cvf $ARCHIVE/simulation_01.tar -C $SCRATCH my_run_dir   NYU (Google) Drive​  Google Drive (NYU Drive) is accessible from the NYU HPC environment and provides an option to users who wish to archive data or share data with external collaborators who do not have access to the NYU HPC environment.  Currently (March 2021) there is no limit on the amount of data a user can store on Google Drive and there is no cost associated with storing data on Google Drive (although we hear rumors that free storage on Google Drive may be ending soon).  However, there are limits to the data transfer rate in moving to/from Google Drive. Thus, moving many small files to Google Drive is not going to be efficient.  Please read the Instructions on how to use cloud storage within the NYU HPC Environment.  HPC Storage Mounts Comparison Table​    Please see the next page for best practices for data management on NYU HPC systems. ","version":"Next","tagName":"h3"},{"title":"Large Number of Small Files","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/large_number_of_small_files/","content":"","keywords":"","version":"Next"},{"title":"Motivation​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/large_number_of_small_files/#motivation","content":" Many datasets contain a large number of files (for example ImageNet contains 14 million images, with ~150 GB size). How to deal with this data? How to store it? How to use for computations? Long-term storage of data is not an issue - an archive like tar.gz can handle this pretty well. However, when you want to use data in computations, the performance may depend on how you handle the data on disk.  Here are some ideas you can try and evaluate performance for your own project  ","version":"Next","tagName":"h2"},{"title":"Squash file system with Singularity​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/large_number_of_small_files/#squash-file-system-with-singularity","content":" Please see Squash File System and Singularity  ","version":"Next","tagName":"h2"},{"title":"Use jpg/png files on disk​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/large_number_of_small_files/#use-jpgpng-files-on-disk","content":" One option is to store image files (like png or jpg) on the disk and read from disk directly.  warning An issue with this approach, is that many linux file system can hold only a limited number of files.  # One can open greene cluster and run the following command $ df -ih /scratch/ Filesystem Inodes IUsed IFree IUse% Mounted on 10.0.0.40@o2ib:10.0.0.41@o2ib:/scratch1 1.6G 209M 1.4G 14% /scratch   This shows us that the total number of files '/scratch' can hold (currently) is about 1.6 G. This looks like a large number. But let us translate this into number of datasets like ImageNet (14 mil images) -&gt; 100 datasets like that would almost fully occupy Total possible slots for files! This is a problem!  And even if you can ignore this on your own PC, on HPC, there is a limit of files each user can put on /scratch to prevent such problems.  This is the reason why you can't just extract all those files in /scratch  ","version":"Next","tagName":"h2"},{"title":"SLURM_TMPDIR​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/large_number_of_small_files/#slurm_tmpdir","content":" Another option would be to start a SLURM job and extract everything into $SLURM_TMPDIR. This can work, but would require you to un-tar every time you run a SLURM command.  ","version":"Next","tagName":"h2"},{"title":"SLURM_RAM_TMPDIR​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/large_number_of_small_files/#slurm_ram_tmpdir","content":" You can also use the custom-made RAM mapped disk using #SLURM_RAM_TMPDIR while submitting the job. In this case when you start a job you first un-tar your files to $SLURM_RAM_TMPDIR and then read from there.  warning This basically requires you to use 2*(size of the data) size of RAM just to hold the data.  ","version":"Next","tagName":"h2"},{"title":"Binary files (pickle, etc)​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/large_number_of_small_files/#binary-files-pickle-etc","content":" Store data in some binary file (say pickle in Python) which you load fully when you start a SLURM job.  This option may require a lot of RAM - thus you may have to wait a long time for the scheduler to find resources for your job. Also this approach would not work on a regular PC without so much RAM, and thus your scripts are not transferable.  ","version":"Next","tagName":"h2"},{"title":"Container files, one-file databases​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/large_number_of_small_files/#container-files-one-file-databases","content":" Special containers, which allow to either load data fast fully or access chosen elements without loading the whole dataset into RAM.  ","version":"Next","tagName":"h2"},{"title":"SQLite​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/large_number_of_small_files/#sqlite","content":" If you have structured data, a good option would be to use SQLite. Please see SQLite: Handling Large Structured Data for more information.  ","version":"Next","tagName":"h3"},{"title":"HDF5​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/large_number_of_small_files/#hdf5","content":" One can think about HDF5 file as a &quot;container file&quot; (database of a sort), which holds a lot of objects inside.  HDF5 files do not have a file size limitation, and can hold huge number of objects inside, providing fast read/write access to those objects.  It is easy to learn how to subset data and load to RAM only to those data objects that you need.  More info:  Developers websitehdf5 in Python book hdf5 in R  tip You can get the Python book for free with your NYU email address. Go to 'log in' at the top right of the page and select to log in using your Google credentials. Use your NYU email address.  hdf5 supports reading and writing in parallel, so you can use several nodes reading from the same file.  More info: Documentation, Tutorial, Help Desk  ","version":"Next","tagName":"h3"},{"title":"LMDB​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/large_number_of_small_files/#lmdb","content":" LMDB (Lightning Memory-Mapped Database) is a light-weight, high-speed embedded database for key-value data.  Essentially, this is a large file sitting on the disk that contains a lot of smaller objects inside.  This is a memory-mapped database meaning, file can be larger than RAM. OS is responsible for managing the pages (like caching frequently uses pages).  For practical use it means: say you have 10 GB of RAM, and LMDB file of 100 GB. When you connect to this file, OS may decide to load 5GB to RAM, and the rest 95GB will be attached as virtual memory. Greene does not have a limit for virtual memory. Of course, if your RAM is larger than LMDB file, this database will perform the best, as OS will have enough resources to keep what is needed directly in RAM.  tip when you write key-value pairs to LMDB they have to be byte-encoded. For example, if you use Python you can use: for string st.encode(), for np.array use ar.tobytes(), or in general pickle.dumps()  warning LMDB uses B Tree, which has O(log n) complexity for search. Thus, when number of elements in LMDB becomes really big, search of specific element slows down considerably  More info:  Developer websitePython package for lmdR package for lmdbDeep Learning Tensorflow with LMDB examplePytorch with LMDB example  LMDB supports reading by many readers and many parallel thread from the same file  Formats inside HDF5/LMDB: binary, numpy, other..​  One can store data in different way inside LMDB or HDF5. For example we can store binary representation of jpeg, or we can store python numpy array. In the first case file can be read from any language, in the second - only from Python. We can also store objects from other languages - for example tibble in R  Other formats​  There are other formats like Bcolz, Zarr, and others. Some examples can be found here.  ","version":"Next","tagName":"h3"},{"title":"Benchmarking Code​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/large_number_of_small_files/#benchmarking-code","content":" A benchmarking of various ways of reading data was performed on now retired Prince HPC cluster. You can find the code used to perform that benchmarking and the results at this repository.For those of you interested in using multiple cores for data reading, this code example below may be useful. Multiple cores on the same node are used. Parallelization is based on joblib Python module ","version":"Next","tagName":"h2"},{"title":"Research Project Space (RPS)","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/research_project_space/","content":"","keywords":"","version":"Next"},{"title":"Description​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/research_project_space/#description","content":" Research Project Space (RPS) volumes provide working space for sharing data and code amongst project or lab members. RPS directories are built on the same parallel file system (GPFS) like HPC Scratch. They are mounted on the cluster Compute Nodes, and thus they can be accessed by running jobs. RPS directories are backed up and there is no old file purging policy. These features of RPS simplify the management of data in the HPC environment as users of the HPC Cluster can store their data and code on RPS directories and they do not need to move data between the HPC Scratch and the HPC Archive file systems.  note Due to limitations of the underlying parallel file system, the total number of RPS volumes that can be created is limited.There is an annual cost associated with RPS.The disk space and inode usage in RPS directories do not count towards quota limits in other HPC file systems (Home, Scratch, and Archive).  ","version":"Next","tagName":"h2"},{"title":"Calculating RPS Costs​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/research_project_space/#calculating-rps-costs","content":" The PI should estimate the cost of the RPS volume by taking into account storage size and number of inodes (files). The cost is calculated annually. Costs are divided into the total space, in terabytes, and the number of inodes, in blocks of 200,000.  1 TB of Storage Cost: $100200,000 inodes Cost: $100  An initial RPS volume request must include both storage space and inodes. Modifications of existing RPS volumes can include just Storage or just inode adjustments.  An initial request includes 1TB and 200,000 inodes for an annual cost of $200.  ","version":"Next","tagName":"h2"},{"title":"Example RPS Requests​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/research_project_space/#example-rps-requests","content":" Requests can include more storage or files, as needed, such as 1TB and 400,000 inodes or 2TB and 200,000 inodes. Both of the previous examples would cost $300, since they are requesting an incremental increase of storage or inodes, respectively.  This would be the breakdown of the examples listed above:  1 TB ($100) + 400,000 inodes ($200) = $3002 TB ($200) + 200,000 inodes ($100) = $300  ","version":"Next","tagName":"h3"},{"title":"Submitting an RPS volume Request or Modification​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/research_project_space/#submitting-an-rps-volume-request-or-modification","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Decide the size (in TB) and number of inodes (files) that is needed for one year​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/research_project_space/#step-1-decide-the-size-in-tb-and-number-of-inodes-files-that-is-needed-for-one-year","content":" The minimum size of an RPS request (to create a new RPS volume or extend an existing one) is 1TB of space.  If this is a new/first request, you must purchase both storage and inodes. A typical request includes 200,000 inodes per TB of storage.  tip Before submitting an RPS request (request for a new RPS volume or extending the size of an existing volume) PIs should estimate the growth of their data (in terms of storage space and number of files) during the entire year, rather than submitting a request based on their data storage needs at the time of the request.  ","version":"Next","tagName":"h3"},{"title":"Step 2: Determine the cost of the request​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/research_project_space/#step-2-determine-the-cost-of-the-request","content":" Determine the total annual cost of the request and the contact info of the School/Department/Center finance person.  ","version":"Next","tagName":"h3"},{"title":"Step 3: Verify that the project PI has a valid HPC user account​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/research_project_space/#step-3-verify-that-the-project-pi-has-a-valid-hpc-user-account","content":" The PI administers the top level RTS directory and grants access to other users. Thus the PI must have a valid HPC user account at the time of request. Please note that the HPC user account of NYU faculty never expires and thus does not need to be renewed every year. If the PI does not have an HPC account, please request one here.  ","version":"Next","tagName":"h3"},{"title":"Step 4: The PI submits the request to the HPC team via email​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/research_project_space/#step-4-the-pi-submits-the-request-to-the-hpc-team-via-email","content":" warning Only PIs can submit RPS requests.  Submit your RPS request by contacting the HPC team via email (hpc@nyu.edu). Please include in the request:  size in TBnumber of inodescontact information of the School/Department/Center finance person  The HPC team will review the request and will contact the PI with any questions. If the request is approved, the HPC team will create (or adjust) the RPS volume with the PI's HPC user account as the owner of the RPS directory. An invoice will be generated by the IT finance team.  ","version":"Next","tagName":"h3"},{"title":"Current HPC RPS Stakeholders​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/research_project_space/#current-hpc-rps-stakeholders","content":" HPC RPS Stakeholders  ","version":"Next","tagName":"h2"},{"title":"FAQs​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/research_project_space/#faqs","content":" ","version":"Next","tagName":"h2"},{"title":"Data Retention and Backups​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/research_project_space/#data-retention-and-backups","content":" How long can I keep the lab data in RPS? For as long as the lab pays for the RPS resources. Even if the current HPC cluster retires, the RPS volumes will be transferred to the next cluster How can I find out how much of the storage and inodes have I used in my lab RPS volume Please contact HPC support What kind of backups are provided? Backups are done once a day (daily incremental). Backups are kept for 30 days. This means that if something was deleted more than 30 days ago, it won't be in the back ups and thus it won't be recoverable. Where are backups stored? RPS backups are stored on public cloud (AWS S3 Storage buckets).  ","version":"Next","tagName":"h3"},{"title":"Billing and Payments​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/research_project_space/#billing-and-payments","content":" What happens if I do not pay my bill? If the invoice is not paid for more than 60 days, the lab RPS directory will be 'tar'-ed and copied to an archival area. If 60 more days pass and the invoice is still not paid the tar files will be deleted. Can I pay for RPS using a credit card? Unfortunately we're unable to process credit card payments Can I pay for multiple years instead of paying every year? Yes, we can arrange for multiyear agreement ","version":"Next","tagName":"h3"},{"title":"Sharing Data on HPC","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/sharing_data_on_hpc/","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/sharing_data_on_hpc/#introduction","content":" To share files on the cluster with other users, we recommend using file access control lists (FACL) for a user to share access to their data with others. FACL mechanism allows a fine-grained control access to any files by any users or groups of users. We discourage users from setting '777' permissions with chmod, because this can lead to data loss (by a malicious user or unintentionally, by accident). Linux commands getfacl and setfacl are used to view and set access.  ACL mechanism, just like regular Linux POSIX, allows three different levels of access control:  Read (r) - the permission to see the contents of a fileWrite (w) - the permission to edit a fileeXecute (X) - the permission to call a file or run it (in this case we use X instead of x because the X permission uses inherited executable permissions and not all files need execution)  This level of access can be granted to  user (owner of the file)group (owner group)other (everyone else)  ACL allows to grant the same type access without modifying file ownership and without changing POSIX permissions.  ","version":"Next","tagName":"h2"},{"title":"Viewing ACL​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/sharing_data_on_hpc/#viewing-acl","content":" Use getfacl to retrieve access permissions for a file.  $ getfacl myfile.txt # file: myfile.txt # owner: ab123 # group: users user::rw- group::--- other::---   The example above illustrates that in most cases ACL looks just like the chmod-based permissions: owner of the file has read and write permission, members of the group and everyone else have no permissions at all.  ","version":"Next","tagName":"h2"},{"title":"Setting ACL​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/sharing_data_on_hpc/#setting-acl","content":" Modify access permissions Use setfacl:  # general syntax: $ setfacl [option] [action/specification] file # most important options are # -m to modify ACL # -x to remove ACL # -R to apply the action recursively (apply to everything inside the directory) # To set permissions for a user (user is either the user name or ID): $ setfacl -m &quot;u:user:permissions&quot; &lt;file/dir&gt; ## To set permissions for a group (group is either the group name or ID): $ setfacl -m &quot;g:group:permissions&quot; &lt;file/dir&gt; # To set permissions for others: $ setfacl -m &quot;other:permissions&quot; &lt;file/dir&gt; # To allow all newly created files or directories to inherit entries from the parent directory (this will not affect files which will be copied into the directory afterwards): $ setfacl -dm &quot;entry&quot; &lt;dir&gt; # To remove a specific entry: $ setfacl -x &quot;entry&quot; &lt;file/dir&gt; # To remove the default entries: $ setfacl -k &lt;file/dir&gt; # To remove all entries (entries of the owner, group and others are retained): $ setfacl -b &lt;file/dir&gt;   tip Give Access to Parent Directories in the Path When you would like to set ACL to say /a/b/c/example.out, you also need to set appropriate ACLs to all the parent directories in the path. If you want to give read/write/execute permissions for the file /a/b/c/example.out, you would also need to give at least r-x permissions to the directories: /a, /a/b, and /a/b/c.  ","version":"Next","tagName":"h2"},{"title":"Remove All ACL Entries​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/sharing_data_on_hpc/#remove-all-acl-entries","content":" $ setfacl -b abc   ","version":"Next","tagName":"h3"},{"title":"Check ACLs​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/sharing_data_on_hpc/#check-acls","content":" $ getfacl abc # file: abc # owner: someone # group: someone user::rw- group::r-- other::r--   You can see with ls -l if a file has extended permissions set with setfacl: the + in the last column of the permissions field indicates that this file has detailed access permissions via ACLs:  $ ls -la total 304 drwxr-x---+ 18 ab123 users 4096 Apr 3 14:32 . drwxr-xr-x 1361 root root 0 Apr 3 09:35 .. -rw------- 1 ab123 users 4502 Mar 28 22:27 my_private_file -rw-r-xr--+ 1 ab123 users 29 Feb 11 23:18 dummy.txt   ","version":"Next","tagName":"h3"},{"title":"Flags​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/sharing_data_on_hpc/#flags","content":" Please read man setfacl for possible flags. For example:  '-m' - modify'-x' - remove'-R' - recursive (apply ACL to all content inside a directory)'-d' - default (set given settings as default - useful for a directory - all the new content inside in the future will have given ACL)  ","version":"Next","tagName":"h3"},{"title":"Examples​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/sharing_data_on_hpc/#examples","content":" ","version":"Next","tagName":"h2"},{"title":"File ACL Example​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/sharing_data_on_hpc/#file-acl-example","content":" Set read, write, and execute (rwX) permissions for user johnny to file named abc:  $ setfacl -m &quot;u:johnny:rwX&quot; abc   note We recommend for the permissions using a capital 'X' as using a lowercase 'x' will make all files executable, so we reommcned this: Check permissions: $ getfacl abc # file: abc # owner: someone # group: someone user::rw- user:johnny:rwX group::r-- mask::rwX other::r-- Change permissions for user johnny: $ setfacl -m &quot;u:johnny:r-X&quot; abc Check permissions: $ getfacl abc # file: abc # owner: someone # group: someone user::rw- user:johnny:r-X group::r-- mask::r-X other::r--   ","version":"Next","tagName":"h3"},{"title":"Directory ACL Example​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/sharing_data_on_hpc/#directory-acl-example","content":" Let's say alice123 wants to share directory /scratch/alice123/shared/researchGroup/group1 with user bob123  ## Read/execute access to /scratch/alice123 setfacl -m u:bob123:r-X /scratch/alice123 ## Read/execute access to /scratch/alice123/shared setfacl -m u:bob123:r-X /scratch/alice123/shared ## Read/execute access to /scratch/alice123/shared/researchGroup setfacl -m u:bob123:r-X /scratch/alice123/shared/researchGroup ## Now I can finally can give access to directory /scratch/alice123/shared/researchGroup/group1 setfacl -Rm u:bob123:rwX /scratch/alice123/shared/researchGroup/group1   note user bob123 will be able to see content of the following directories /scratch/alise123//scratch/alise123/shared/scratch/alise123/shared/researchGroup//scratch/alise123/shared/researchGroup/group1 ","version":"Next","tagName":"h3"},{"title":"Slurm: Main Commands","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_main_commands/","content":"","keywords":"","version":"Next"},{"title":"srun​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_main_commands/#srun","content":" Run a parallel job on cluster managed by Slurm, can be used:  Individual job submission where resources are allocated.In sbatch batch scripts as job steps making use of the allocated resource pool.within salloc instance making use of the resource pool.  man srun # for more information   Option\tDescription--help\tDisplay help information and exit --account\tCharge resource used by this job to a specified account --ntasks or --nodes\tRequest the number of tasks for the job Or Request the number of nodes to be allocated for this job --ntasks-per-node\tRequest that ntasks be invoked on each node. Meant to be used with --nodes --cpus-per-task\tRequest that ncpus be allocated per process. This may be useful if the job is multithreaded and requires more than one CPU per task for optimal performance. --mem or --mem-per-cpu\tSpecify the real memory required per node. Default units are megabytes. Different units can be specified using the suffix [ K | M | G | T ] Or Minimum memory required per allocated CPU --output\tRedirect stdout to a file --error\tRedirect stderr to a file --label\tPrepend task numbers to lines of stdout/err --partition\tRequest a specific partition for the resource allocation. If not specified, the default behavior is to allow the slurm controller to select the default partition as designated by the system administrator. --pty\tExecute task zero with pseudo terminal mode or using pseudo terminal specified by &lt;File Descriptor&gt;. --gres\tSpecifies a comma-delimited list of generic consumable resources, examples: --gres=gpu:1, --gres=gpu:v100:2, --gres=help or --gres=none --chdir\tSet the working directory of srun before it is executed  ","version":"Next","tagName":"h2"},{"title":"sbatch​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_main_commands/#sbatch","content":" man sbatch # for more information   Some of the popularly used directives are:  Option\tDescription#SBATCH --account\tCharge resource used by this jab to a specified account #SBATCH --nodes or #SBATCH --ntasks\tRequest allocation of minimum or maximum nodes for this job #SBATCH --ntasks-per-node\tRequest that ntasks be invoked on each node, used with --nodes #SBATCH --cpus-per-task\tAdvise the Slurm controller that ensuing job steps will require ncpus number of processors per task. Without this option, the controller will just try to allocate one processor per task #SBATCH --mem\tSpecify the real memory required per node. Default units are megabytes. Different units can be specified using the suffix [ K | M | G | T ] #SBATCH --gres\tSpecifies a comma-delimited list of generic consumable resources. #SBATCH --output\tInstruct Slurm to connect the batch script's standard output directly to a specified filename #SBATCH --error\tInstruct Slurm to connect the batch script's standard error directly to a specified filename #SBATCH --mail-user\tUser to receive email notifications of state changes as defined by --mail-type #SBATCH --mail-type\tNotify user by email when certain event types occur. Valid type values are NONE, BEGIN, END, FAIL, REQUEUE, ALL etc. Multiple type values may be specified in a comma separated list. The user to be notified is indicated with --mail-user. #SBATCH --job-name\tSpecify a name for the job allocation, the default is the name of the batch script or just sbatch #SBATCH --constraint\tEnable constraints such as --constraint=&quot;nvidia&quot; to select any kind of nvidia GPUs or --constraint=&quot;amd&quot; to select any kind of amd GPUs or --constraint=&quot;a100|h100&quot; to select either any one of two GPUs #SBATCH --chdir\tSet the working directory of sbatch script before it is executed  ","version":"Next","tagName":"h2"},{"title":"salloc​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_main_commands/#salloc","content":" The options for salloc are similar to the ones used by srun or sbatch, consult the salloc manual pages for more information on additional options and their environment variables:  man salloc # for detailed information   ","version":"Next","tagName":"h2"},{"title":"sinfo​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_main_commands/#sinfo","content":" View information about slurm nodes and partitions.  man sinfo # for more information   sinfo --Format=Partition,GRES,CPUs,Features:26,NodeList   Format\tDescriptionAvailable\tState/availability of a partition CPUs\tNumber of CPUs per node CPUsState\tNumber of CPUs by state in the format &quot;allocated/idle/other/total&quot; Features:26\tFeatures available on the node, use : followed by a number which specifies the max number of characters printed for this column. sinfo prints max 20 characters by default per column Gres\tGeneric resource associated with the nodes GresUsed\tGeneric resource currently in use on the nodes MaxCPUsPerNode\tThe Max number of CPUs per node available to jobs in this partition Memory\tSize of memory per node in Megabytes NodeAI\tNumber of nodes by state in the format &quot;allocated/idle&quot; Nodes\tNumber of nodes NodeList\tList of node names Partition or PartitionName\tPartition name  ","version":"Next","tagName":"h2"},{"title":"squeue​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_main_commands/#squeue","content":" View information about jobs located on slurm scheduling queue.  man squeue # for more information   Options\tDescription--me\tPrints queued jobs for the current user --user\tPrints queued jobs under a specific user, or a comma list of users --job\tSpecify a comma seperated list of job IDs to display --help\tPrint a help message describing all options squeue  ","version":"Next","tagName":"h2"},{"title":"sacct​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_main_commands/#sacct","content":" Displays accounting data for all jobs and job steps in the Slurm job accounting log or Slurm database.  man sacct # for more information   Most popularly used format options are:  Options\tDescription--format\tComma separated list of fields. (use &quot;--helpformat&quot; for a list of available fields). NOTE: When using the format option for listing various fields you can put a %NUMBER afterwards to specify how many characters should be printed. e.g. format=name%30 will print 30 characters of field name right justified. A %-30 will print 30 characters left justified. --helpformat\tPrint a list of fields that can be specified with --format option  Some popular options for --format are:  Format\tDescriptionJobID\tThe identification number of the job or job step JobName\tThe name of the job or job step State\tDisplays the job status or state, such as COMPLETED, TIMEOUT, FAILED etc AllocCPUS\tNumber of CPUs allocated to the job Elapsed\tElapsed time for the job Start\tInitiation time for the job End\tTermination time for the job  ","version":"Next","tagName":"h2"},{"title":"scancel​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_main_commands/#scancel","content":" Used to signal jobs or job steps that are under the control of slurm. A signal in the sense, send a termination signal to cancel a job.  Options\tDescription--interactive\tInteractive mode. Confirm each job_id.step_id before performing the cancel operation --jobname\tRestrict the scancel operations to a provided job name --me\tCancel all your jobs scancel &lt;a_job_id&gt;\tCancel a job and all it's steps scancel &lt;a_job_id&gt;.&lt;step_id_a&gt; &lt;a_job_id&gt;.&lt;step_id_b&gt;\tOnly cancel steps a and b for a given job, but not the rest of the steps scancel &lt;JobID_ArrayID&gt;\tOnly cancel a array id of an job array ","version":"Next","tagName":"h2"},{"title":"Singularity with Conda","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/containers/singularity_with_conda/","content":"","keywords":"","version":"Next"},{"title":"What is Singularity?​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/containers/singularity_with_conda/#what-is-singularity","content":" Singularity is a free, cross-platform and open-source program that creates and executes containers on the HPC clusters. Containers are streamlined, virtualized environments for specific programs or packages. Singularity is an industry standard tool to utilize containers in HPC environments. Containers allow for the support of highly specific environments and further increase scientific reproducibility and portability. Using Singularity containers, researchers can work in the reproducible containerized environments of their choice can easily tailor them to their needs.  ","version":"Next","tagName":"h2"},{"title":"Using Singularity Overlays for Miniforge (Python & Julia)​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/containers/singularity_with_conda/#using-singularity-overlays-for-miniforge-python--julia","content":" ","version":"Next","tagName":"h2"},{"title":"Preinstallation Warning​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/containers/singularity_with_conda/#preinstallation-warning","content":" warning If you have initialized Conda in your base environment, your prompt on Greene may show something like: (base) [NETID@log-1 ~]$ then you must first comment out or remove this portion of your ~/.bashrc file: # &gt;&gt;&gt; conda initialize &gt;&gt;&gt; # !! Contents within this block are managed by 'conda init' !! __conda_setup=&quot;$('/share/apps/anaconda3/2020.07/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)&quot; if [ $? -eq 0 ]; then eval &quot;$__conda_setup&quot; else if [ -f &quot;/share/apps/anaconda3/2020.07/etc/profile.d/conda.sh&quot; ]; then . &quot;/share/apps/anaconda3/2020.07/etc/profile.d/conda.sh&quot; else export PATH=&quot;/share/apps/anaconda3/2020.07/bin:$PATH&quot; fi fi unset __conda_setup # &lt;&lt;&lt; conda initialize &lt;&lt;&lt; The above code automatically makes your environment look for the default shared installation of Conda on the cluster and will sabotage any attempts to install packages to a Singularity environment. Once removed or commented out, log out and back into the cluster for a fresh environment.  ","version":"Next","tagName":"h3"},{"title":"Miniforge Environment PyTorch Example​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/containers/singularity_with_conda/#miniforge-environment-pytorch-example","content":" Conda environments allow users to create customizable, portable work environments and dependencies to support specific packages or versions of software for research. Common conda distributions include Anaconda, Miniconda and Miniforge. Packages are available via &quot;channels&quot;. Popular channels include &quot;conda-forge&quot; and &quot;bioconda&quot;. In this tutorial we shall use Miniforge which sets &quot;conda-forge&quot; as the package channel. Traditional conda environments, however, also create a large number of files that can cut into quotas. To help reduce this issue, we suggest using Singularity, a container technology that is popular on HPC systems. Below is an example of how to create a pytorch environment using Singularity and Miniforge.  Create a directory for the environment  mkdir /scratch/&lt;NetID&gt;/pytorch-example cd /scratch/&lt;NetID&gt;/pytorch-example   Copy an appropriate gzipped overlay images from the overlay directory. You can browse available images to see available options  ls /scratch/work/public/overlay-fs-ext3   In this example we use overlay-15GB-500K.ext3.gz as it has enough available storage for most conda environments. It has 15GB free space inside and is able to hold 500K files You can use another size as needed.  cp -rp /scratch/work/public/overlay-fs-ext3/overlay-15GB-500K.ext3.gz . gunzip overlay-15GB-500K.ext3.gz   Choose a corresponding Singularity image. For this example we will use the following image  /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif   For Singularity image available on nyu HPC greene, please check the singularity images folder  /scratch/work/public/singularity/   For the most recent supported versions of PyTorch, please check the PyTorch website.  Launch the appropriate Singularity container in read/write mode (with the :rw flag)  singularity exec --overlay overlay-15GB-500K.ext3:rw /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif /bin/bash   The above starts a bash shell inside the referenced Singularity Container overlayed with the 15GB 500K you set up earlier. This creates the functional illusion of having a writable filesystem inside the typically read-only Singularity container.  Now, inside the container, download and install miniforge to /ext3/miniforge3  wget --no-check-certificate https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh bash Miniforge3-Linux-x86_64.sh -b -p /ext3/miniforge3 # rm Miniforge3-Linux-x86_64.sh # if you don't need this file any longer   Next, create a wrapper script /ext3/env.sh using a text editor, like nano.  touch /ext3/env.sh nano /ext3/env.sh   The wrapper script will activate your conda environment, to which you will be installing your packages and dependencies. The script should contain the following:  #!/bin/bash unset -f which source /ext3/miniforge3/etc/profile.d/conda.sh export PATH=/ext3/miniforge3/bin:$PATH export PYTHONPATH=/ext3/miniforge3/bin:$PATH   Activate your conda environment with the following:  source /ext3/env.sh   If you have the &quot;defaults&quot; channel enabled, please disable it with  conda config --remove channels defaults   Now that your environment is activated, you can update and install packages:  conda update -n base conda -y conda clean --all --yes conda install pip -y conda install ipykernel -y # Note: ipykernel is required to run as a kernel in the Open OnDemand Jupyter Notebooks   To confirm that your environment is appropriately referencing your Miniforge installation, try out the following:  unset -f which which conda # output: /ext3/miniforge3/bin/conda which python # output: /ext3/miniforge3/bin/python python --version # output: Python 3.8.5 which pip # output: /ext3/miniforge3/bin/pip exit # exit Singularity   Install packages​  You may now install packages into the environment with either the pip install or conda install commands.  First, start an interactive job with adequate compute and memory resources to install packages. The login nodes restrict memory to 2GB per user, which may cause some large packages to crash.  srun --cpus-per-task=2 --mem=10GB --time=04:00:00 --pty /bin/bash # wait to be assigned a node singularity exec --overlay overlay-15GB-500K.ext3:rw /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif /bin/bash source /ext3/env.sh # activate the environment   After it is running, you’ll be redirected to a compute node. From there, run singularity to setup on conda environment, same as you were doing on login node.  We will install PyTorch as an example:  pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116 pip3 install jupyter jupyterhub pandas matplotlib scipy scikit-learn scikit-image Pillow   For the latest versions of PyTorch please check the PyTorch website.  You can see the available space left on your image with the following commands:  find /ext3 | wc -l # output: should be something like 45445 du -sh /ext3 # output should be something like 4.9G /ext3   Now, exit the Singularity container and then rename the overlay image. Typing 'exit' and hitting enter will exit the Singularity container if you are currently inside it. You can tell if you're in a Singularity container because your prompt will be different, such as showing the prompt 'Singularity&gt;'  exit mv overlay-15GB-500K.ext3 my_pytorch.ext3   Test your PyTorch Singularity Image​  singularity exec --overlay /scratch/&lt;NetID&gt;/pytorch-example/my_pytorch.ext3:ro /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif /bin/bash -c 'source /ext3/env.sh; python -c &quot;import torch; print(torch.__file__); print(torch.__version__)&quot;' #output: /ext3/miniforge3/lib/python3.8/site-packages/torch/__init__.py #output: 1.8.0+cu111   note the end ':ro' addition at the end of the pytorch ext3 image starts the image in read-only mode. To add packages you will need to use ':rw' to launch it in read-write mode.  ","version":"Next","tagName":"h3"},{"title":"Using your Singularity Container in a SLURM Batch Job​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/containers/singularity_with_conda/#using-your-singularity-container-in-a-slurm-batch-job","content":" Below is an example script of how to call a python script, in this case torch-test.py, from a SLURM batch job using your new Singularity image  torch-test.py:  #!/bin/env python import torch print(torch.__file__) print(torch.__version__) # How many GPUs are there? print(torch.cuda.device_count()) # Get the name of the current GPU print(torch.cuda.get_device_name(torch.cuda.current_device())) # Is PyTorch using a GPU? print(torch.cuda.is_available())   Now we will write the SLURM job script, run-test.SBATCH, that will start our Singularity Image and call the torch-test.py script.  run-test.SBATCH:  #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=1:00:00 #SBATCH --mem=2GB #SBATCH --gres=gpu #SBATCH --job-name=torch module purge singularity exec --nv \\ --overlay /scratch/&lt;NetID&gt;/pytorch-example/my_pytorch.ext3:ro \\ /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif\\ /bin/bash -c &quot;source /ext3/env.sh; python torch-test.py&quot;   You will notice that the singularity exec command features the '--nv flag' - this flag is reguired to pass the CUDA drivers from a GPU to the Singularity container.  Run the run-test.SBATCH script  sbatch run-test.SBATCH   Check your SLURM output for results, an example is shown below  cat slurm-3752662.out # example output: # /ext3/miniforge3/lib/python3.8/site-packages/torch/__init__.py # 1.8.0+cu111 # 1 # Quadro RTX 8000 # True   ","version":"Next","tagName":"h3"},{"title":"Optional: Convert ext3 to a compressed, read-only squashfs filesystem​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/containers/singularity_with_conda/#optional-convert-ext3-to-a-compressed-read-only-squashfs-filesystem","content":" Singularity images can be compressed into read-only squashfs filesystems to conserve space in your environment. Use the following steps to convert your ext3 Singularity image into a smaller squashfs filesystem.  srun -N1 -c4 singularity exec --overlay my_pytorch.ext3:ro /scratch/work/public/singularity/centos-8.2.2004.sif mksquashfs /ext3 /scratch/&lt;NetID&gt;/pytorch-example/my_pytorch.sqf -keep-as-directory -processors 4   Here is an example of the amount of compression that can be realized by converting:  ls -ltrsh my_pytorch.* 5.5G -rw-r--r-- 1 wang wang 5.5G Mar 14 20:45 my_pytorch.ext3 2.2G -rw-r--r-- 1 wang wang 2.2G Mar 14 20:54 my_pytorch.sqf   Notice that it saves over 3GB of storage in this case, though your results may vary.  Use a squashFS Image for Running Jobs​  You can use squashFS images similarly to the ext3 images.  singularity exec --overlay /scratch/&lt;NetID&gt;/pytorch-example/my_pytorch.sqf:ro /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif /bin/bash -c 'source /ext3/env.sh; python -c &quot;import torch; print(torch.__file__); print(torch.__version__)&quot;' #example output: /ext3/miniforge3/lib/python3.8/site-packages/torch/__init__.py #example output: 1.8.0+cu111   Adding Packages to a Full ext3 or squashFS Image​  If the first ext3 overlay image runs out of space or you are using a squashFS conda enviorment, but need to install a new package inside, please copy another writable ext3 overlay image to work together.  Open the first image in read only mode  cp -rp /scratch/work/public/overlay-fs-ext3/overlay-2GB-100K.ext3.gz . gunzip overlay-2GB-100K.ext3.gz singularity exec --overlay overlay-2GB-100K.ext3 --overlay /scratch/&lt;NetID&gt;/pytorch-example/my_pytorch.ext3:ro /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu-22.04.2.sif /bin/bash source /ext3/env.sh pip install tensorboard   note Click here for information on how to configure your conda environment.  Please also keep in mind that once the overlay image is opened in default read-write mode, the file will be locked. You will not be able to open it from a new process. Once the overlay is opened either in read-write or read-only mode, it cannot be opened in RW mode from other processes either. For production jobs to run, the overlay image should be open in read-only mode. You can run many jobs at the same time as long as they are run in read-only mode. In this ways, it will protect the computation software environment, software packages are not allowed to change when there are jobs running.  ","version":"Next","tagName":"h3"},{"title":"Julia Singularity Image​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/containers/singularity_with_conda/#julia-singularity-image","content":" Singularity can be used to set up a Julia environment.  Create a directory for your julia work, such as /scratch/&lt;NetID&gt;/julia, and then change to your home directory. An example is shown below.  mkdir /home/&lt;NetID&gt;/julia cd /home/&lt;NetID&gt;/julia   Copy an overlay image, such as the 2GB 100K overlay, which generally has enough storage for Julia packages. Once copied, unzip to the same folder, rename to julia-pkgs.ext3  cp -rp /scratch/work/public/overlay-fs-ext3/overlay-2GB-100K.ext3.gz . gunzip overlay-2GB-100K.ext3.gz mv overlay-2GB-100K.ext3 julia-pkgs.ext3   Copy the following wrapper script in the directory  cp -rp /share/apps/utils/julia-setup/* .   Now launch writable Singularity overlay to install packages  module purge module load knitro/12.3.0 module load julia/1.5.3 ~/julia/my-julia-writable using Pkg Pkg.add(&quot;KNITRO&quot;) Pkg.add(&quot;JuMP&quot;)   Now exit from the container to launch a read only version to test (example below)  ~/julia/my-julia _ _ _ _(_)_ | Documentation: https://docs.julialang.org (_) | (_) (_) | _ _ _| |_ __ _ | Type &quot;?&quot; for help, &quot;]?&quot; for Pkg help. | | | | | | |/ _` | | | | |_| | | | (_| | | Version 1.5.3 (2020-11-09) _/ |\\__'_|_|_|\\__'_| | Official https://julialang.org/ release |__/ | julia&gt; using Pkg julia&gt; using JuMP, KNITRO julia&gt; m = Model(with_optimizer(KNITRO.Optimizer)) A JuMP Model Feasibility problem with: Variables: 0 Model mode: AUTOMATIC CachingOptimizer state: EMPTY_OPTIMIZER Solver name: Knitro julia&gt; @variable(m, x1 &gt;= 0) x1 julia&gt; @variable(m, x2 &gt;= 0) x2 julia&gt; @NLconstraint(m, x1*x2 == 0) x1 * x2 - 0.0 = 0 julia&gt; @NLobjective(m, Min, x1*(1-x2^2)) julia&gt; optimize!(m)   You can make the above code into a julia script to test batch jobs. Save the following as test-knitro.jl  using Pkg using JuMP, KNITRO m = Model(with_optimizer(KNITRO.Optimizer)) @variable(m, x1 &gt;= 0) @variable(m, x2 &gt;= 0) @NLconstraint(m, x1*x2 == 0) @NLobjective(m, Min, x1*(1-x2^2)) optimize!(m)   You can add additional packages with commands like the one below.  note Please do not install new packages when you have Julia jobs running, this may create issues with your Julia installation)  ~/julia/my-julia-writable -e 'using Pkg; Pkg.add([&quot;Calculus&quot;, &quot;LinearAlgebra&quot;])'   Run a SLURM job to test with the following sbatch command (e.g. julia-test.SBATCH)  #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=1:00:00 #SBATCH --mem=2GB #SBATCH --job-name=julia-test module purge module load julia/1.5.3 module load knitro/12.3.0 ~/julia/my-julia test-knitro.jl   Then run the command with the following:  sbatch julia-test.SBATCH   Once the job completes, check the SLURM output (example below)  cat slurm-1022969.out ======================================= Academic License (NOT FOR COMMERCIAL USE) Artelys Knitro 12.3.0 ======================================= Knitro presolve eliminated 0 variables and 0 constraints. datacheck: 0 hessian_no_f: 1 par_numthreads: 1 Problem Characteristics ( Presolved) ----------------------- Objective goal: Minimize Objective type: general Number of variables: 2 ( 2) bounded below only: 2 ( 2) bounded above only: 0 ( 0) bounded below and above: 0 ( 0) fixed: 0 ( 0) free: 0 ( 0) Number of constraints: 1 ( 1) linear equalities: 0 ( 0) quadratic equalities: 0 ( 0) gen. nonlinear equalities: 1 ( 1) linear one-sided inequalities: 0 ( 0) quadratic one-sided inequalities: 0 ( 0) gen. nonlinear one-sided inequalities: 0 ( 0) linear two-sided inequalities: 0 ( 0) quadratic two-sided inequalities: 0 ( 0) gen. nonlinear two-sided inequalities: 0 ( 0) Number of nonzeros in Jacobian: 2 ( 2) Number of nonzeros in Hessian: 3 ( 3) Knitro using the Interior-Point/Barrier Direct algorithm. Iter Objective FeasError OptError ||Step|| CGits -------- -------------- ---------- ---------- ---------- ------- 0 0.000000e+00 0.000e+00 WARNING: The initial point is a stationary point and only the first order optimality conditions have been verified. EXIT: Locally optimal solution found. Final Statistics ---------------- Final objective value = 0.00000000000000e+00 Final feasibility error (abs / rel) = 0.00e+00 / 0.00e+00 Final optimality error (abs / rel) = 0.00e+00 / 0.00e+00 # of iterations = 0 # of CG iterations = 0 # of function evaluations = 1 # of gradient evaluations = 1 # of Hessian evaluations = 0 Total program time (secs) = 1.03278 ( 1.014 CPU time) Time spent in evaluations (secs) = 0.00000 ===============================================================================   ","version":"Next","tagName":"h3"},{"title":"Using CentOS 8 for Julia (for Module Compatibility)​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/containers/singularity_with_conda/#using-centos-8-for-julia-for-module-compatibility","content":" Building on the previous Julia example, this will demonstrate how to set up a similar environment using the Singularity CentOS 8 image for additional customization. Using the CentOS 8 overlay allows for the loading of modules installed on Greene, such as Knitro 12.3.0  Copy overlay image  cp -rp /scratch/work/public/overlay-fs-ext3/overlay-2GB-100K.ext3.gz . gunzip overlay-2GB-100K.ext3.gz mv overlay-2GB-100K.ext3 julia-pkgs.ext3   The path in this example is /scratch/&lt;NetID&gt;/julia/julia-pkgs.ext3  To use modules installed into /share/apps you can make two directories  mkdir julia-compiled julia-logs   Now, in this example, the absoulte paths are as follows  /scratch/&lt;NetID&gt;/julia/julia-compiled /scratch/&lt;NetID&gt;/julia/julia-logs   To launch Singularity with overlay images in writable mode to install packages  singularity exec \\ --overlay /scratch/&lt;NetID&gt;/julia/julia-pkgs.ext3 \\ --bind /share/apps \\ --bind /scratch/&lt;NetID&gt;/julia/julia-compiled:/ext3/pkgs/compiled \\ --bind /scratch/&lt;NetID&gt;/julia/julia-logs:/ext3/pkgs/logs \\ /scratch/work/public/apps/greene/centos-8.2.2004.sif \\ /bin/bash   Implement a wrapper script /ext3/env.sh  #/bin/bash export JULIA_DEPOT_PATH=/ext3/pkgs # this changes the default installation path to the environment source /opt/apps/lmod/lmod/init/bash module use /share/apps/modulefiles module purge module load knitro/12.3.0 module load julia/1.5.3   Load julia via the wrapper script and check that it loads properly  source /ext3/env.sh which julia # example output: /share/apps/julia/1.5.3/bin/julia julia --version # example output: julia version 1.5.3   Run julia to install packages  julia &gt; using Pkg &gt; Pkg.add(&quot;KNITRO&quot;) &gt; Pkg.add(&quot;JuMP&quot;)   Set up a similar test script like the test-knitro.jl script above. Name it test.jl:  using Pkg using JuMP, KNITRO m = Model(with_optimizer(KNITRO.Optimizer)) @variable(m, x1 &gt;= 0) @variable(m, x2 &gt;= 0) @NLconstraint(m, x1*x2 == 0) @NLobjective(m, Min, x1*(1-x2^2)) optimize!(m)   Now implement a wrapper script named julia into ~/bin, the overlay image is in readonly mode  #!/bin/bash args='' for i in &quot;$@&quot;; do i=&quot;${i//\\\\/\\\\\\\\}&quot; args=&quot;$args \\&quot;${i//\\&quot;/\\\\\\&quot;}\\&quot;&quot; done module purge singularity exec \\ --overlay /scratch/&lt;NetID&gt;/julia/julia-pkgs.ext3:ro \\ --bind /share/apps \\ --bind /scratch/&lt;NetID&gt;/julia/julia-compiled:/ext3/pkgs/compiled \\ --bind /scratch/&lt;NetID&gt;/julia/julia-logs:/ext3/pkgs/logs \\ /scratch/work/public/apps/greene/centos-8.2.2004.sif \\ /bin/bash -c &quot; source /ext3/env.sh julia $args &quot;   Make the wrapper executable  chmod 755 ~/bin/julia   Test your installation with a SLURM job example. The following code has been put into a file called test-julia-centos.SBATCH  #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=1:00:00 #SBATCH --mem=2GB #SBATCH --job-name=julia-test module purge julia test.jl   Run the above with the following:  sbatch test-julia-centos.SBATCH   Read the output (example below)  cat slurm-764085.out   Installing New Julia Packages Later​  Implement another writable julia-writable with overlay image writable in order to install new Julia packages later  cd /home/&lt;NetID&gt;/bin cp -rp julia julia-writable #!/bin/bash args='' for i in &quot;$@&quot;; do i=&quot;${i//\\\\/\\\\\\\\}&quot; args=&quot;$args \\&quot;${i//\\&quot;/\\\\\\&quot;}\\&quot;&quot; done module purge singularity exec \\ --overlay /scratch/&lt;NetID&gt;/julia/julia-pkgs.ext3 \\ --bind /share/apps \\ --bind /scratch/&lt;NetID&gt;/julia/julia-compiled:/ext3/pkgs/compiled \\ --bind /scratch/&lt;NetID&gt;/julia/julia-logs:/ext3/pkgs/logs \\ /scratch/work/public/apps/greene/centos-8.2.2004.sif \\ /bin/bash -c &quot; source /ext3/env.sh julia $args &quot;   Check the writable image  which julia-writable #example output: ~/bin/julia-writable   Install packages to the writable image  julia-writable -e 'using Pkg; Pkg.add([&quot;Calculus&quot;, &quot;LinearAlgebra&quot;])'   If you do not need host packages installed in /share/apps, you can work with Singularity OS image  /scratch/work/public/singularity/ubuntu-20.04.1.sif   download Julia installation package from https://julialang-s3.julialang.org/bin/linux/x64/1.5/julia-1.5.3-linux-x86_64.tar.gz  install Julia to /ext3, setup PATH properly. It will be easy to move to other servers in future. ","version":"Next","tagName":"h3"},{"title":"Greene System Status","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/system_status/","content":"","keywords":"","version":"Next"},{"title":"Resources Allocation and Queue (AMD nodes not included)​","type":1,"pageTitle":"Greene System Status","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/system_status/#resources-allocation-and-queue-amd-nodes-not-included","content":"   ","version":"Next","tagName":"h2"},{"title":"Resource Allocation and Queue by partitions (AMD nodes not included)​","type":1,"pageTitle":"Greene System Status","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/system_status/#resource-allocation-and-queue-by-partitions-amd-nodes-not-included","content":"   ","version":"Next","tagName":"h2"},{"title":"AMD Nodes System Status​","type":1,"pageTitle":"Greene System Status","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/system_status/#amd-nodes-system-status","content":"   ","version":"Next","tagName":"h2"},{"title":"Storage System Status​","type":1,"pageTitle":"Greene System Status","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/system_status/#storage-system-status","content":" Below you may find data for the following file system mounts  GPFS file system: /home, /scratch, /archiveVAST file system: /vast   ","version":"Next","tagName":"h2"},{"title":"Transferring Cloud Storage Data with rclone","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/","content":"","keywords":"","version":"Next"},{"title":"Transferring files to and from Google Drive with RCLONE​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#transferring-files-to-and-from-google-drive-with-rclone","content":" Having access to Google Drive from the HPC environment provides an option to archive data and even share data with collaborators who have no access to the NYU HPC environment. Other options to archiving data include the HPC Archive file system and using Globus to share data with collaborators.  Access to Google Drive is provided by rclone - rsync for cloud storage - a command line program to sync files and directories to and from cloud storage systems such as Google Drive, Amazon Drive, S3, B2 etc. rclone is available on Greene cluster as a module, the module versions currently available (March 2025) are:  rclone/1.68.2rclone/1.66.0rclone/1.60.1  For more details on how to use rclone to sync files to Google Drive, please see: RClone documentation for Google Drive  rclone can be invoked in one of the three modes:  Copy mode to just copy new/changed filesSync (one way) mode to make a directory identicalCheck mode to check for file hash equality  Please try with these options:  rclone --transfers=32 --checkers=16 --drive-chunk-size=16384k --drive-upload-cutoff=16384k copy source:sourcepath dest:destpath   This option works great for file sizes 1Gb+ to 250GB. Keep in mind that there is a rate limiting of 2 files/sec for upload into Google Drive. Small file transfers don’t work that well. If you have many small jobs, please tar the parent directory of such folders and split the tar file into 100GB chunks and then uploads then into Google Drive.  ","version":"Next","tagName":"h2"},{"title":"rclone Configuration​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#rclone-configuration","content":" You need to configure rclone before you will be able to move files between the HPC Environment and Google Drive  There are specific instruction on the rclone web site and here is an example of the process for configuring rclone for use on Greene:  ","version":"Next","tagName":"h2"},{"title":"Step 1: Login to Greene:​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#step-1-login-to-greene","content":" Follow instructions to log into the Greene HPC cluster.  ","version":"Next","tagName":"h3"},{"title":"Step 2: Load the rclone module​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#step-2-load-the-rclone-module","content":" $ module load rclone/1.68.2   ","version":"Next","tagName":"h3"},{"title":"Step 3: Configure rclone​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#step-3-configure-rclone","content":" Configuring rclone and setting up remote access to your Google Drive, using the command:  $ rclone config   This will try to open the config files and you will see the below content:  You can select one of the options (here we show how to set up a new remote)  2025/03/14 16:39:23 NOTICE: Config file &quot;/Users/$USER/.config/rclone/rclone.conf&quot; not found - using defaults No remotes found, make a new one? n) New remote s) Set configuration password q) Quit config n/s/q&gt; n   Please enter n to create a new remote  Enter name for new remote. name&gt; nyu_google_drive   Please enter nyu_google_drive or something similar to name the new remote  Option Storage. Type of storage to configure. Choose a number from below, or type in your own value. 1 / 1Fichier \\ (fichier) 2 / Akamai NetStorage \\ (netstorage) 3 / Alias for an existing remote \\ (alias) 4 / Amazon S3 Compliant Storage Providers including AWS, Alibaba, ArvanCloud, Ceph, ChinaMobile, Cloudflare, DigitalOcean, Dreamhost, GCS, HuaweiOBS, IBMCOS, IDrive, IONOS, LyveCloud, Leviia, Liara, Linode, Magalu, Minio, Netease, Petabox, RackCorp, Rclone, Scaleway, SeaweedFS, StackPath, Storj, Synology, TencentCOS, Wasabi, Qiniu and others \\ (s3) 5 / Backblaze B2 \\ (b2) 6 / Better checksums for other remotes \\ (hasher) 7 / Box \\ (box) 8 / Cache a remote \\ (cache) 9 / Citrix Sharefile \\ (sharefile) 10 / Combine several remotes into one \\ (combine) 11 / Compress a remote \\ (compress) 12 / Dropbox \\ (dropbox) 13 / Encrypt/Decrypt a remote \\ (crypt) 14 / Enterprise File Fabric \\ (filefabric) 15 / FTP \\ (ftp) 16 / Files.com \\ (filescom) 17 / Gofile \\ (gofile) 18 / Google Cloud Storage (this is not Google Drive) \\ (google cloud storage) 19 / Google Drive \\ (drive) 20 / Google Photos \\ (google photos) 21 / HTTP \\ (http) 22 / Hadoop distributed file system \\ (hdfs) 23 / HiDrive \\ (hidrive) 24 / ImageKit.io \\ (imagekit) 25 / In memory object storage system. \\ (memory) 26 / Internet Archive \\ (internetarchive) 27 / Jottacloud \\ (jottacloud) 28 / Koofr, Digi Storage and other Koofr-compatible storage providers \\ (koofr) 29 / Linkbox \\ (linkbox) 30 / Local Disk \\ (local) 31 / Mail.ru Cloud \\ (mailru) 32 / Mega \\ (mega) 33 / Microsoft Azure Blob Storage \\ (azureblob) 34 / Microsoft Azure Files \\ (azurefiles) 35 / Microsoft OneDrive \\ (onedrive) 36 / OpenDrive \\ (opendrive) 37 / OpenStack Swift (Rackspace Cloud Files, Blomp Cloud Storage, Memset Memstore, OVH) \\ (swift) 38 / Oracle Cloud Infrastructure Object Storage \\ (oracleobjectstorage) 39 / Pcloud \\ (pcloud) 40 / PikPak \\ (pikpak) 41 / Pixeldrain Filesystem \\ (pixeldrain) 42 / Proton Drive \\ (protondrive) 43 / Put.io \\ (putio) 44 / QingCloud Object Storage \\ (qingstor) 45 / Quatrix by Maytech \\ (quatrix) 46 / SMB / CIFS \\ (smb) 47 / SSH/SFTP \\ (sftp) 48 / Sia Decentralized Cloud \\ (sia) 49 / Storj Decentralized Cloud Storage \\ (storj) 50 / Sugarsync \\ (sugarsync) 51 / Transparently chunk/split large files \\ (chunker) 52 / Uloz.to \\ (ulozto) 53 / Union merges the contents of several upstream fs \\ (union) 54 / Uptobox \\ (uptobox) 55 / WebDAV \\ (webdav) 56 / Yandex Disk \\ (yandex) 57 / Zoho \\ (zoho) 58 / premiumize.me \\ (premiumizeme) 59 / seafile \\ (seafile) Storage&gt; 19   Please enter the number that corresponds to Google Drive. In the example above it is 19.  Option client_id. Google Application Client Id Setting your own is recommended. See https://rclone.org/drive/#making-your-own-client-id for how to create your own. If you leave this blank, it will use an internal key which is low performance. Enter a value. Press Enter to leave empty. client_id&gt;   Please leave this field blank and hit enter/return.  Option client_secret. OAuth Client Secret. Leave blank normally. Enter a value. Press Enter to leave empty. client_secret&gt;   Please leave this field blank and hit enter/return.  Scope that rclone should use when requesting access from drive. Enter a string value. Press Enter for the default (&quot;&quot;). Choose a number from below, or type in your own value 1 / Full access all files, excluding Application Data Folder. \\ &quot;drive&quot; 2 / Read-only access to file metadata and file contents. \\ &quot;drive.readonly&quot; / Access to files created by rclone only. 3 | These are visible in the drive website. | File authorization is revoked when the user deauthorizes the app. \\ &quot;drive.file&quot; / Allows read and write access to the Application Data folder. 4 | This is not visible in the drive website. \\ &quot;drive.appfolder&quot; / Allows read-only access to file metadata but 5 | does not allow any access to read or download file content. \\ &quot;drive.metadata.readonly&quot; scope&gt; 1   Please enter 1 or the number corresponding to the option for 'full access' if the options above have changed.  Service Account Credentials JSON file path Leave blank normally. Needed only if you want use SA instead of interactive login. Leading `~` will be expanded in the file name as will environment variables such as `${RCLONE_CONFIG_DIR}`. Enter a string value. Press Enter for the default (&quot;&quot;). service_account_file&gt;   Please leave blank and hit enter/return.  Edit advanced config? (y/n) y) Yes n) No (default) y/n&gt; n   Please enter 'n'.  Use web browser to automatically authenticate rclone with remote? * Say Y if the machine running rclone has a web browser you can use * Say N if running rclone on a (remote) machine without web browser access If not sure try Y. If Y failed, try N. y) Yes (default) n) No y/n&gt;   Please enter 'n'  Option config_token. For this to work, you will need rclone available on a machine that has a web browser available. For more help and alternate methods see: https://rclone.org/remote_setup/ Execute the following on the machine with the web browser (same rclone version recommended): rclone authorize &quot;drive&quot; &quot;eyJzY29wZSI6ImRyaXZlIn0&quot; Then paste the result. Enter a value. config_token&gt;   As instructed, please copy the line in the output you get that is similar to:rclone authorize &quot;drive&quot; &quot;eyJzY29wZSI6ImRyaXZlIn0&quot;and paste that into a terminal on your local system and press enter/return.  That should open a web browser that will have you select your NYU account and will verify that you would like to give rclone access to your Google Drive. It will then provide you with a long code:  user@ITS tmp % rclone authorize &quot;drive&quot; &quot;eysi39xv82pmJzY29XZlIn0&quot; 2025/03/21 11:12:29 NOTICE: Make sure your Redirect URL is set to &quot;http://127.0.0.1:53682/&quot; in your custom config. 2025/03/21 11:12:29 NOTICE: If your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth?state=4c5yNwx6EsFWeise83svie 2025/03/21 11:12:29 NOTICE: Log in and authorize rclone for access 2025/03/21 11:12:29 NOTICE: Waiting for code... 2025/03/21 11:12:39 NOTICE: Got code Paste the following into your remote machine ---&gt; code removed &lt;---End paste   Please copy this code and paste it in as a response to the config_token&gt; prompt.  Configure this as a team drive? y) Yes n) No (default) y/n&gt; n   Please enter 'n'.  -------------------- [remote1] type = drive scope = drive token = {&quot;access_token&quot;:&quot;, removed &quot;} -------------------- y) Yes this is OK (default) e) Edit this remote d) Delete this remote y/e/d&gt; y   Please enter 'y'.  Current remotes: Name Type ==== ==== remote1 drive e) Edit existing remote n) New remote d) Delete remote r) Rename remote c) Copy remote s) Set configuration password q) Quit config e/n/d/r/c/s/q&gt; q   Please enter 'q' and we're done with configuration.  ","version":"Next","tagName":"h3"},{"title":"Step 4: Transfer​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#step-4-transfer","content":" warning Please be sure to perform data transters on a data transfer node (DTN). It can degrade performace for other users to perform transfers on other types of nodes. For more information please see Data Transfers  Sample commands:  $ rclone lsd nyu_google_drive:   Transfer files to Google Drive, using the command below:  $ rclone copy &lt;source_folder or file&gt; &lt;remote_name&gt;:&lt;name_of_folder_on_gdrive&gt;   It looks something like below:  $ rclone copy /home/user1 nyu_google_drive:backup_home_user1   ","version":"Next","tagName":"h3"},{"title":"Step 5: Confirmation​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#step-5-confirmation","content":" The files are transferred and you can find the files on your Google Drive.  note Rclone only copies new files or files different from the already existing files on Google Drive. ","version":"Next","tagName":"h3"},{"title":"Slurm: Tutorial","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_tutorial/","content":"","keywords":"","version":"Next"},{"title":"Introduction to High Performance Computing Clusters​","type":1,"pageTitle":"Slurm: Tutorial","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_tutorial/#introduction-to-high-performance-computing-clusters","content":" In a High Performance Computing Cluster, such as the NYU-IT HPC Greene cluster, there are hundreds of computing nodes interconnected by high-speed networks.  Linux operating system ( in our case Red Hat Enterprise Linux) runs on each of the nodes individually. The resources are shared among many users for their technical or scientific computing purposes.  Slurm is a cluster software layer built on top of the interconnected nodes, aiming at orchestrating the nodes' computing activites, so that the cluster could be viewed as a unified, enhanced and scalable computing system by its users.  In NYU HPC clusters the users coming from many departments with various disciplines and subjects, with their own computing projects, impose on us very diverse requirements regarding hardware, software resources, and processing parallelism. Users submit jobs, which compete for computing resources.  The Slurm software system is a resource manager and a job scheduler, which is designed to allocate resources and schedule jobs. Slurm is an open-source software, with a large user community, and has been installed on many top 500 supercomputers.  This tutorial assumes you have a NYU HPC account. If not, you may find the steps to apply for an account on the Getting and renewing an account page. It also assumes you are comfortable with Linux command-line environment. To learn about linux please read our Linux Tutorial. Please review the Hardware Specs page for more information on Greene's hardware specifications.  ","version":"Next","tagName":"h2"},{"title":"Slurm Commands​","type":1,"pageTitle":"Slurm: Tutorial","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_tutorial/#slurm-commands","content":" For an overview of useful Slurm commands, please read Slurm Main Commands page before continuing the tutorial.  ","version":"Next","tagName":"h2"},{"title":"Software and Environment Modules​","type":1,"pageTitle":"Slurm: Tutorial","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_tutorial/#software-and-environment-modules","content":" Lmod, an Environment Module system, is a tool for managing multiple versions and configurations of software packages and is used by many HPC centers around the world. With Environment Modules, software packages are installed away from the base system directories, and for each pacakge, an associated modulefile describes what must be altered in a user's shell environment - such as the $PATH environment variable - in order to use the software package. The modulefile also describes dependencies and conflicts between this software package and other packages and versions.  To use a given software package, you load the corresponding module. Unloading the module afterwards cleanly undoes the changes that loading the modules made to your environment, thus freeing you to use other software packages that might have conflicted with the first one.  Below is a list of modules and their associated functions:  module load &lt;module-name&gt; : loads a module For example : module load python module unload &lt;module-name&gt; : unloads a module For example : module unload python module show &lt;module-name&gt; : see exactly what effect loading a module will have module purge : remove all loaded modules from your environment module whatis &lt;module-name&gt; : Find out more about a software package module list : check which modules are currently loaded in your environment module avail : check what software packages are available module help &lt;module-name&gt; : A module file may include more detailed help for software package  ","version":"Next","tagName":"h2"},{"title":"Batch Job Example​","type":1,"pageTitle":"Slurm: Tutorial","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_tutorial/#batch-job-example","content":" Batch jobs require a script file for the SLURM scheduler to interpret and execute. The SBATCH file contains both commands specific for SLURM to interpret as well as programs for it to execute. Below is a simple example of a batch job to run a Stata do file, the file is named myscript.sbatch :  #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=5:00:00 #SBATCH --mem=2GB #SBATCH --job-name=myTest #SBATCH --mail-type=END #SBATCH --mail-user=bob.smith@nyu.edu #SBATCH --output=slurm_%j.out module purge module load stata/14.2 RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*} mkdir -p $RUNDIR DATADIR=$SCRATCH/my_project/data cd $RUNDIR stata -b do $DATADIR/data_0706.do   Below we will break down each line of the SBATCH script. More options can be found on the SchedMD website.  ## This tells the shell how to execute the script #!/bin/bash ## The #SBATCH lines are read by SLURM for options. ## In the lines below we ask for a single node, ## one task for that node, and one cpu for each task. #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 ## Time is the estimated time to complete, in this case 5 hours. #SBATCH --time=5:00:00 ## We expect no more than 2GB of memory to be needed #SBATCH --mem=2GB ## To make them easier to track, ## it's best to name jobs something recognizable. ## You can then use the name to look up reports with tools like squeue. #SBATCH --job-name=myTest ## These lines manage mail alerts for when the job ends, ## and who the email should be sent to. #SBATCH --mail-type=END #SBATCH --mail-user=bob.smith@nyu.edu ## This places the standard output and standard error into the same file, ## in this case slurm_&lt;job_id&gt;.out #SBATCH --output=slurm_%j.out ## First we ensure a clean environment by purging the current one module purge ## Load the desired software, in this case stata 14.2 module load stata/14.2 ## Create a unique directory to run the job in. RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*} mkdir -p $RUNDIR ## Set an environment variable for where the data is stored. DATADIR=$SCRATCH/my_project/data ## Change directories to the unique run directory cd $RUNDIR ## Execute the desired Stata do file script stata -b do $DATADIR/data_0706.do   You can submit the job with the following command:  sbatch myscript.sbatch   The command will result in the job queuing as it awaits resources to become available (which varies on the number of other jobs being run on the cluster and the resources requested). You can see the status of yor jobs with the following command:  squeue --me   NOTE: Calling just squeue without passing the --me option will display all users' job queue status by default  Lastly, you can read the output of your job in the slurm-&lt;job_ID&gt;.out file produced by running your job. This is where logs regarding the execution of your job can be found, including errors or system messages. You can print the contents to the screen from the directory containing the output file with the following command:  cat slurm-&lt;job_ID&gt;.out   ","version":"Next","tagName":"h2"},{"title":"Interactive Job Example​","type":1,"pageTitle":"Slurm: Tutorial","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_tutorial/#interactive-job-example","content":" While the majority of the jobs on the cluster are submitted with the sbatch command, and executed in the background, there are also methods to run applications interactively through the srun command. Interactive jobs allow the users to enter commands and data on the command line (or in a graphical interface), providing an experience similar to working on a desktop or laptop. Examples of common interactive tasks are:  Editing files Compiling and debugging code Exploring data, to obtain a rough idea of characteristics on the topic Getting graphical windows to run visualization Running software tools in interactive sessions  Interactive jobs also help avoid issues with the login nodes. If you are working on a login node and your job is too IO intensive, it may be removed without notice. Running interactive jobs on compute nodes does not impact many users and in addition provides access to resources that are not available on the login nodes, such as interactive access to GPUs, high memory, exclusive access to all the resources of a compute node, etc.  In the srun example below, through --pty /bin/bash we request allocation of a pseudo terminal (pty) and start a bash shell session. By default the resource allocated is a single CPU core and 2GB memory for 1 hour time limit.  srun --pty /bin/bash   To request resources such as 4 CPU cores, 4 GB memory for 2 hours of maximum duration, you can add the following arguments:  srun --cpus-per-task=4 --time=2:00:00 --mem=4GB --pty /bin/bash   Similarly, to request one GPU card, 3 GB memory for a duration of 1.5 hours you can pass the following arguments to srun:  srun --time=1:30:00 --mem=3GB --gres=gpu:1 --pty /bin/bash   Once the job begins you will notice your prompt change, for example:  [mdw303@log-3 ~]$ srun --pty /bin/bash srun: job 7864254 queued and waiting for resources srun: job 7864254 has been allocated resources [mdw303@cs080 ~]$   You can see above that the prompt changed from log-3 ( one of the login nodes ) to cs080 ( one of the compute nodes ), meaning we have created a pseudo terminal and logged in with a bash shell on a compute node from our login node.  You can now load modules, software and run them interactively on the compute node having the resources ( CPUs, memory, GPUs etc ) that we asked for.  Below outlines the steps to start an interactive session and launch R:  [sk6404@log-1 ~]$ srun --cpus-per-task=1 --pty /bin/bash [sk6404@cs022 ~]$ module purge [sk6404@cs022 ~]$ module load r/intel/4.0.3 [sk6404@cs022 ~]$ module list Currently Loaded Modules: 1) intel/19.1.2 2) r/intel/4.0.3 [sk6404@cs022 ~]$ R R version 4.0.3 (2020-10-10) -- &quot;Bunny-Wunnies Freak Out&quot; Copyright (C) 2020 The R Foundation for Statistical Computing Platform: x86_64-centos-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. &gt; 5 + 10 [1] 15 &gt; q() Save workspace image? [y/n/c]: n [sk6404@cs022 ~]$ exit exit [sk6404@log-1 ~]$   ","version":"Next","tagName":"h2"},{"title":"MPI Job Example​","type":1,"pageTitle":"Slurm: Tutorial","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_tutorial/#mpi-job-example","content":" MPI stands for &quot;Message Passing Interface&quot; and is managed by a program, such as OpenMPI, to coordinate code and resources across the HPC cluster for your job to run workloads in parallel. You may have heard of HPC sometimes referred to as &quot;parallel computing&quot; because the ability to run many processes simultaneously - aka in parallel - is how the best efficiencies can be realized on the cluster. Users interested in MPI generally must compile the program they want to run using an MPI compiler.  Greene supports many MPI compilers. We'll be using the OpenMPI GCC compiler in this tutorial. It can be loaded as a module:  module load openmpi/gcc/4.1.6   Below we will illustrate an example of how to compile a C script for MPI. Copy this into your working directory as hellompi.c :  #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;mpi.h&gt; int main(int argc, char *argv[], char *envp[]) { int numprocs, rank, namelen; char processor_name[MPI_MAX_PROCESSOR_NAME]; MPI_Init(&amp;argc, &amp;argv); MPI_Comm_size(MPI_COMM_WORLD, &amp;numprocs); MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); MPI_Get_processor_name(processor_name, &amp;namelen); printf(&quot;Process %d on %s out of %d\\n&quot;, rank, processor_name, numprocs); MPI_Finalize(); }   Once copied into your directory, load OpenMPI and compile it with the following:  module load openmpi/gcc/4.1.6 mpicc hellompi.c -o hellompi   Next, create a hellompi.sbatch script:  #!/bin/bash #SBATCH --nodes=4 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=1:00:00 #SBATCH --mem=2GB #SBATCH --job-name=hellompi #SBATCH --output=hellompi.out # Load the default OpenMPI module. module purge module load openmpi/intel/4.1.1 # Run the hellompi program with mpirun. The -n flag is not required; # mpirun will automatically figure out the best configuration from the # Slurm environment variables. mpirun ./hellompi   Run the job with the following command:  sbatch hellompi.sbatch   After the job runs, cat the hellompi.out output file to see that your processes ran on multiple nodes. There may be some errors, but your output should contain something like the following, indicating the process was run in parallel on multiple nodes:  Process 0 on cs265.nyu.cluster out of 4 Process 1 on cs266.nyu.cluster out of 4 Process 2 on cs267.nyu.cluster out of 4 Process 3 on cs268.nyu.cluster out of 4   ","version":"Next","tagName":"h2"},{"title":"GPU Job Example​","type":1,"pageTitle":"Slurm: Tutorial","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_tutorial/#gpu-job-example","content":" To request one GPU card, use SBATCH directives in job script:  #SBATCH --gres=gpu:1   To request a specific card type, use e.g. --gres=gpu:v100:1. The card types currently available are:  NVIDIA RTX 8000V100A100 NVIDIA 8358A100 NVIDIA 8380H100 NVIDIA AMD MI100MI250  As an example, let's submit an Amber job. Amber is a molecular dynamics software package. The recipe is:  mkdir -p /scratch/$USER/myambertest cd /scratch/$USER/myambertest cp /share/apps/Tutorials/slurm/example/amberGPU/* . sbatch run-amber.s   From the tutorial example directory we copy over Amber input data files &quot;inpcrd&quot;, &quot;prmtop&quot; and &quot;mdin&quot;, and the job script file &quot;run-amber.s&quot;.  NOTE: At the time of writing this you may need to update the run-amber.s script to load amber version 20.06, rather than the default 16.06.  The content of the job script &quot;run-amber.s&quot; should be as follows:  #!/bin/bash #SBATCH --job-name=myAmberJobGPU #SBATCH --nodes=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:30:00 #SBATCH --mem=3GB #SBATCH --gres=gpu:1 module purge module load amber/openmpi/intel/20.06 cd /scratch/$USER/myambertest pmemd.cuda -O   The demo Amber job should take ~2 minutes to finish once it starts running. When the job is done, several output files are generated. Check the one named mdout, which has a section most relevant here:  |--------------------- INFORMATION ---------------------- | GPU (CUDA) Version of PMEMD in use: NVIDIA GPU IN USE. | Version 16.0.0 | | 02/25/2016 [......] |------------------- GPU DEVICE INFO -------------------- | | CUDA_VISIBLE_DEVICES: 0 | CUDA Capable Devices Detected: 1 | CUDA Device ID in use: 0 | CUDA Device Name: Tesla V100 | CUDA Device Global Mem Size: 11439 MB | CUDA Device Num Multiprocessors: 13 | CUDA Device Core Freq: 0.82 GHz | |--------------------------------------------------------   ","version":"Next","tagName":"h2"},{"title":"Array Job Example​","type":1,"pageTitle":"Slurm: Tutorial","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_tutorial/#array-job-example","content":" Using job array you may submit many similar jobs with almost identical job requirement. This reduces loads on both users and the scheduler system. Job arrays can only be used in batch jobs. Usually the only requirement difference among jobs in a job array is the input file or files. Please follow the recipe below to try the example. There are 5 input files named sample-1.txt, sample-2.txt to sample-5.txt in sequential order. Running one command sbatch run-jobarray.s, you submit 5 jobs to process each of these input files individually. Run the following commands to create the directory and submit the array job:  mkdir -p /scratch/$USER/myjarraytest cd /scratch/$USER/myjarraytest cp /share/apps/Tutorials/slurm/example/jobarray/* . ls   OUTPUT: run-jobarray.s sample-1.txt sample-2.txt sample-3.txt sample-4.txt sample-5.txt wordcount.py  sbatch --array=1-5 run-jobarray.s   The content of the job script run-jobarray.s is copied below:  #!/bin/bash #SBATCH --job-name=myJobarrayTest #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --array=1-5 # this creates an array! #SBATCH --time=5:00 #SBATCH --mem=1GB #SBATCH --output=wordcounts_%A_%a.out #SBATCH --error=wordcounts_%A_%a.err module purge module load python/intel/3.8.6 cd /scratch/$USER/myjarraytest python2 wordcount.py sample-$SLURM_ARRAY_TASK_ID.txt   Job array submissions create an environment variable called SLURM_ARRAY_TASK_ID, which is unique for each job in the array job. It is usually embedded somewhere so that at a job running time it's unique value is incorporated into producing a proper file name. Also as shown above: two additional options %A and %a, denoting the job ID and the task ID (i.e. job array index) respectively, are available for specifying a job's stdout, and stderr file names. ","version":"Next","tagName":"h2"},{"title":"Slurm: Submitting Jobs","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/","content":"","keywords":"","version":"Next"},{"title":"Batch vs Interactive Jobs​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#batch-vs-interactive-jobs","content":" HPC workloads are usually better suited to batch processing than interactive working.A batch job is sent to the system when submitted with an sbatch command.The working pattern we are all familiar with is interactive - where we type ( or click ) something interactively, and the computer performs the associated action. Then we type ( or click ) the next thing.Comments at the start of the script, which match a special pattern ( #SBATCH ) are read as Slurm options.  ","version":"Next","tagName":"h2"},{"title":"The trouble with interactive environments​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#the-trouble-with-interactive-environments","content":" There is a reason why GUIs are less common in HPC environments: point-and-click is necessarily interactive. In HPC environments (as we'll see in session 3) work is scheduled in order to allow exclusive use of the shared resources. On a busy system there may be several hours wait between when you submit a job and when the resources become available, so a reliance on user interaction is not viable. In Unix, commands need not be run interactively at the prompt, you can write a sequence of commands into a file to be run as a script, either manually (for sequences you find yourself repeating frequently) or by another program such as the batch system.  tip The job might not start immediately, and might take hours or days, so we prefer a batch approach: Plan the sequence of commands which will perform the actions we need Write them into a script. You can now run the script interactively, which is a great way to save effort if i frequently use the same workflow, or ... Submit the script to a batch system, to run on dedicated resources when they become available.  ","version":"Next","tagName":"h3"},{"title":"Where does the output go ?​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#where-does-the-output-go-","content":" The batch system writes stdout and stderr from a job to a file named for example &quot;slurm-12345.out&quot; You can change either stdout or stderr using sbatch options. While a job is running, it caches the stdout an stderr in the job working directory.You can use redirection to send output of a specific command into a file.  ","version":"Next","tagName":"h3"},{"title":"Writing and Submitting a Job​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#writing-and-submitting-a-job","content":" There are two aspects to a batch job script:  A set of SBATCH directives describing the resources required and other information about the job.The script itself, comprised of commands to set up and perform the computations without additional user interaction.  ","version":"Next","tagName":"h3"},{"title":"A simple example​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#a-simple-example","content":" A typical batch script on an NYU HPC cluster looks something like these two examples:  #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=5:00:00 #SBATCH --mem=2GB #SBATCH --job-name=myTest #SBATCH --mail-type=END #SBATCH --mail-user=bob.smith@nyu.edu #SBATCH --output=slurm_%j.out #SBATCH --error=slurm_%j.err module purge module load stata/17.0 RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*} mkdir -p $RUNDIR DATADIR=$SCRATCH/my_project/data cd $RUNDIR stata -b do $DATADIR/data_0706.do   #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=5:00:00 #SBATCH --mem=2GB #SBATCH --job-name=myTest #SBATCH --mail-type=END #SBATCH --mail-user=bob.smith@nyu.edu #SBATCH --output=slurm_%j.out #SBATCH --error=slurm_%j.err module purge SRCDIR=$HOME/my_project/code RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*} mkdir -p $RUNDIR cd $SLURM_SUBMIT_DIR cp my_input_params.inp $RUNDIR cd $RUNDIR module load fftw/intel/3.3.9 $SRCDIR/my_exec.exe &lt; my_input_params.inp   We'll work through them more closely in a moment. You submit the job with sbatch:  [NetID@log-1 ~]$ sbatch myscript.sh   And monitor it's progress with:  [NetID@log-1 ~]$ squeue -u $USER   What just happened ? Here's an annotated version of the first script:  #!/bin/bash # This line tells the shell how to execute this script, and is unrelated # to SLURM. # at the beginning of the script, lines beginning with &quot;#SBATCH&quot; are read by # SLURM and used to set queueing options. You can comment out a SBATCH # directive with a second leading #, eg: ##SBATCH --nodes=1 # we need 1 node, will launch a maximum of one task and use one cpu for the task: #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 # we expect the job to finish within 5 hours. If it takes longer than 5 # hours, SLURM can kill it: #SBATCH --time=5:00:00 # we expect the job to use no more than 2GB of memory: #SBATCH --mem=2GB # we want the job to be named &quot;myTest&quot; rather than something generated # from the script name. This will affect the name of the job as reported # by squeue: #SBATCH --job-name=myTest # when the job ends, send me an email at this email address. #SBATCH --mail-type=END #SBATCH --mail-user=bob.smith@nyu.edu # both standard output and standard error are directed to the same file. # It will be placed in the directory I submitted the job from and will # have a name like slurm_12345.out #SBATCH --output=slurm_%j.out # once the first non-comment, non-SBATCH-directive line is encountered, SLURM # stops looking for SBATCH directives. The remainder of the script is executed # as a normal Unix shell script # first we ensure a clean running environment: module purge # and load the module for the software we are using: module load stata/17.0 # next we create a unique directory to run this job in. We will record its # name in the shell variable &quot;RUNDIR&quot;, for better readability. # SLURM sets SLURM_JOB_ID to the job id, ${SLURM_JOB_ID/.*} expands to the job # id up to the first '.' We make the run directory in our area under $SCRATCH, because at NYU HPC # $SCRATCH is configured for the disk space and speed required by HPC jobs. RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*} mkdir $RUNDIR # we will be reading data in from somewhere, so define that too: DATADIR=$SCRATCH/my_project/data # the script will have started running in $HOME, so we need to move into the # unique directory we just created cd $RUNDIR # now start the Stata job: stata -b do $DATADIR/data_0706.do The second script has the same SBATCH directives, but this time we are using code we compiled ourselves. Starting after the SBATCH directives: # first we ensure a clean running environment: module purge # and ensure we can find the executable: SRCDIR=$HOME/my_project/code # create a unique directory to run this job in, as per the script above RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*} mkdir $RUNDIR # By default the script will have started running in the directory we ran sbatch from. # Let's assume our input file is in the same directory in this example. SLURM # sets some environment variables with information about the job, including # SLURM_SUBMIT_DIR which is the directory the job was submitted from. So lets # go there and copy the input file to the run directory on /scratch: cd $SLURM_SUBMIT_DIR cp my_input_params.inp $RUNDIR # go to the run directory to begin the run: cd $RUNDIR # load whatever environment modules the executable needs: module load fftw/intel/3.3.9 # run the executable (sending the contents of my_input_params.inp to stdin) $SRCDIR/my_exec.exe &lt; my_input_params.inp   ","version":"Next","tagName":"h3"},{"title":"Batch Jobs​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#batch-jobs","content":" Jobs are submitted with the sbatch command:  sbatch options job-script   The options tell SLURM information about the job, such as what resources will be needed. These can be specified in the job-script as SBATCH directives, or on the command line as options, or both  note When SBATCH options are provided in both the script and the command line, the command line options take precedence should the two contradict each other.  For each option there is a corresponding SBATCH directive with the syntax:  #SBATCH option   For example, you can specify that a job needs 2 nodes and 4 cores on each node ( by default one CPU core per task ) on each node by adding to the script the directive:  #!/bin/bash #SBATCH --nodes=2 #SBATCH --ntasks-per-node=4   or as a command-line option to sbatch when you submit the job:  [NetID@log-1 ~]$ sbatch --nodes=2 --ntasks-per-node=4 my_script.sh   ","version":"Next","tagName":"h2"},{"title":"Options to manage job output​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#options-to-manage-job-output","content":" -J jobname Give the job a name. The default is the filename of the job script. Within the job, $SLURM_JOB_NAME expands to the job name. -o path/for/stdout Send stdout to path/for/stdout. The default filename is slurm-${SLURM_JOB_ID}.out, e.g. slurm-12345.out, in the directory from which the job was submitted. -e path/for/stderr Send stderr to path/for/stderr. --mail-user=my_email_address@nyu.edu Send mail to my_email_address@nyu.edu when certain events occur. --mail-type=type Valid type values are NONE, BEGIN, END, FAIL, REQUIRE, ALL.  ","version":"Next","tagName":"h3"},{"title":"Options to set the job environment:​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#options-to-set-the-job-environment","content":" --export=VAR1,VAR2=&quot;some value&quot;,VAR3 Pass variables to the job, either with a specific value (the VAR= form) or from the submitting environment ( without &quot;=&quot; ) --get-user-env[=timeout][mode] Run something like &quot;su - &lt;username&gt; -c /usr/bin/env&quot; and parse the output. Default timeout is 8 seconds. The mode value can be &quot;S&quot;, or &quot;L&quot; in which case &quot;su&quot; is executed with &quot;-&quot; option.  ","version":"Next","tagName":"h3"},{"title":"Options to request compute resources​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#options-to-request-compute-resources","content":" -t, --time=time Set a limit on the total run time. Acceptable formats include &quot;minutes&quot;, &quot;minutes:seconds&quot;, &quot;hours:minutes:seconds&quot;, &quot;days-hours&quot;, &quot;days-hours:minutes&quot; and &quot;days-hours:minutes:seconds&quot;. --mem=MB Maximum memory per node the job will need in MegaBytes --mem-per-cpu=MB Memory required per allocated CPU in MegaBytes -N, --node=num Number of nodes are required. Default is 1 node.-n, --ntasks=numMaximum number tasks will be launched. Default is one task per node.--ntasks-per-node=ntasksRequest that ntasks be invoked on each node.-c, --cpus-per-task=ncpusRequire ncpus number of CPU cores per task. Without this option, allocate one core per task. Requesting the resources you need, as accurately as possible, allows your job to be started at the earliest opportunity as well as helping the system to schedule work efficiently to everyone's benefit.  ","version":"Next","tagName":"h3"},{"title":"Options for running interactively on the compute nodes with srun​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#options-for-running-interactively-on-the-compute-nodes-with-srun","content":" -nnum Specify the number of tasks to run, eg. -n4. Default is one CPU core per task. Don't just submit the job, but also wait for it to start and connect stdout, stderrand stdin to the current terminal. -ttime Request job running duration, eg. -t1:30:00 --mem=MB Specify the real memory required per node in MegaBytes, eg. --mem=4000--ptyExecute the first task in pseudo terminal mode, eg. --pty /bin/bash, to start a bash command shell --x11 Enable X forwarding, so programs using a GUI can be used during the session (provided you have X forwarding to your workstation set up)To leave an interactive batch session, type exit at the command prompt  ","version":"Next","tagName":"h3"},{"title":"Options for delaying starting a job​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#options-for-delaying-starting-a-job","content":" --begin=time Delay starting this job until after the specified date and time, eg. --begin=9:42:00, to start the job at 9:42:00 am -d, --dependency=dependency_list (More info here https://slurm.schedmd.com/sbatch.html)Example 1 --dependency=afterok:12345, to delay starting this job until the job 12345 has completed successfully Example 2 Let us say job 1 uses sbatch file job1.sh, and job 2 uses job2.shInside the batch file of the second job (job2.sh) add#SBATCH --dependency=afterok:$job1Start the first job and get id of the jobjob1=$(echo $(sbatch job1.sh) | grep -Eo &quot;[0-9]+&quot;)Schedule second jobs to start when the first one endssbatch job2.sh  ","version":"Next","tagName":"h3"},{"title":"Options for running many similar jobs​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#options-for-running-many-similar-jobs","content":" -a, --array=indexes Submit an array of jobs with array ids as specified. Array ids can be specified as a numerical range, a comma-seperated list of numbers, or as some combination of the two. Each job instance will have an environment variable SLURM_ARRAY_JOB_ID and SLURM_ARRAY_TASK_ID. For example:--array=1-11, to start an array job with index from 1 to 11--array=1-7:2, to submit an array job with index step size 2--array=1-9%4, to submit an array job with simultaneously running job elements set to 4The srun command is similar to pbsdsh. It launches tasks on allocated resources  ","version":"Next","tagName":"h3"},{"title":"R Job Example​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#r-job-example","content":" Create a directory and an example R script  [NetID@log-1 ~]$ mkdir /scratch/$USER/examples [NetID@log-1 ~]$ cd /scratch/$USER/examples   Create example.R inside the examples directory:  df &lt;- data.frame(x=c(1,2,3,1), y=c(7,19,2,2)) df indices &lt;- order(df$x) order(df$x) df[indices,] df[rev(order(df$y)),]   Create the following SBATCH script named run-R.sbatch :  #!/bin/bash # #SBATCH --job-name=RTest #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --mem=2GB #SBATCH --time=01:00:00 module purge module load r/intel/4.0.4 cd /scratch/$USER/examples R --no-save -q -f example.R &gt; example.out 2&gt;&amp;1   Run the job using sbatch.  [NetID@log-1 ~]$ sbatch run-R.sbatch   ","version":"Next","tagName":"h2"},{"title":"Array Jobs​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#array-jobs","content":" Using job array you may submit many similar jobs with almost identical job requirement. This reduces loads on both shoulders of users and the scheduler system. Job array can only be used in batch jobs. Usually the only requirement difference among jobs in a job array is the input file or files.  Please follow the recipe below to try the example. There are 5 input files named sample-1.txt, sample-2.txt to sample-5.txt in sequential order. Running one command sbatch --array=1-5 run-jobarray.s, you submit 5 jobs to process each of these input files individually.  Prepare the data before submitting an array job:  [NetID@log-1 ~]$ mkdir -p /scratch/$USER/myjarraytest [NetID@log-1 ~]$ cd /scratch/$USER/myjarraytest [NetID@log-1 ~]$ cp /share/apps/Tutorials/slurm/example/jobarray/* . [NetID@log-1 ~]$ ls   Submit the array job:  [NetID@log-1 ~]$ sbatch --array=1-5 run-jobarray.s   The content of the job script run-jobarray.s is copied below:  #!/bin/bash #SBATCH --job-name=myJobarrayTest #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --time=5:00 #SBATCH --mem=1GB #SBATCH --output=wordcounts_%A_%a.out #SBATCH --error=wordcounts_%A_%a.err module purge module load python/intel/3.8.6 cd /scratch/$USER/myjarraytest python wordcount.py sample-$SLURM_ARRAY_TASK_ID.txt   Job array submission introduces an environment variable, SLURM_ARRAY_TASK_ID, which is unique for each job array job. It is usually embedded somewhere so that when a job runs, its unique value is incorporated into producing a proper file name.  Also as shown above: two additional options %A and %a, denoting the job ID and the task ID ( i.e. job array index ) respectively, are available for specifying a job's stdout, and stderr file names.  ","version":"Next","tagName":"h2"},{"title":"More examples​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#more-examples","content":" You can find more examples in the slurm jobarray examples directory:  /scratch/work/public/examples/slurm/jobarry/   ","version":"Next","tagName":"h2"},{"title":"GPU Jobs​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#gpu-jobs","content":" To request one GPU card, use SBATCH directive in job script:  #SBATCH --gres=gpu:1   To request a specific card type, use eg. --gres=gpu:v100:1. As an example, let's submit an Amber job. Amber is a molecular dynamics software package. The recipe is:  [NetID@log-1 ~]$ mkdir -p /scratch/$USER/myambertest [NetID@log-1 ~]$ cd /scratch/$USER/myambertest [NetID@log-1 ~]$ cp /share/apps/Tutorials/slurm/example/amberGPU/* . [NetID@log-1 ~]$ sbatch run-amber.s   There are three NVIDIA GPU types and one AMD GPU type that can be used.  warning AMD GPUs require code to be compatible with ROCM drivers, not CUDA  To request NVIDIA GPUs  RTX8000  #SBATCH --gres=gpu:rtx8000:1   V100  #SBATCH --gres=gpu:v100:1   A100  #SBATCH --gres=gpu:a100:1   H100  #SBATCH --gres=gpu:h100:1   To request AMD GPUs  MI100  #SBATCH --gres=gpu:mi100:1   MI250  #SBATCH --gres=gpu:mi250:1   From the tutorial example directory we copy over Amber input data files &quot;inpcrd&quot;, &quot;prmtop&quot; and &quot;mdin&quot;, and the job script file &quot;run-amber.s&quot;. The content of the job script &quot;run-amber.s&quot; is:  #!/bin/bash # #SBATCH --job-name=myAmberJobGPU #SBATCH --nodes=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:30:00 #SBATCH --mem=3GB #SBATCH --gres=gpu:1 module purge module load amber/openmpi/intel/20.06 cd /scratch/$USER/myambertest pmemd.cuda -O   The demo Amber job should take ~2 minutes to finish once it starts runnning. When the job is done, several output files are generated. Check the one named &quot;mdout&quot;, which has a section most relevant here:  |--------------------- INFORMATION ---------------------- | GPU (CUDA) Version of PMEMD in use: NVIDIA GPU IN USE. | Version 16.0.0 | | 02/25/2016 [......] |------------------- GPU DEVICE INFO -------------------- | | CUDA_VISIBLE_DEVICES: 0 | CUDA Capable Devices Detected: 1 | CUDA Device ID in use: 0 | CUDA Device Name: Tesla K80 | CUDA Device Global Mem Size: 11439 MB | CUDA Device Num Multiprocessors: 13 | CUDA Device Core Freq: 0.82 GHz | |--------------------------------------------------------   ","version":"Next","tagName":"h2"},{"title":"Interactive Jobs​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#interactive-jobs","content":" ","version":"Next","tagName":"h2"},{"title":"Bash Sessions​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#bash-sessions","content":" The majority of the jobs on the NYU HPC cluster are submitted with the sbatch command, and executed in the background. These jobs' steps and workflows are predefined by users, and their executions are driven by the scheduler system.  There are cases when users need to run applications interactively ( interactive jobs ). Interactive jobs allow the users to enter commands and data on the command line (or in a graphical interface ), providing an experience similar to working on a desktop or laptop.  Examples of common interactive tasks are:  Editing files Compiling and debugging code Exploring data, to insights A graphical window to run visualization etc  To support interactive use in a batch environment, Slurm allows for interactive batch jobs.  warning Please do not run interactive jobs on the HPC Login nodes. Login nodes of the HPC cluster are shared between many users so running interactive jobs that require significant computing and IO resources on the login nodes will impact many users. For this reason running compute and IO intensive interactive jobs on the HPC login nodes is not allowed. Such jobs may be removed without notice!  tip Instead of running interactive jobs on Login nodes, users can run interactive jobs on the HPC Compute nodes using SLURM's srun utility. Running interactive jobs on compute nodes does not impact many users and in addition provides access to resources that are not available on the login nodes, such as interactive access to GPUs, high memory, exclusive access to all the resources of a compute node, etc. note There is no partition on the HPC cluster that has been reserved for Interactive jobs.  ","version":"Next","tagName":"h3"},{"title":"Start an Interactive Job​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#start-an-interactive-job","content":" When you start an interactive batch job the command prompt is not immediately returned. Instead, you wait until the resource is available when the prompt is returned and you are on a compute node and in a batch job - much like the process of logging in to a host with ssh. To end the session, type 'exit', again just like the process of logging in and out with ssh.  [NetID@log-1 ~]$ srun --pty /bin/bash srun: job 58699789 queued and waiting for resources srun: job 58699789 has been allocated resources [NetID@cm034 ~]$ hostname cm034.hpc.nyu.edu   To use any GUI-based program within the interactive batch session you will need to extend X forwarding with the --x11 option. This of course still relies on you having X forwarding at your login session. To test if you have X forwarding running, you can try running the gnuplot test as shown:  [NetID@log-1 ~]$ module load gnuplot/gcc/5.4.1 [NetID@log-1 ~]$ gnuplot gnuplot&gt; test   If a window opens on your display with a gnuplot test window, you know that Xforwarding is working. Please see SSH Tunneling and X11 Forwarding for details.  ","version":"Next","tagName":"h3"},{"title":"Request Resources​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#request-resources","content":" You can request resources for an interactive batch session just as you would for any other job, for example to request 4 processors with 4GB memory for 2 hours.  If you do not request resources you will get the default settings. If after some directory navigation in your interactive session, you can jump back to the directory you submitted from with:  [NetID@cm034 ~]$ cd $SLURM_SUBMIT_DIR   ","version":"Next","tagName":"h3"},{"title":"Interactive Job Options​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#interactive-job-options","content":" (Don't just submit the job, but also wait for it to start and connect stdout, stderr and stdin to the current terminal)  -nnum Specify the number of tasks to run, eg. -n4. Default is one CPU core per task -ttime Request job running duration, eg. -t1:30:00 --mem=MB Specify the real memory required per node in MegaBytes, eg. --mem=4000--ptyExecute the first task in pseudo terminal mode, eg. --pty /bin/bash, to start a bash command shell --gres=gpu:N To request N number of GPUs --x11 Enable X forwarding, so programs using a GUI can be used during the session (provided you have X forwarding to your workstation set up)To leave an interactive batch session, type exit at the command prompt  Certain tasks need user iteraction - such as debugging and some GUI-based applications. However the HPC clusters rely on batch job scheduling to efficiently allocate resources. Interactive batch jobs allow these apparently conflicting requirements to be met.  ","version":"Next","tagName":"h3"},{"title":"Interactive Bash Job Examples​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#interactive-bash-job-examples","content":" Example (Without x11 forwarding)  Through srun SLURM provides rich command line options for users to request resources from the cluster, to allow interactive jobs. Please see some examples and short accompanying explanations in the code block below, which should cover many of the use cases.  In the srun examples below, through --pty /bin/bash we request to start bash command shell session in pseudo terminal by default the resource allocated is single CPU core and 2GB memory for 1 hour:  [NetID@log-1 ~]$ srun --pty /bin/bash   To request 4 CPU cores, 4 GB memory, and 2 hour running duration:  [NetID@log-1 ~]$ srun -c4 -t2:00:00 --mem=4000 --pty /bin/bash   To request one GPU card, 3 GB memory, and 1.5 hour running duration:  [NetID@log-1 ~]$ srun -t1:30:00 --mem=3000 --gres=gpu:1 --pty /bin/bash   Example (x11 forwarding)  In srun there is an option &quot;–x11&quot;, which enables X forwarding, so programs using a GUI can be used during an interactive session (provided you have X forwarding to your workstation set up).  To request computing resources, and export x11 display on allocated node(s)  [NetID@log-1 ~]$ srun --x11 -c4 -t2:00:00 --mem=4000 --pty /bin/bash [NetID@cm034 ~]$ module load gnuplot/gcc/5.4.1 [NetID@cm034 ~]$ gnuplot gnuplot&gt; test   To request GPU card etc, and export x11 display:  [NetID@log-1 ~]$ srun --x11 -t1:30:00 --mem=3000 --gres=gpu:1 --pty /bin/bash   ","version":"Next","tagName":"h3"},{"title":"R interactive job​","type":1,"pageTitle":"Slurm: Submitting Jobs","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/submitting_jobs/slurm_submitting_jobs/#r-interactive-job","content":" The following example shows how to work with Interactive R session on a compute node:  [NetID@log-1 ~]$ srun -c 1 --pty /bin/bash [NetID@cm034 ~]$ module purge [NetID@cm034 ~]$ module list No modules loaded [NetID@cm034 ~]$ module load r/gcc/4.4.0 [NetID@cm034 ~]$ module list Currently Loaded Modules: 1) r/intel/4.4.0 [NetID@cm034 ~]$ R R version 4.4.0 (2024-04-24) -- &quot;Puppy Cup&quot; Copyright (C) 2024 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. &gt; 5 + 10 [1] 15 &gt; 6 ** 2 [1] 36 &gt; tan(45) [1] 1.619775 &gt; &gt; q() Save workspace image? [y/n/c]: n [NetID@cm034 ~]$ exit exit [NetID@log-1 ~]$  ","version":"Next","tagName":"h3"},{"title":"Tools and Software","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/intro/","content":"Tools and Software","keywords":"","version":"Next"},{"title":"Python Packages with Virtual Environments","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/python_packages_with_virtual_environments/","content":"","keywords":"","version":"Next"},{"title":"Create project directory and load Python module​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#create-project-directory-and-load-python-module","content":" ## Find python version you need module avail python ## created directory for your project and cd there mkdir /scratch/$USER/my_project cd /scratch/$USER/my_project ## load python module (different versions available) module load python/intel/3.8.6   ","version":"Next","tagName":"h2"},{"title":"Automatic deletion of your files​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#automatic-deletion-of-your-files","content":" This page describes the installation of packages on /scratch. One has to remember, though, that files stored in the HPC scratch file system are subject to the HPC Scratch old file purging policy: Files on the /scratch file system that have not been accessed for 60 or more days will be purged (read more).  Thus you can consider the following options  Reinstall your packages if some of the files get deleted You can do this manuallyYou can do this automatically. For example, within a workflow of a pipeline software like Nextflow Pay for &quot;Research Project Space&quot; - read more hereUse Singularity and install packages within a corresponding overlay file - read more here  ","version":"Next","tagName":"h2"},{"title":"Create virtual environment​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#create-virtual-environment","content":" It is advisable to create private environment inside the project directory. This boosts reproducibility and does not use space in /home/$USER  ","version":"Next","tagName":"h2"},{"title":"virtualenv​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#virtualenv","content":" virtualenv is a tool to create isolated Python environments  Since Python 3.3, a subset of it has been integrated into the standard library under the venv module.  Note: you may need to install virtualenv first, if it is not yet installed (instructions)  Now create new virtual environment in current directory  EmptyORinherit all packages from those installed on HPC already (and available in PATH after you load python module)  ## created directory for your project and cd there mkdir /scratch/$USER/my_project cd /scratch/$USER/my_project ## Create an EMPTY virtual environment virtualenv venv ## Create an virtual environment that inherits system packages virtualenv venv --system-site-packages   ","version":"Next","tagName":"h3"},{"title":"venv​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#venv","content":" venv is package shipped with Python3. It provides subset of options available in virtualenv tool (link).  python3 -m venv venv   Create new virtual environment in current directory  EmptyORinherit all packages from those installed on HPC already (and available in PATH after you load python module)  ## created directory for your project and cd there mkdir /scratch/$USER/my_project cd /scratch/$USER/my_project ##EMPTY ## (use venv command to create environment called &quot;venv&quot;) python3 -m venv venv ## Inhering all packages python3 -m venv venv --system-site-packages   ","version":"Next","tagName":"h3"},{"title":"Install packages. Keep things reproducible​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#install-packages-keep-things-reproducible","content":" ## activate source venv/bin/activate ## install packages pip install &lt;package you need&gt; ## If package was inherited, but you want to install it in your own env anyway pip install &lt;package you need&gt; --ignore-installed ## export list of packages (to report together with paper and/or to reproduce environment on another computer) pip freeze &gt; requirements.txt ## restore pip install -r requirements.txt   ","version":"Next","tagName":"h2"},{"title":"Close an Activated Virtual Environment​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#close-an-activated-virtual-environment","content":" If you have activated a virtual environment, you can exit it with the following command:  deactivate   ","version":"Next","tagName":"h2"},{"title":"Use with sbatch​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#use-with-sbatch","content":" When you use this env in sbatch script, please use  module purge; source venv/bin/activate; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; python python_script.py   If you use mpi  mpiexec bash -c &quot;module purge; source venv/bin/activate; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; python python_script.py&quot;  ","version":"Next","tagName":"h2"},{"title":"R Packages with renv","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/r_packages_with_renv/","content":"","keywords":"","version":"Next"},{"title":"Setup​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/r_packages_with_renv/#setup","content":" Say your R code is in directory /scratch/$USER/projects/project1  cd /scratch/$USER/projects/project1 module purge module load r/gcc/4.4.0 R   ","version":"Next","tagName":"h2"},{"title":"Automatic deletion of your files​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/r_packages_with_renv/#automatic-deletion-of-your-files","content":" This page describes the installation of packages on /scratch. One has to remember, though, that files stored in the HPC scratch file system are subject to the HPC Scratch old file purging policy: Files on the /scratch file system that have not been accessed for 60 or more days will be purged (read more).  Thus you can consider the following options:  Reinstall your packages if some of the files get deleted You can do this manuallyYou can do this automatically. For example, within a workflow of a pipeline software like Nextflow Pay for &quot;Research Project Space&quot; - read more hereUse Singularity and install packages within a corresponding overlay file - read more here  ","version":"Next","tagName":"h3"},{"title":"Cache directory setup​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/r_packages_with_renv/#cache-directory-setup","content":" By default, renv will cache package installation files to your home directory (most likely either in ~/.local/share/renv or ~/.cache/R/renv/ or something similar).  To avoid filling up your home directory, we advise to set up path to alternative cache directory (otherwise your home directory may fill up quickly)  Create directory  mkdir -p /scratch/$USER/.cache/R/renv   Create a file in you project directory named .Renviron and put the following in in the file. It is the R project directory (/scratch/$USER/projects/project1) in this example.  RENV_PATHS_ROOT=/scratch/&lt;USER_NETID&gt;/.cache/R/renv   ","version":"Next","tagName":"h3"},{"title":"Init renv​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/r_packages_with_renv/#init-renv","content":" The renv package is already installed for module r/gcc/4.4.0. You need to install it yourself if you use other R module version  ## Do this if renv is not available (already installed for r/gcc/4.4.0) # install.packages(&quot;renv&quot;) ## By default this will install renv package into a sub-directory within your home directory ## init renv in project's directory renv::init(&quot;.&quot;)   Restart R for renv to take effect. Once you start R, your renv environment will be loaded automatically.  R version 4.4.0 (2024-04-24) -- &quot;Puppy Cup&quot; ... * Project '/scratch/$USER/projects/project1' loaded. [renv 1.0.7]   ","version":"Next","tagName":"h3"},{"title":"Check​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/r_packages_with_renv/#check","content":" You can check your library paths with the .libPaths() command  &gt; .libPaths() [1] &quot;/scratch/$USER/projects/project1/renv/library/R-4.1/x86_64-pc-linux-gnu&quot;   You can check where the cache is set with the following:  renv::paths$cache() #[1] &quot;/home/$USER/.cache/R/renv/cache/v5/R-4.1/x86_64-pc-linux-gnu&quot;   ","version":"Next","tagName":"h3"},{"title":"Add/remove, etc. packages​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/r_packages_with_renv/#addremove-etc-packages","content":" Install a package, such as reshape2. Below we can see it is not yet installed and then install it.  R library(reshape2) Error in library(reshape2) : there is no package called ‘reshape2’ install.packages(&quot;reshape2&quot;)   note You must be in the project1 directory for renv to load your project and the appropriate personal environment that you have created. If you want to copy your environment to a new location, use the bundle package, as shown below.  Test R file  print(&quot;hello&quot;) renv::restore() library(reshape2) names(airquality) &lt;- tolower(names(airquality)) head(airquality) aql &lt;- melt(airquality) print(&quot;hello again&quot;)   For testing run it as  srun --pty /bin/bash Rscript test.R   note Your .Rprofile file will include line source(&quot;renv/activate.R&quot;)  The file will output the following:  [1] &quot;hello&quot; * The library is already synchronized with the lockfile. ozone solar.r wind temp month day 1 41 190 7.4 67 5 1 2 36 118 8.0 72 5 2 3 12 149 12.6 74 5 3 4 18 313 11.5 62 5 4 5 NA NA 14.3 56 5 5 6 28 NA 14.9 66 5 6 No id variables; using all as measure variables [1] &quot;hello again&quot;   ","version":"Next","tagName":"h3"},{"title":"Clean up​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/r_packages_with_renv/#clean-up","content":" Keep only the packages that you use in this particular project (not all the packages available on the system)  R # launch R renv::clean() # remove packages not recorded in the lockfile from the target library   ","version":"Next","tagName":"h3"},{"title":"Recommended Workflow​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/r_packages_with_renv/#recommended-workflow","content":" The general workflow when working with renv is:  Call renv::init() to initialize a new project-local environment with a private R library,Work in the project as normal, installing and removing new R packages as they are needed in the project,Call renv::snapshot() to save the state of the project library to the lockfile (called renv.lock), By default, renv::snapshot() will only capture packages listed in your R scripts within the R Project. For more options read the renv::snapshot() documentation. Continue working on your project, installing and updating R packages as needed.If needed, call renv::restore() to revert to the previous state as encoded in the lockfile if your attempts to update packages introduced some new problems.  The renv::init() function attempts to ensure the newly-created project library includes all R packages currently used by the project. It does this by crawling R files within the project for dependencies with the renv::dependencies() function. The discovered packages are then installed into the project library with the renv::hydrate() function, which will also attempt to save time by copying packages from your user library (rather than reinstalling from CRAN) as appropriate.  Calling renv::init() will also write out the infrastructure necessary to automatically load and use the private library for new R sessions launched from the project root directory. This is accomplished by creating (or amending) a project-local .Rprofile with the necessary code to load the project when the R session is started.  If you’d like to initialize a project without attempting dependency discovery and installation – that is, you’d prefer to manually install the packages your project requires on your own – you can use renv::init(bare = TRUE) to initialize a project with an empty project library.  ","version":"Next","tagName":"h3"},{"title":"Use with sbatch​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/r_packages_with_renv/#use-with-sbatch","content":" When you launch a job with sbatch, R will check if there is renv directory, and if renv is on it will pick up packages, installed using renv in the current directory.  Before you launch sbatch job, you need to make sure your project renv environment is ready, as outlined in the previous section.  ","version":"Next","tagName":"h2"},{"title":"Store and Share your R Project's R version and R Package Versions​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/r_packages_with_renv/#store-and-share-your-r-projects-r-version-and-r-package-versions","content":" Reproduce Environment​  If you already have file renv.lock or bundle file skip step 1  In the original location (your own laptop for example) go to project directory and execute (Make sure the whole path to project directory and names of your script files don't have empty spaces!)  R # install.packages(&quot;renv&quot;) ## if needed renv::init() renv::snapshot()   Take file renv.lock and copy it to a new location for the projectAt the new location - restore environment: go to directory of the project and execute. (Make sure version of R is the same)  ## Reproduce environment module purge module load r/gcc/4.4.0 R renv::restore() renv::init()   renv will install/compile what is needed on any system (Linux, Windows, etc). You can share your code with other researchers no matter what system they use. However, you should be careful that the same version of R is used between systems.  What to save/publish/commit with Git​  In order to have your work reproducible by you or/and others, save and/or commit your code in git, please including  renv.lock (which lists all packages and versions that you use including the version of R)  Migrating from Packrat​  The renv package has replaced the now deprecated Packrat package. The renv::migrate() function makes it possible to migrate projects from Packrat to renv. See the ?migrate documentation for more details. In essence, calling renv::migrate(&quot;&lt;project path&gt;&quot;) will be enough to migrate the Packrat library and lockfile such that they can then be used by renv.  ","version":"Next","tagName":"h3"},{"title":"Useful links​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/r_packages_with_renv/#useful-links","content":" https://rstudio.github.io/renv/articles/renv.html ","version":"Next","tagName":"h3"},{"title":"Conda Environments (Python, R)","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/conda_environments/","content":"","keywords":"","version":"Next"},{"title":"What is Conda?​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/conda_environments/#what-is-conda","content":" Package, dependency and environment management for any language—Python, R, Ruby, Lua, Scala, Java, JavaScript, C/ C++, FORTRAN, and more.  Please find more information at this link: https://docs.conda.io/en/latest/  Conda provides a great way to install packages that are already compiled, so you don't need to go over the long compilation process. If a package you need is not available, you can install it (and compile it when needed) using pip (Python) or install.packages (R).  note Reproducibility: One of the ways to ensure the reproducibility of your results is to have an independent conda environment in the directory of each project (one of the options shown below). This will also keep conda environment files away from your /home/$USER directory.  ","version":"Next","tagName":"h2"},{"title":"Advantages/disadvantages of using Conda​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/conda_environments/#advantagesdisadvantages-of-using-conda","content":" ","version":"Next","tagName":"h2"},{"title":"Advantages​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/conda_environments/#advantages","content":" A lot of pre-compiled packages (fast and easy to install)Note for Python: pip also offers pre-compiled packages (wheels). List can be found here https://pythonwheels.com. However, Conda has a significantly larger number of pre-compiled packages.Compiled packages use highly efficient Intel Math Kernel Library (MKL) library  ","version":"Next","tagName":"h3"},{"title":"Disadvantages​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/conda_environments/#disadvantages","content":" Conda does not take advantage of packages already installed in the system (while virtualenv and venv do)As you will see below, you may need to do additional steps to keep track of all installed packages (including those installed by pip and/or install.packages)  ","version":"Next","tagName":"h3"},{"title":"Initializing Conda​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/conda_environments/#initializing-conda","content":" Load anaconda module  module purge module load anaconda3/2020.07   Conda init can create problems with package installation, so we suggest using source activate instead of conda activate, even though conda activate is considered a best practice by the Anaconda developers.  ","version":"Next","tagName":"h2"},{"title":"Automatic deletion of your files​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/conda_environments/#automatic-deletion-of-your-files","content":" This page describes the installation of packages on /scratch. One has to remember, though, that files stored in the HPC scratch file system are subject to the HPC Scratch old file purging policy: Files on the /scratch file system that have not been accessed for 60 or more days will be purged (read more about Data Management.  Thus you can consider the following options  Reinstall your packages if some of the files get deleted You can do this manuallyYou can do this automatically. For example, within a workflow of a pipeline software like Nextflow Pay for &quot;Research Project Space&quot; - read more hereUse Singularity and install packages within a corresponding overlay file - read more here  ","version":"Next","tagName":"h3"},{"title":"Python​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/conda_environments/#python","content":" Load anaconda module  module purge module load anaconda3/2020.07   tip Keep your program/project in /scratch and create conda environment using '-p' parameter. This will keep all the files inside the project's directory, instead of putting in in your /home/$USER  conda create -p ./penv python=3 ## environment will be created in project directory conda activate ./penv   Also, you need to create a symbolic link, so conda will download files for packages to be installed into scratch, not your home directory.  mkdir /home/&lt;NetID&gt;/.conda mkdir /scratch/&lt;NetID&gt;/conda_pkgs ln -s /scratch/&lt;NetID&gt;/conda_pkgs /home/&lt;NetID&gt;/.conda/pkgs   Install pre-compiled packages available in conda  conda install -c anaconda pandas   Other packages may be installed (and compiled when needed) using pip  pip install &lt;package_name&gt;   note Conda and packages install by default to ~/.local/lib/python&lt;version&gt;  If you did use 'pip install --user' to install some packages (without conda or other virtual environment), they will be available in ~/.local/lib/python&lt;version&gt;  warning The primary takeaway: Let say you have tornado v.6 installed in ~/.local/lib/python&lt;version&gt;, and tornado v.5 installed by conda install. When you will do conda activate you will have tornado v.6 available!! Not v.5!! (this behaviour is the same for packages installed by to ~/.local/lib/python&lt;version&gt; before or after you create your conda environment) pip freeze will give v.6 conda list will give v.5 Solution To overcome this, do export PYTHONNOUSERSITE=True after conda activate  ","version":"Next","tagName":"h2"},{"title":"R​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/conda_environments/#r","content":" Load anaconda module  module load anaconda3/2020.07   tip Keep your program/project in /scratch and create conda environment using '-p' parameter. This will keep all the files inside the project's directory, instead of putting them in your /home/$USER  conda create -p ./renv r=3.5 ## environment will be created in project directory ## OR conda create -c conda-forge -p ./penv r-base=3.6.3 ## environment will be created in project directory conda activate ./renv   Install pre-compiled packages available in conda:  https://docs.anaconda.com/anaconda/packages/r-language-pkg-docs/  conda install -c r r-dplyr   Other packages may be installed (and compiled) using install.packages()  install.packages(&quot;&lt;package_name&gt;&quot;)   ","version":"Next","tagName":"h2"},{"title":"Reproducibility​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/conda_environments/#reproducibility","content":" Packages installed only using conda  Save a list of packages (so you are able to report environment in publication, and to restore/reproduce env on another machine at any time)  # save conda list --export &gt; requirements.txt # restore conda create -p ./penv --file requirements.txt   note This will not list packages installed by pip or install.packages()  If you installed extra packages using pip (Python)  In this you can use  export PYTHONNOUSERSITE=True ## to ingnore packages in ~/.local/lib/python&lt;version&gt; # save conda list --export &gt; conda_requirements.txt pip freeze &gt; pip_requirements.txt # restore conda create -p ./penv --file conda_requirements.txt pip install -r pip_requirements.txt   note Alternatively, you can use conda env export &gt; all_requirements.txt, which will save both: packages installed by conda and by pip.  However, this may fail if your conda environment is created as a sub-directory of your project's directory (which we recommend)  Installed extra packages using install.packages? (R)  Usecase: You need packages not availalbe in conda channels, and want to use install.packages.  Command conda list --export will not include packages installed by &quot;install.packages&quot;. So, do not use conda install at all. To have reproducibility in this case you need to use Conda and renv together, as described below  Conda + pakcrat: specific version of R and install.packages (R)  use conda to install version of R you needdo not use 'conda install' at alluse renvinstall all the packages using install.packagesuse renv as described here to keep track of the environment  In order for conda + renv to work, you need to add following steps:  After you activate conda AND before loading R export R_RENV_DEFAULT_LIBPATHS=&lt;path_to_project_directory&gt;/renv/lib/x86_64-conda_cos6-linux-gnu/&lt;version&gt;/ Start R and execute .libPaths(c(.libPaths(), Sys.getenv(&quot;R_RENV_SYSTEM_LIBRARY&quot;)))   ","version":"Next","tagName":"h2"},{"title":"Use conda env in a batch script​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/conda_environments/#use-conda-env-in-a-batch-script","content":" The part of the batch script which will call the command shall look like (replace &lt;path_to_env&gt; to an appropriate value)  ","version":"Next","tagName":"h2"},{"title":"Python​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/conda_environments/#python-1","content":" Single node​  #!/bin/bash #SBATCH --job-name=test #SBATCH --nodes=1 #SBATCH --cpus-per-task=1 #SBATCH --ntasks-per-node=4 #SBATCH --mem=8GB #SBATCH --time=1:00:00 module purge; module load anaconda3/2020.07; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; source /share/apps/anaconda3/2020.07/etc/profile.d/conda.sh; conda activate ./penv; export PATH=./penv/bin:$PATH; python python_script.py   Multiple nodes, using MPI​  mpiexec --mca bash -c &quot;module purge; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; module load anaconda3/2020.07; source /share/apps/anaconda3/2020.07/etc/profile.d/conda.sh; conda activate ./penv; export PATH=./penv/bin:$PATH; python python_script.py&quot;   ","version":"Next","tagName":"h3"},{"title":"R (conda packages only)​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/conda_environments/#r-conda-packages-only","content":" #!/bin/bash #SBATCH --job-name=test #SBATCH --nodes=1 #SBATCH --cpus-per-task=1 #SBATCH --ntasks-per-node=4 #SBATCH --mem=8GB #SBATCH --time=1:00:00 module purge; module load anaconda3/2020.07; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; source /share/apps/anaconda3/2020.07/etc/profile.d/conda.sh; conda activate ./renv; export PATH=./renv/bin:$PATH; Rscript r_script.R   Multiple nodes, using MPI​  mpiexec --mca bash -c &quot;module purge; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; module load anaconda3/2020.07; source /share/apps/anaconda3/2020.07/etc/profile.d/conda.sh; conda activate ./renv; export PATH=./renv/bin:$PATH; Rscript r_script.R&quot;   ","version":"Next","tagName":"h3"},{"title":"R (conda with renv combination)​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/conda_environments/#r-conda-with-renv-combination","content":" In this case, when you use sbatch you would activate conda in sbatch script, and R script will pickup packages installed in renv  module purge module load anaconda3/2020.07 source /share/apps/anaconda3/2020.07/etc/profile.d/conda.sh conda activate ./renv Rscript test.R  ","version":"Next","tagName":"h3"},{"title":"Singularity: Run Custom Applications with Containers","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/singularity_run_custom_applications_with_containers/","content":"","keywords":"","version":"Next"},{"title":"What is Singularity​","type":1,"pageTitle":"Singularity: Run Custom Applications with Containers","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/singularity_run_custom_applications_with_containers/#what-is-singularity","content":" Singularity is a container based Linux kernel workspace that works just like docker. You can run pre-built programs in containers without having to worry about the pre-install environment.  For users who are familiar with Docker containers, Singularity works very similarly, and can even run Docker containers.  For a detailed introduction on Singularity, visit their official site here  ","version":"Next","tagName":"h2"},{"title":"why do we use Singularity​","type":1,"pageTitle":"Singularity: Run Custom Applications with Containers","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/singularity_run_custom_applications_with_containers/#why-do-we-use-singularity","content":" There are multiple reasons to use Singularity on the HPC clusters:  Security: Singularity provides a layer of security as it does not require any root access on our clusters. This makes it safer against malware and bad scripts that might jeopardize the outer system. Thus we only support Singularity on our clusters(there are not other options such as Kubernetes or Docker on our clusters right now)Containerization: Singularity will run all your images(packaged and pre-built programs) inside of its containers, each container works like a small vm. They contain all the required environment and files of a single Linux kernel and you don't have to worry about any pre-installation nonsenseInter-connectivity: Containers are able to talk to each other, as well as the home system, so while each container has its own small space, they are still a part of a big interconnected structure. Thus enabling you to connect your programs.Accessibility: Probably the most important feature of all, Singularity allows you to run your program in 2 to 3 simple steps, as shown in the topic how to run a singularity container.  ","version":"Next","tagName":"h2"},{"title":"how to run a singularity container​","type":1,"pageTitle":"Singularity: Run Custom Applications with Containers","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/singularity_run_custom_applications_with_containers/#how-to-run-a-singularity-container","content":" There are 3 steps to run a Singularity container on our clusters:  pulling a image from Singularity hub or Docker hub  $ singularity pull &lt;image name&gt; # image name can be for example shub://vsoch/hello-world or docker://godlovedc/lolcow     build the image  $ singularity build &lt;a name of your choosing&gt;.simg:rw &lt;image name&gt; # the image name can be a local image or an image from a hub   We add the :rw tag at the end of the .simg to explicitly give it &quot;read and write&quot; permissions while building.    You can now run your container using the built image:  run container  # this is one way of running a container $ singularity run &lt;image name&gt;.simg:ro # this is another way to run a container $ ./&lt;image name&gt;.simg:ro   Unlike in the build phase, we add the :ro tag which means &quot;read only&quot; - as we are now just executing the image, not building it, and thus do not need it to be written. Writing access causes the Singularity image to be locked and it can become inaccessible while it is in read/write mode, so read only mode is best for executing commands.  running this would yield a menu for output:    go into container  singularity shell &lt;image name&gt;.simg:ro # after this step, you will be going into the container and start your programming     you can run commands for the container using exec arguments without actually going into the container  $ singularity exec &lt;image name&gt;.simg:ro &lt;commands&gt; # adding commands to the back will return the display result of these commands in the container without actually going into the container   Example:    That's it! Now you're good to go and can just use these simple steps to run singularity images and run your programs  For full information and documentation on Singularity, visit their site here  ","version":"Next","tagName":"h2"},{"title":"How to Create a Singularity Container​","type":1,"pageTitle":"Singularity: Run Custom Applications with Containers","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/singularity_run_custom_applications_with_containers/#how-to-create-a-singularity-container","content":" So what if you want to create an image from your container and save it for a rainy day?  The instructions are here for your convenience, read through them to create your own Singularity container and package it into an image!  For those that know how docker containers are built, you can build docker containers using the information here and upload them onto docker hub and pulling them using Singularity. Singularity supports all docker images!  ","version":"Next","tagName":"h2"},{"title":"Singularity vs Docker​","type":1,"pageTitle":"Singularity: Run Custom Applications with Containers","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/singularity_run_custom_applications_with_containers/#singularity-vs-docker","content":" Why are there so many mentions of Docker? The reason is that Singularity is essentially the same as Docker and you don't need to relearn Singularity if you already have experience with Docker. Now let's get into some pros and cons between the two programs.  Docker is more accepted commercially than Singularity. You can download and run Docker on your own computer with any operating system and build containers with ease while Singularity is used in a more academic setting. Singularity only supports Linux operating systems and cannot run on a windows linux kernel(your windows ubuntu), so it is much more limited.However, Docker requires root or admin access for the operating system it deploys on, and our clusters do not offer that access to any software that requires this criteria. Thus Docker is not available on the clusters and Singularity is.A silver lining in all of this is that Singularity fully supports docker images and you can do everything in docker and push your image to docker hub and pull them on the clusters. Thus making sure that you don't need to relearn Singularity all over again and can just use it through the simplest of commands in this wiki.  Good luck with Singularity, and have fun! ","version":"Next","tagName":"h2"},{"title":"SQLite: Handling Large Structured Data","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#overview","content":" Storing your data in the SQLite format allows you to get benefits of a database, and at the same time the simplicity of storage of data in a file on a disk.  SQLite is the most used database engine in the world. SQLite is built into all mobile phones and most computers and comes bundled inside countless other applications that people use every day. The SQLite file format is stable, cross-platform, and backwards compatible and the developers pledge to keep it that way through at least the year 2050. -- SQLite website:  ","version":"Next","tagName":"h2"},{"title":"Some use-cases​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#some-use-cases","content":" You think you need MySQL, PostreSQL, etc for your ML project. Usually you don'tYou have to deal with hundreds of GB of table-stuctured data (or larger) and your script (for whatever reason) can't be made parallel.You would request a lot of RAM and work with data slowly.  warning This would be a waste of RAM.  tip It is better in this case to request smaller amount of RAM and read data (efficiently) from disk - for example using SQLite  ","version":"Next","tagName":"h3"},{"title":"Benefits:​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#benefits","content":" You are not limited by RAM any longerCompared to other file formats SQLite is very good in selecting certain lines (especially if you use indexing)You can use familiar dplyr syntax or execute SQL queries directly dplyr is an interface for working with data in a database, not for modifying remote tables.DBI package allows to both read and modify tables SQLite is actually faster for common data analysis tasks than other popular databases.You can have multiple threads accessing an SQLite database simultaneously (for read operations. Writing is more tricky)Merging/Joining datasets on disk  ","version":"Next","tagName":"h3"},{"title":"Major benefits of SQLite compared to MySQL (PostgreSQL, etc)​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#major-benefits-of-sqlite-compared-to-mysql-postgresql-etc","content":" You control your own data (sqlite file). You don't depend on any service like MySQLYou can copy a file to your own laptop and work with itAgain, SQLite is faster!  ","version":"Next","tagName":"h3"},{"title":"Limits​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#limits","content":" SQLite has some limitations in terms of concurrency, which usually don't apply for typical ML/AI jobs.See Four Different Ways To Handle SQLite Concurrency for more information.  ","version":"Next","tagName":"h3"},{"title":"Command line (CLI) example​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#command-line-cli-example","content":" Create environment  mkdir projects/sqlite-test cd projects/sqlite-test conda create -p ./cenv conda activate ./cenv conda install -y sqlite   Then follow this SQLite example.  sqlite3 db_file.sqlite create table tbl1(one varchar(10), two smallint); insert into tbl1 values('hello!',10); insert into tbl1 values('goodbye', 20); select * from tbl1;   Now Close session (Ctrl-D).  Reopen session to check if changes are saved  sqlite3 db_file.sqlite select * from tbl1;   ","version":"Next","tagName":"h2"},{"title":"R example​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#r-example","content":" ","version":"Next","tagName":"h2"},{"title":"Install​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#install","content":" Here we use conda, as a great way to keep everything isolated and reproducible.  note conda will install pre-compiled packages. Which is good (faster) and bad (not fully optimized for a specific hardware)  tip Alternative: install packages to a local directory or use renv as described in R Packages with renv mkdir /scratch/$USER/projects/myTempProject cd /scratch/$USER/projects/myTempProject module load anaconda3/2020.07 conda create -p ./cenv -c conda-forge r=4.1 conda activate ./cenv conda install -c r r-rsqlite conda install -c r r-tidyverse conda install -c conda-forge r-remotes conda install -c r r-feather conda install -c r r-nycflights13 note window functions (row_number in particular) require newer version of rsqlite R remotes::install_github(&quot;r-dbi/RSQLite&quot;) ## update ALL   tip Save list of installed packages for reproducibility ## conda list --export &gt; requirements.txt   ","version":"Next","tagName":"h3"},{"title":"Use​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#use","content":" Many examples can be found here:  SQL syntaxdplyr syntax  library(tidyverse) library(DBI) # Create RSQLite database file with name &quot;allData&quot; con &lt;- dbConnect(RSQLite::SQLite(), &quot;allData&quot;)   Copy data frame to database (dplyr)  copy_to(con, nycflights13::flights, &quot;fl&quot;, temporary=FALSE)   Or copy data to database using DBI  dbCreateTable(con, &quot;fl&quot;, nycflights13::flights, temporary = FALSE) dbAppendTable(con, &quot;fl&quot;, nycflights13::flights)   Connect to a specific table  dbListTables(con) df_con &lt;- tbl(con, &quot;fl&quot;) ## check number of rows df_con %&gt;% count()   Subset  df_temp &lt;- df_con %&gt;% filter( row_number() %in% c(1, 3) ) %&gt;% collect   Save as feather  feather::write_feather(df_temp, paste0(&quot;file_&quot;, ind, &quot;.feather&quot;))   ","version":"Next","tagName":"h3"},{"title":"Alternative: read csv file to SQLite directly​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#alternative-read-csv-file-to-sqlite-directly","content":" If you already have a large csv file on disk, and you don't want to read it to RAM, you can read it to SQLite file directly  conda install -c conda-forge r-sqldf R library(sqldf) ## create data file sqldf(&quot;attach allData as new&quot;) ## read file directly from csv to sqlite read.csv.sql(file = &quot;test.tab&quot;, sql = &quot;create table states_data as select * from file&quot;, dbname = &quot;allData&quot;)   ","version":"Next","tagName":"h3"},{"title":"UI for SQLite - SQLiteStudio​","type":1,"pageTitle":"SQLite: Handling Large Structured Data","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/sqlite_handling_large_structured_data/#ui-for-sqlite---sqlitestudio","content":" Once you have SQLite file, you can easily transfer it to your own laptop and explore it using SQLiteStudio, if you like to use UI instead of terminal ","version":"Next","tagName":"h2"},{"title":"Support","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/training_and_support/support/","content":"Support Some of your questions may be already answered here Introductory HPC Video PlaylistCluster Getting Started DocumentationConsider to sign up for Training and Workshops NYU HPC offers personalized help through personal consultations for simple and advanced cases: Do you have trouble with something that seems trivial?Would you like to discuss how to better apply Deep Learning to your case?Something else?Contact us directly at hpc@nyu.edu","keywords":"","version":"Next"},{"title":"HPC training at NYU","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/training_and_support/training_at_nyu/","content":"","keywords":"","version":"Next"},{"title":"Workshops offered by the HPC team at the library:​","type":1,"pageTitle":"HPC training at NYU","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/training_and_support/training_at_nyu/#workshops-offered-by-the-hpc-team-at-the-library","content":" The HPC team offers multiple workshops at the library covering various aspects of HPC usage from novice to advanced level. You can find the list of available HPC couses can be viewed at nyu.libcal.com.  ","version":"Next","tagName":"h3"},{"title":"HPC Caprentry tutorials for Greene​","type":1,"pageTitle":"HPC training at NYU","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/training_and_support/training_at_nyu/#hpc-caprentry-tutorials-for-greene","content":" Introduction to using the shell in a High-Performance Computing context: This lesson provides an introduction to the bash shell aimed at researchers who will be using the command line to use remote, high-performance computing (HPC) systems. The material is also suitable for teaching the use of the shell for any remote, advanced computing resources. An Introduction to High Performance Computing: This lesson teaches the basics of interacting with high-performance computing (HPC) clusters through the command line.  ","version":"Next","tagName":"h3"},{"title":"Introductory Videos​","type":1,"pageTitle":"HPC training at NYU","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/training_and_support/training_at_nyu/#introductory-videos","content":" We have a playlist of how-to videos covering the basics of accessing and using Greene here (www.youtube.com). ","version":"Next","tagName":"h3"},{"title":"HPC training outside NYU","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/training_and_support/training_outside/","content":"","keywords":"","version":"Next"},{"title":"ACCESS workshops​","type":1,"pageTitle":"HPC training outside NYU","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/training_and_support/training_outside/#access-workshops","content":" As part of the Advanced Cyberinfrastructure Coordination Ecosystem: Services &amp; Support program, NSF provides tutorials for HPC, OpenOnDemand, etc. ","version":"Next","tagName":"h3"},{"title":"Introduction to High-Performance Computing","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_hpc/intro_hpc/","content":"Introduction to High-Performance Computing This workshop is an introduction to using high-performance computing systems effectively. We can’t cover every case or give an exhaustive course on parallel programming in just two days’ teaching time. Instead, this workshop is intended to give students a good introduction and overview of the tools available and how to use them effectively.","keywords":"","version":"Next"},{"title":"Software on Greene","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/software_on_greene/","content":"","keywords":"","version":"Next"},{"title":"Software Overview​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/software_on_greene/#software-overview","content":" There are different types of software packages available  Use module avail command to see preinstalled software. This includes the licensed software listed below Singularity Containers You can find those already built and ready to use, at location /scratch/work/public/singularity/For more information on running software with Singularity, click here. Python/R/Julia packages can be installed by a user  If you need another linux program installed, please contact us at hpc@nyu.edu  ","version":"Next","tagName":"h2"},{"title":"Software and Environment Modules​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/software_on_greene/#software-and-environment-modules","content":" Lmod, an Environment Module system, is a tool for managing multiple versions and configurations of software packages and is used by many HPC centers around the world. With Environment Modules, software packages are installed away from the base system directories, and for each package, an associated modulefile describes what must be altered in a user's shell environment - such as the $PATH environment variable - in order to use the software package. The modulefile also describes dependencies and conflicts between this software package and other packages and versions.  To use a given software package, you load the corresponding module. Unloading the module afterwards cleanly undoes the changes that loading the module made to your environment, thus freeing you to use other software packages that might have conflicted with the first one.  Below is a list of modules and their associated functions:  Command\tFunctionmodule unload &lt;module-name&gt;\tunload a module module show &lt;module-name&gt;\tsee exactly what effect loading the module will have with module purge\tremove all loaded modules from your environment module load &lt;module-name&gt;\tload a module module whatis &lt;module-name&gt;\tfind out more about a software package module list\tcheck which modules are currently loaded in your environment module avail\tcheck what software packages are available module help &lt;module-name&gt;\tA module file may include more detailed help for the software package  ","version":"Next","tagName":"h2"},{"title":"Package Management for R, Python, & Julia, and Conda in general​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/software_on_greene/#package-management-for-r-python--julia-and-conda-in-general","content":" Conda environments (Python, R)Using virtual environments for PythonManaging R packages with renvSingularity with Miniconda  ","version":"Next","tagName":"h2"},{"title":"Examples of software usage on Greene​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/software_on_greene/#examples-of-software-usage-on-greene","content":" Examples can be found under /scratch/work/public/examples/ and include the following   alphafold\tknitro\tSingularity amd GPUs\tlammps\tslurm comsol\tmatlab\tspark c-sharp\tmathematica\tstata crystal17\tnamd\tsquashfs fluent\torca\ttrinity gaussian\tquantum-espresso\tvnc hadoop-streaming\tR\tvscode julia\tsas\txvfb jupyter notebooks\tschrodinger\t  ","version":"Next","tagName":"h2"},{"title":"Accessing Datasets with Singularity​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/software_on_greene/#accessing-datasets-with-singularity","content":" Singularity for Datasets  ","version":"Next","tagName":"h2"},{"title":"Licensed Software​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/software_on_greene/#licensed-software","content":" ","version":"Next","tagName":"h2"},{"title":"SCHRODINGER​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/software_on_greene/#schrodinger","content":" Schrödinger provides a complete suite of software solutions with the latest advances in pharmaceutical research and computational chemistry. The NYU New York campus has a limited number of licenses for the Biologics Suite (ConfGen, Epik, Jaguar, Jaguar pKa, MacroModel, Prime, QSite, SiteMap), BioLuminate and the Basic Docking Suite.  note Schrödinger can be used for non-commercial, academic purposes ONLY.  Using SCHRODINGER on HPC Cluster​  To load Schrodinger module execute  $ module load schrodinger/2021-1   Using SCHRODINGER on NYU Lab Computers​  Request your account at: https://www.schrodinger.com/request-accountDownload the software at: https://www.schrodinger.com/downloads/releasesContact NYU-HPC team to request your license file.  These license servers are accessible from NYU subnet.  Please see the following links for installation of the license file:  https://www.schrodinger.com/kb/377238https://www.schrodinger.com/license-installation-instructions  To check licenses status  # module load schrodinger/2021-1 # load schrodinger if not already loaded # licadmin STAT # licutil -jobs ## For example: [wang@cs001 ~]$ licutil -jobs ######## Server /share/apps/schrodinger/schrodinger.lic Product &amp; job type Jobs BIOLUMINATE 10 BIOLUMINATE, Docking 1 BIOLUMINATE, Shared 10 CANVAS 50 COMBIGLIDE, Grid Generation 11 COMBIGLIDE, Library Generation 50 COMBIGLIDE, Protein Prep 11 COMBIGLIDE, Reagent Prep 1 EPIK 11 GLIDE, Grid Generation 11 GLIDE, Protein Prep 11 GLIDE, SP Docking 1 GLIDE, XP Descriptors 1 GLIDE, XP Docking 1 IMPACT 11 JAGUAR 5 JAGUAR, PKA 5 KNIME 50 LIGPREP, Desalter 1 LIGPREP, Ionizer 3511 LIGPREP, Ligparse 1 LIGPREP, Neutralizer 1 LIGPREP, Premin Bmin 1 LIGPREP, Ring Conf 1 LIGPREP, Stereoizer 1 LIGPREP, Tautomerizer 1 MACROMODEL 5 MACROMODEL, Autoref 5 MACROMODEL, Confgen 5 MACROMODEL, Csearch Mbae 5 MAESTRO, Unix 1000 MMLIBS 3511 PHASE, CL Phasedb Confsites 1 PHASE, CL Phasedb Convert 1 PHASE, CL Phasedb Manage 1 PHASE, DPM Ligprep Clean Structures 1 PHASE, DPM Ligprep Generate Conformers 5 PHASE, MD Create sites 1 PRIME, CM Build Membrane 2 PRIME, CM Build Structure 2 PRIME, CM Edit Alignment 2 PRIME, CM Struct Align 18 PRIME, Threading Search 2 QSITE 5 SITEMAP 10   Schrodinger Example Files​  Example SBATCH jobs and outputs are available to review here:  /scratch/work/public/examples/schrodinger/   ","version":"Next","tagName":"h3"},{"title":"COMSOL​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/software_on_greene/#comsol","content":" COMSOL is a problem-solving simulation environment, enforcing compatibility guarantees consistent multiphysics models. COMSOL Multiphysics is a general-purpose software platform, based on advanced numerical methods, for modeling and simulating physics-based problems. The package is cross-platform (Windows, Mac, Linux). The COMSOL Desktop helps you organize your simulation by presenting a clear overview of your model at any point. It uses functional form, structure, and aesthetics as the means to achieve simplicity for modeling complex realities.  note This license is for academic use only with Floating Network Licensing in nature i.e., authorized users are allowed to use the software on desktops. Please contact hpc@nyu.edu for the license. However, COMSOL is also available on NYU HPC cluster Greene.  In order to check what Comsol licenses are available on Greene use comsol_licenses command in your terminal session.  Several versions of COMSOL are available on the HPC cluster. To use COMSOL on the Greene HPC cluster, please load the relevant module in your batch job submission script:  module load comsol/5.6.0.280   To submit a COMSOL job in a parallel fashion, running on multiple processing cores, follow the steps below:  Create a directory on &quot;scratch&quot; as given below.  mkdir /scratch/&lt;net_id&gt;/example cd /scratch/&lt;net_id&gt;/example   Copy example files to your newly created directory  cp /scratch/work/public/examples/comsol/run-comsol.sbatch /scratch/&lt;net_id&gt;/example/ cp /scratch/work/public/examples/comsol/test-input.mph /scratch/&lt;net_id&gt;/example/   Edit the slurm batch script file (run-comsol.sbatch) to match your case (for example chance location of the run directory).Once the slurm batch script file is ready, it can be submitted to the job scheduler using sbatch. After successful completion of job, verify output log file for detail output information.  sbatch run-comsol.sbatch   ","version":"Next","tagName":"h3"},{"title":"MATHEMATICA​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/software_on_greene/#mathematica","content":" Mathematica is a general computing environment with organizing algorithmic, visualization, and user interface capabilities. The many mathematical algorithms included in Mathematica make computation easy and fast.  To run Mathematica on the Greene HPC cluster, please load the relevant module in your batch job submission script:  module load mathematica/12.1.1   note In the example below the module is loaded already in the sbatch script.  To submit a batch Mathematica job for running in a parallel mode on multiple processing cores, follow below steps:  Create a directory on &quot;scratch&quot; as given below.  mkdir /scratch/&lt;net_id&gt;/example cd /scratch/&lt;net_id&gt;/example   Copy example files to your newly created directory.  cp /scratch/work/public/examples/mathematica/basic/example.m /scratch/&lt;net_id&gt;/example/ cp /scratch/work/public/examples/mathematica/basic/run-mathematica.sbatch /scratch/&lt;net_id&gt;/example   Edit the slurm batch script file (run-mathematica.sbatch) to match your case (for example chance location of the run directory).Once the sbatch script file is ready, it can be submitted to the job scheduler using sbatch. After successful completion of job, verify output log file generated.  sbatch run-mathematica.sbatch   ","version":"Next","tagName":"h3"},{"title":"SAS​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/software_on_greene/#sas","content":" SAS is a software package which enables programmers to perform many tasks, including:  Information retrievalData managementReport writing &amp; graphicsStatistical analysis and data miningBusiness planningForecasting and decision supportOperations research and project managementQuality improvementApplications developmentData warehousing (extract, transform, load)Platform independent and remote computing.  There are licenses for 2 CPUs on the HPC Cluster.  Running a parallel SAS job on HPC cluster (Greene):​  To submit a SAS job for running on multiple processing elements, follow below steps:  Create a directory on &quot;scratch&quot;:  mkdir /scratch/&lt;net_id&gt;/example cd /scratch/&lt;net_id&gt;/example   Copy example files to your newly created directory.  cp /scratch/work/public/examples/sas/test.sas /scratch/&lt;net_id&gt;/example/ cp /scratch/work/public/examples/sas/test2.sas /scratch/&lt;net_id&gt;/example/ cp /scratch/work/public/examples/sas/run-sas.sbatch /scratch/&lt;net_id&gt;/example/   Submit as shown below. After successful completion of job, verify output log file generated.  sbatch run-sas.sbatch   ","version":"Next","tagName":"h3"},{"title":"MATLAB​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/software_on_greene/#matlab","content":" MATLAB is a technical computing environment for high performance numeric computation and visualization. MATLAB integrates numerical analysis, matrix computation, signal processing, and graphics in an easy to use environment without using traditional programming.  MATLAB on personal computers and laptops​  NYU has a Total Academic Headcount (TAH) license which provides campus-wide access to MATLAB, Simulink, and a variety of add-on products. All faculty, researchers, and students (on any NYU campus) can use MATLAB on their personal computers and laptops and may go to the following site to download the NYU site license software free of charge.  https://www.mathworks.com/academia/tah-portal/new-york-university-618777.html  MATLAB can be used for non-commercial, academic purposes.  There are several versions of Matlab available on the cluster and the relevant version can be loaded.  module load matlab/2020b module load matlab/2021a   In order to run MATLAB interactively on the cluster, start an interactive slurm job, load the matlab module and launch an interactive matlab session in the terminal.  Mathworks has provided a Greene Matlab User Guide that presents useful tips and practices for using Matlab on the cluster.  ","version":"Next","tagName":"h3"},{"title":"STATA​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/software_on_greene/#stata","content":" Stata is a command and menu-driven software package for statistical analysis. It is available for Windows, Mac, and Linux operating systems. Most of its users work in research. Stata's capabilities include data management, statistical analysis, graphics, simulations, regression and custom programming.  Running a parallel STATA job on HPC cluster (Greene):​  To submit a STATA job for running on multiple processing elements, follow below steps.  Create a directory on &quot;scratch&quot;:  mkdir /scratch/&lt;net_id&gt;/example cd /scratch/&lt;net_id&gt;/example   Copy example files to your newly created directory.  cp /scratch/work/public/examples/stata/run-stata.sbatch /scratch/&lt;net_id&gt;/example/ cp /scratch/work/public/examples/stata/stata-test.do /scratch/&lt;net_id&gt;/example/   Submit using sbatch. After successful completion of job, verify output log file generated.  sbatch run-stata.sbatch   ","version":"Next","tagName":"h3"},{"title":"GAUSSIAN​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/software_on_greene/#gaussian","content":" Gaussian uses basic quantum mechanic electronic structure programs. This software is capable of handling proteins and large molecules using semi-empirical, ab initio molecular orbital (MO), density functional, and molecular mechanics calculations.  The NYU Gaussian license only covers PIs at the Washington Square Park campus. We will grant access to you after verifying your WSP affiliation. For access, please email hpc@nyu.edu.  Running a parallel Gaussian job on HPC cluster (Greene):​  To submit a Gaussian job for running on multiple processing elements, follow below steps.  Create a directory on &quot;scratch&quot;:  mkdir /scratch/&lt;net_id&gt;/example cd /scratch/&lt;net_id&gt;/example #Copy example files to your newly created directory. cp /scratch/work/public/examples/gaussian/basic/test435.com /scratch/&lt;net_id&gt;/example/ cp /scratch/work/public/examples/gaussian/basic/run-gaussian.sbatch /scratch/&lt;net_id&gt;/example/   Once the sbatch script file is ready, it can be submitted to the job scheduler using sbatch. After successful completion of job, verify output log file generated.  sbatch run-gaussian.sbatch   ","version":"Next","tagName":"h3"},{"title":"Knitro​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tools_and_software/software_on_greene/#knitro","content":" Knitro is a commercial software package for solving large scale mathematical optimization problems. Knitro is specialized for nonlinear optimization, but also solves linear programming problems, quadratic programming problems, systems of nonlinear equations, and problems with equilibrium constraints. The unknowns in these problems must be continuous variables in continuous functions; however, functions can be convex or nonconvex. Knitro computes a numerical solution to the problem—it does not find a symbolic mathematical solution. Knitro versions 9.0.1 and 10.1.1 are available.  Running a parallel Knitro job on HPC cluster (Greene):​  To submit a Knitro job for running on multiple processing elements, follow below steps.  Create a directory on &quot;scratch&quot;:  mkdir /scratch/&lt;net_id&gt;/example cd /scratch/&lt;net_id&gt;/example   Copy example files to your newly created directory.  cp /scratch/work/public/examples/knitro/knitro.py /scratch/&lt;net_id&gt;/example/   There is no sample sbatch script available for knitro.After creating your own sbatch script you can execute it as follows:  sbatch &lt;script&gt;.sbatch  ","version":"Next","tagName":"h3"},{"title":"Introduction to Using the Shell on Greene","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/intro/","content":"","keywords":"","version":"Next"},{"title":"Why Use a Cluster​","type":1,"pageTitle":"Introduction to Using the Shell on Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/intro/#why-use-a-cluster","content":" Overview Questions: Why would I be interested in High Performance Computing (HPC) on Greene?What can I expect to learn from this course? Objectives: Be able to describe what an HPC system is.Identify how an HPC system could benefit you.  ","version":"Next","tagName":"h2"},{"title":"Why Use Greene?​","type":1,"pageTitle":"Introduction to Using the Shell on Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/intro/#why-use-greene","content":" What do you need? How does computing help you do your research? How could more computing help you do more or better research?  Frequently, research problems that use computing can outgrow the desktop or laptop computer where they started:  A statistics student wants to do cross-validate their model. This involves running the model 1000 times — but each run takes an hour. Running on their laptop will take over a month!A genomics researcher has been using small datasets of sequence data, but soon will be receiving a new type of sequencing data that is 10 times as large. It’s already challenging to open the datasets on their computer — analyzing these larger datasets will probably crash it.An engineer is using a fluid dynamics package that has an option to run in parallel. So far, they haven’t used this option on their desktop, but in going from 2D to 3D simulations, simulation time has more than tripled and it might be useful to take advantage of that feature.  In all these cases, what is needed is access to more computers than can be used at the same time. Luckily, large a scale computing system — shared computing resources with lots of computers — is available at NYU and it is named Greene. Greene has more central processing units (CPUs), CPUs that operate at higher speeds, more memory, more storage, and faster connections with other computer systems. This kind of system is frequently referred to as a “cluster”, “supercomputer” or an “high performance computing” (HPC) system. In this lesson, we will usually use the terminology HPC and HPC cluster.  Using a cluster often has the following advantages for researchers:  Speed: With many more CPU cores, often with higher performance specs than a typical laptop or desktop, HPC systems can offer significant speed up.Volume: Many HPC systems have both the processing memory (RAM) and disk storage to handle very large amounts of data. Terabytes of RAM and petabytes of storage are available for research projects.Efficiency: Many HPC systems operate a pool of resources that are drawn on by many users. In most cases when the pool is large and diverse enough the resources on the system are used almost constantly.Cost: Bulk purchasing and government funding mean that the cost to the research community for using these systems in significantly less that it would be otherwise.Convenience: Maybe your calculations just take a long time to run or are otherwise inconvenient to run on your personal computer. There’s no need to tie up your own computer for hours when you can use someone else’s instead.  This is how a large-scale compute system like a cluster can help solve problems like those listed at the start of the lesson.  Thinking ahead How do you think using a large-scale computing system will be different from using your laptop? Please think about some differences/difficulties you may run into.  ","version":"Next","tagName":"h3"},{"title":"On Command Line​","type":1,"pageTitle":"Introduction to Using the Shell on Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/intro/#on-command-line","content":" Using HPC systems often involves the use of a shell through a command line interface (CLI) and either specialized software or programming techniques. The shell is a program with the special role of having the job of running other programs rather than doing calculations or similar tasks itself. What the user types goes into the shell, which then figures out what commands to run and orders the computer to execute them. (Note that the shell is called “the shell” because it encloses the operating system in order to hide some of its complexity and make it simpler to interact with.) The most popular Unix shell is Bash, the Bourne Again SHell (so-called because it’s derived from a shell written by Stephen Bourne). Bash is the default shell on most modern implementations of Unix and in most packages that provide Unix-like tools for Windows.  Interacting with the shell is done via a command line interface (CLI) on most HPC systems. In the earliest days of computers, the only way to interact with early computers was to rewire them. From the 1950s to the 1980s most people used line printers. These devices only allowed input and output of the letters, numbers, and punctuation found on a standard keyboard, so programming languages and software interfaces had to be designed around that constraint and text-based interfaces were the way to do this. A typing-based interface is often called a command-line interface, or CLI, to distinguish it from a graphical user interface, or GUI, which most people now use. The heart of a CLI is a read-evaluate-print loop, or REPL: when the user types a command and then presses the Enter (or Return) key, the computer reads it, executes it, and prints its output. The user then types another command, and so on until the user logs off.  Learning to use Bash or any other shell sometimes feels more like programming than like using a mouse. Commands are terse (often only a couple of characters long), their names are frequently cryptic, and their output is lines of text rather than something visual like a graph. However, using a command line interface can be extremely powerful, and learning how to use one will allow you to reap the benefits described above.  ","version":"Next","tagName":"h3"},{"title":"The rest of this lesson​","type":1,"pageTitle":"Introduction to Using the Shell on Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/intro/#the-rest-of-this-lesson","content":" The only way to use these types of resources is by learning to use the command line. This introduction to HPC systems has two parts:  We will learn to use the UNIX command line (also known as Bash).We will use our new Bash skills to connect to and operate a high-performance computing supercomputer.  The skills we learn here have other uses beyond just HPC: Bash and UNIX skills are used everywhere, be it for web development, running software, or operating servers. It’s become so essential that Microsoft now ships it as part of Windows! Knowing how to use Bash and HPC systems will allow you to operate virtually any modern device. With all of this in mind, let’s connect to a cluster and get started!  Key Points High Performance Computing (HPC) typically involves connecting to very large computing systems elsewhere in the world.These HPC systems can be used to do work that would either be impossible or much slower or smaller systems.The standard method of interacting with such systems is via a command line interface such as Bash. ","version":"Next","tagName":"h3"},{"title":"Connecting to the remote HPC system Greene","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/","content":"","keywords":"","version":"Next"},{"title":"Remote Connections with the NYU VPN & HPC Gateway Server​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#remote-connections-with-the-nyu-vpn--hpc-gateway-server","content":" If you are connecting from a remote location that is not on the NYU network (your home for example), you have two options:  VPN Option: set up your computer to use the NYU VPN. Once you’ve created a VPN connection, you can proceed as if you were connected to the NYU net.Gateway Option: go through our gateway servers (example below). Gateways are designed to support only a very minimal set of commands and their only purpose is to let users connect to HPC systems without needing to first connect to the VPN.  ","version":"Next","tagName":"h2"},{"title":"Connect to the NYU VPN​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#connect-to-the-nyu-vpn","content":" To connect to VPN, please see the NYU instructions.  ","version":"Next","tagName":"h2"},{"title":"Log into the Greene Cluster​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#log-into-the-greene-cluster","content":" ","version":"Next","tagName":"h2"},{"title":"Inside the NYU network (non-Windows)​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#inside-the-nyu-network-non-windows","content":" From within the NYU network, that is, from an on-campus location, or after you VPN inside NYU’s network, you can log in to the HPC clusters directly. You do not need to log in to the gateway host.  To log in to the HPC cluster (Greene), simply use:  ssh &lt;NetID&gt;@greene.hpc.nyu.edu   ","version":"Next","tagName":"h3"},{"title":"Outside the NYU network (non-Windows)​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#outside-the-nyu-network-non-windows","content":" From an off-campus location (outside NYU-NET), logging in to the HPC clusters is a two-step process:  log in to the gateway host, gw.hpc.nyu.edu. From a Mac or Linux workstation, this is a simple terminal command (replace &lt;NetID&gt; with your NetID). Your password is the same password you use for NYU Home:  ssh &lt;NetID&gt;@gw.hpc.nyu.edu   log in to the cluster. For Greene, this is done with:  ssh &lt;NetID&gt;@greene.hpc.nyu.edu   ","version":"Next","tagName":"h3"},{"title":"From Windows​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#from-windows","content":" Windows users will need to use PuTTY, please see the NYU instructions.  ","version":"Next","tagName":"h3"},{"title":"Opening a Terminal​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#opening-a-terminal","content":" Accessing the Greene HPC cluster is primarily done through the Command Line Interface (CLI). A CLI provides a text-based environment that allows users to manage files, run programs, and navigate directories via command input. On macOS, the built-in CLI tool is Terminal, while Windows 10 users can leverage the Windows Subsystem for Linux (WSL) for similar functionality. Additionally, a popular tool for connecting to Linux servers from Windows is PuTTY, a free SSH client.  Connecting to an HPC system is most often done through a tool known as “SSH” (Secure SHell) and usually SSH is run through a terminal. So, to begin using an HPC system we need to begin by opening a terminal. Different operating systems have different terminals, none of which are exactly the same in terms of their features and abilities while working on the operating system. When connected to the remote system the experience between terminals will be identical as each will faithfully present the same experience of using that system.  Here is the process for opening a terminal in each operating system.  ","version":"Next","tagName":"h2"},{"title":"Linux​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#linux","content":" There are many different versions (aka “flavours”) of Linux and how to open a terminal window can change between flavours. Fortunately most Linux users already know how to open a terminal window since it is a common part of the workflow for Linux users. If this is something that you do not know how to do then a quick search on the Internet for “how to open a terminal window in” with your particular Linux flavour appended to the end should quickly give you the directions you need.  To connect to the gateway servers, simply open a terminal application and enter the following command:  ssh &lt;NetID&gt;@gw.hpc.nyu.edu   After typing in your password you will be logged in to the cluster. Once this connection is established, you can make one more hop and connect to one of the HPC clusters:  # this will connect you to Greene HPC cluster ssh &lt;NetID&gt;@greene.hpc.nyu.edu   ","version":"Next","tagName":"h3"},{"title":"Mac​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#mac","content":" Macs have had a terminal built in since the first version of OS X, since it is built on a UNIX-like operating system, leveraging many parts from BSD (Berkeley Software Distribution). The terminal can be quickly opened through the use of the Searchlight tool. Hold down the command key and press the spacebar. In the search bar that shows up type “terminal”, choose the terminal app from the list of results (it will look like a tiny, black computer screen) and you will be presented with a terminal window. Alternatively, you can find Terminal under “Utilities” in the Applications menu in the Finder.  To connect to the gateway servers, simply open a terminal application and enter the following command:  ssh &lt;NetID&gt;@gw.hpc.nyu.edu   After typing in your password you will be logged in to the cluster. Once this connection is established, you can make one more hop and connect to Greene:  # this will connect you to Greene HPC cluster ssh &lt;NetID&gt;@greene.hpc.nyu.edu   ","version":"Next","tagName":"h3"},{"title":"Windows​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#windows","content":" While Windows does have a command-line interface known as the “Command Prompt” that has its roots in MS-DOS (Microsoft Disk Operating System) it does not have an SSH tool built into it and so one needs to be installed. There are a variety of programs that can be used for this; a few common ones we describe here, as follows:  Git BASH​  Git BASH gives you a terminal like interface in Windows. You can use this to connect to a remote computer via SSH. It can be downloaded for free from Git for Windows.  Windows Subsystem for Linux​  The Windows Subsystem for Linux also allows you to connect to a remote computer via SSH. Please see the instructions from Microsoft.  MobaXterm​  MobaXterm is a terminal window emulator for Windows and the home edition can be downloaded for free from mobatek.net. If you follow the link you will note that there are two editions of the home version available: Portable and Installer. The portable edition puts all MobaXterm content in a folder on the desktop (or anywhere else you would like it) so that it is easy to add plug-ins or remove the software. The installer edition adds MobaXterm to your Windows installation and menu as any other program you might install. If you are not sure that you will continue to use MobaXterm in the future, the portable edition is likely the best choice for you.  If you use MobaXterm you can use MobaKeyGen to manage your ssh keys. Please see the MoabXterm documentation for details.  Download the version that you would like to use and install it as you would any other software on your Windows installation. Once the software is installed you can run it by either opening the folder installed with the portable edition and double-clicking on the executable file named MobaXterm_Personal_11.1 (your version number may vary) or, if the installer edition was used, finding the executable through either the start menu or the Windows search option.  Once the MobaXterm window is open you should see a large button in the middle of that window with the text “Start Local Terminal”. Click this button and you will have a terminal window at your disposal.  PuTTY​  It is strictly speaking not necessary to have a terminal running on your local computer in order to access and use a remote system, only a window into the remote system once connected. PuTTY is likely the oldest, most well-known, and widely used software solution to take this approach.  PuTTY is available for free download. Download the version that is correct for your operating system and install it as you would other software on your Windows system. Once installed it will be available through the start menu or similar.  You can use puttygen to create ssh keys if you are using PuTTY. Please see the puttygen page in the PuTTY documentation for details.  Running PuTTY will not initially produce a terminal but instead a window full of connection options. Putting the address of the remote system in the “Host Name (or IP Address)” box and either pressing enter or clicking the “Open” button should begin the connection process.  If this works you will see a terminal window open that prompts you for a username through the “login as:” prompt and then for a password. If both of these are passed correctly then you will be given access to the system and will see a message saying so within the terminal. If you need to escape the authentication process you can hold the Control (Ctrl) key and press the c key to exit and start again.  Note that you may want to paste in your password rather than typing it. Use Ctrl plus a right-click of the mouse to paste content from the clipboard to the PuTTY terminal.  For those logging in with PuTTY it would likely be best to cover the terminal basics already mentioned above before moving on to navigating the remote system.  ","version":"Next","tagName":"h3"},{"title":"Open OnDemand (Web-based Graphical User Interface)​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#open-ondemand-web-based-graphical-user-interface","content":" Open OnDemand (OOD) is an open source project funded by the National Science Foundation (NSF). OOD is designed to create easier access for users to interface with HPC systems. Originally developed by Ohio Supercomputer Center (OSC), used by many universities around the world, and now servicing the NYU Greene HPC cluster.  OOD has a variety of convenient tools to manage files, access the command line, manage and monitor jobs, and launch interactive applications, such as Jupyter Notebooks, RStudio sessions, and even full Linux Desktops.  Features Include:  Easy file management - upload and download files, view HTML and pictures without downloadingCommand-line shell access without any SSH client locally installedJob management and monitoringFull Linux desktop experience without X11Interactive Apps such as JupyterHub and RStudio without the need for port forwarding  OOD is accessible to all users with a valid NYU HPC account while on-campus network or through a VPN.  To access OOD visit: https://ood.hpc.nyu.edu (VPN Required)  ","version":"Next","tagName":"h2"},{"title":"Access the Shell​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#access-the-shell","content":" Under the clusters menu you can select the Greene Shell Access option to access the Linux shell. No local SSH client is required.  ","version":"Next","tagName":"h3"},{"title":"Interactive Applications​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#interactive-applications","content":" GUI based applications are accessible without the need for port or X11 forwarding. Select the Interactive Apps menu, select the desired application, and submit the job based on required resources and options.  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting Connections to Open OnDemand​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#troubleshooting-connections-to-open-ondemand","content":" A common issue that can occur is receiving an error that the Open OnDemand page cannot be reached. Sometimes this can indicate that the service is down, but often this is an issue with the the local browser cache. You can test this by opening a private browser window and seeing if https://ood.hpc.nyu.edu will load. If it does, try deleting the cache for https://ood.hpc.nyu.edu in your browser history to resolve this issue.  In Chrome, this can be done by navigating to this page in your settings:  chrome://settings/content/all?searchSubpage=ood.hpc.nyu.edu&amp;search=site+data  The link above will automatically search for the Open OnDemand site data and cookies. You can then simply click on the trashcan icon to delete the site cache.  Once done, try navigating again to https://ood.hpc.nyu.edu and the site should load. For other issues please email hpc@nyu.edu.  ","version":"Next","tagName":"h3"},{"title":"Creating an SSH key (optional)​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#creating-an-ssh-key-optional","content":" SSH keys are an alternative method for authentication to obtain access to remote computing systems. They can also be used for authentication when transferring files or for accessing version control systems. In this section you will create a pair of SSH keys, a private key which you keep on your own computer and a public key which is placed on the remote HPC system that you will log into.  ","version":"Next","tagName":"h2"},{"title":"Windows​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#windows-1","content":" We mentioned methods for creating SSH keys using some of the Windows SSH options above.  ","version":"Next","tagName":"h3"},{"title":"Linux, Mac and Windows Subsystem for Linux​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#linux-mac-and-windows-subsystem-for-linux","content":" Once you have opened a terminal check for existing SSH keys and filenames since existing SSH keys could be overwritten by the following command if the filename is the same. If you already have a key with the name given after the -f option you will need to change the filename to keep from losing your existing file.  $ ls ~/.ssh/   then generate a new public-private key pair:  $ ssh-keygen -o -a 100 -t rsa -b 4096 -f ~/.ssh/id_Greene_rsa   -o (no default): use the OpenSSH key format, rather than PEM.-a (default is 16): number of rounds of passphrase derivation; increase to slow down brute force attacks.-t (default is rsa): specify the “type” or cryptographic algorithm.-b (default is 2048): sets the number of bits in the key.-f (default is /home/user/.ssh/id_algorithm): filename to store your keys. If you already have SSH keys, make sure you specify a different name: ssh-keygen will overwrite the default key if you don’t specify!  When prompted, enter a strong password that you will remember. Cryptography is only as good as the weakest link, and this will be used to connect to a powerful, precious, computational resource.  Take a look in ~/.ssh (use ls ~/.ssh). You should see the two new files: your private key (~/.ssh/key_Greene_rsa) and the public key (~/.ssh/key_Greene_rsa.pub). If a key is requested by the system administrators, the public key is the one to provide.  danger Private keys are your private identity A private key that is visible to anyone but you should be considered compromised, and must be destroyed. This includes having improper permissions on the directory it (or a copy) is stored in, traversing any network in the clear, attachment on unencrypted email, and even displaying the key (which is ASCII text) in your terminal window. Protect this key as if it unlocks your front door. In many ways, it does.  Further information For more information on SSH security and some of the flags set here, an excellent resource is Secure Secure Shell.  ","version":"Next","tagName":"h3"},{"title":"Modifying your .ssh/config file​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#modifying-your-sshconfig-file","content":" Please add the following lines to your ~/.ssh/config file:  Host greene.hpc.nyu.edu dtn.hpc.nyu.edu gw.hpc.nyu.edu StrictHostKeyChecking no ServerAliveInterval 60 ForwardAgent yes UserKnownHostsFile /dev/null LogLevel ERROR Host hpcgwtunnel HostName gw.hpc.nyu.edu ForwardX11 no StrictHostKeyChecking no LocalForward 8027 greene.hpc.nyu.edu:22 UserKnownHostsFile /dev/null User &lt;Your NetID&gt; Host greene HostName localhost Port 8027 ForwardX11 yes StrictHostKeyChecking no UserKnownHostsFile /dev/null LogLevel ERROR User &lt;Your NetID&gt;   You'll need to replace the sections above labelled &lt;Your NetID&gt; with your NetID. You can find more details about this at the Quickstart section of Accessing HPC at NYU  ","version":"Next","tagName":"h2"},{"title":"Logging onto the system​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#logging-onto-the-system","content":" With all of this in mind, let’s connect to our remote HPC system. In this tutorial, we will connect to Greene — an HPC system located at New York University. Although it’s unlikely that every system will be exactly like Greene, it’s a very good example of what you can expect from an HPC installation. To connect to Greene we will use SSH (if you are using PuTTY see details above).  SSH allows us to connect to UNIX computers remotely, and use them as if they were our own. The general syntax of the connection command follows the format:  ssh yourUsername@some.computer.address   Let’s attempt to connect to the HPC system now:  If you'd like to connect without typing your password you'll need to copy your public key file to greene first:  scp ~/.ssh/id_Greene_rsa.pub &lt;NetID&gt;@greene.hpc.nyu.edu:/home/&lt;NetID&gt;/.ssh/authorized_keys   If you are on NYU WiFi or VPN you can connect directly with:  ssh &lt;NetID&gt;@greene.hpc.nyu.edu   otherwise, you'll need to go through the NYU gateway first:  ssh &lt;NetID&gt;@gw.hpc.nyu.edu ssh &lt;NetID&gt;@greene.hpc.nyu.edu   When you are logged in you will see information about your last login, the host you've connected to, and your storage quota. It should look something like this:  Last login: Fri May 9 09:45:18 2025 from 0.0.0.0 Hostname: log-1 at Mon May 12 10:48:19 EDT 2025 Filesystem Environment Backed up? Allocation Current Usage Space Variable /Flushed? Space / Files Space(%) / Files(%) /home $HOME Yes/No 50.0GB/30.0K 23.74GB(47.48%)/4913(16.38%) /scratch $SCRATCH No/Yes 5.0TB/1.0M 35.91GB(0.70%)/19585(1.96%) /archive $ARCHIVE Yes/No 2.0TB/20.0K 0.00GB(0.00%)/1(0.00%) /vast $VAST NO/YES 2TB/5.0M 0.0TB(0.0%)/2(0%) [NetID@log-1 ~]$   By looking at the information after Hostname: and in the prompt you'll notice that the machine you're currently logged into is not Greene. This is expected. You've just logged into a login node that is connected to Greene. It is from the login nodes that you will submit jobs to Greene.  If you logged in using PuTTY this will not apply because it does not offer a local terminal.  ","version":"Next","tagName":"h2"},{"title":"Telling the Difference between the Local Terminal and the Remote Terminal​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#telling-the-difference-between-the-local-terminal-and-the-remote-terminal","content":" You can see that the prompt has changed after you log into a remote system. Let's take a closer look at the prompt after login: [NetID@log-1 ~]$ (in this example) tells us that we are logged into the login node log-1 with the identity NetID.  This change presents a small complication that we will need to navigate throughout this workshop. Exactly what is reported before the $ in the terminal when it is connected to the local system and the remote system will typically be different for every user. We still need to indicate which system we are entering commands on though so we will adopt the following convention:  [local]$ when the command is to be entered on a terminal connected to your local computer[NetID@glogin-1 ~]$ when the command is to be entered on a terminal connected to the remote system$ when it really doesn’t matter which system the terminal is connected to  ","version":"Next","tagName":"h3"},{"title":"Being certain which system your terminal is connected to​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#being-certain-which-system-your-terminal-is-connected-to","content":" If you ever need to be certain which system a terminal you are using is connected to then use the following command:  $ hostname   ","version":"Next","tagName":"h3"},{"title":"Keep two terminal windows open​","type":1,"pageTitle":"Connecting to the remote HPC system Greene","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/connecting_to_hpc/#keep-two-terminal-windows-open","content":" It is strongly recommended that you have two terminals open, one connected to the local system and one connected to the remote system that you can switch back and forth between. If you only use one terminal window then you will need to reconnect to the remote system using one of the methods above when you see a change from [local]$ to [NetID@login-1 ~]$ and disconnect when you see the reverse. ","version":"Next","tagName":"h3"},{"title":"Wildcards and Pipes","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/wildcards_pipes/","content":"","keywords":"","version":"Next"},{"title":"Redirecting output​","type":1,"pageTitle":"Wildcards and Pipes","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/wildcards_pipes/#redirecting-output","content":" Each of the commands we’ve used so far does only a very small amount of work. However, we can chain these small UNIX commands together to perform otherwise complicated actions!  For our first foray into piping, or redirecting output, we are going to use the &gt; operator to write output to a file. When using &gt;, whatever is on the left of the &gt; is written to the filename you specify on the right of the arrow. The actual syntax looks like command &gt; filename.  Let’s try several basic usages of &gt;. echo simply prints back, or echoes, whatever you type after it.  $ echo &quot;this is a test&quot; this is a test $ echo &quot;this is a test&quot; &gt; test.txt $ ls bash-lesson.tar.gz fastq dmel-all-r6.19.gtf gene_association.fb dmel_unique_protein_isoforms_fb_2016_01.tsv test.txt $ cat test.txt this is a test   Awesome, let’s try that with a more complicated command, like wc -l.  $ wc -l * &gt; word_counts.txt wc: fastq: Is a directory $ cat word_counts.txt 53037 bash-lesson.tar.gz 542048 dmel-all-r6.19.gtf 22129 dmel_unique_protein_isoforms_fb_2016_01.tsv 106290 gene_association.fb 1 test.txt 723505 total   Notice how we still got some output to the console even though we “piped” the output to a file? Our expected output still went to the file, but how did the error message get skipped and not go to the file?  This phenomena is an artefact of how UNIX systems are built. There are 3 input/output streams for every UNIX program you will run: stdin, stdout, and stderr.  Let’s dissect these three streams of input/output in the command we just ran: wc -l * &gt; word_counts.txt  stdin is the input to a program. In the command we just ran, stdin is represented by *, which is simply every filename in our current directory.stdout contains the actual, expected output. In this case, &gt; redirected stdout to the file word_counts.txt.stderr typically contains error messages and other information that doesn’t quite fit into the category of “output”. If we insist on redirecting both stdout and stderr to the same file we would use &amp;&gt; instead of &gt;. (We can redirect just stderr using 2&gt;.)  Knowing what we know now, let’s try re-running the command, and send all of the output (including the error message) to the same word_counts.txt files as before.  $ wc -l * &amp;&gt; word_counts.txt   Notice how there was no output to the console that time. Let’s check that the error message went to the file like we specified.  $ cat word_counts.txt 53037 bash-lesson.tar.gz 542048 dmel-all-r6.19.gtf 22129 dmel_unique_protein_isoforms_fb_2016_01.tsv wc: fastq: Is a directory 106290 gene_association.fb 1 test.txt 7 word_counts.txt 723512 total   Success! The wc: fastq: Is a directory error message was written to the file. Also, note how the file was silently overwritten by directing output to the same place as before. Sometimes this is not the behaviour we want. How do we append (add) to a file instead of overwriting it?  Appending to a file is done the same was as redirecting output. However, instead of &gt;, we will use &gt;&gt;.  $ echo &quot;We want to add this sentence to the end of our file&quot; &gt;&gt; word_counts.txt $ cat word_counts.txt 22129 dmel_unique_protein_isoforms_fb_2016_01.tsv 471308 Drosophila_melanogaster.BDGP5.77.gtf 0 fastq 1304914 fb_synonym_fb_2016_01.tsv 106290 gene_association.fb 1 test.txt 1904642 total We want to add this sentence to the end of our file   ","version":"Next","tagName":"h2"},{"title":"Chaining commands together​","type":1,"pageTitle":"Wildcards and Pipes","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/wildcards_pipes/#chaining-commands-together","content":" We now know how to redirect stdout and stderr to files. We can actually take this a step further and redirect output (stdout) from one command to serve as the input (stdin) for the next. To do this, we use the | (pipe) operator.  grep is an extremely useful command. It finds things for us within files. Basic usage (there are a lot of options for more clever things, see the man page) uses the syntax grep whatToFind fileToSearch. Let’s use grep to find all of the entries pertaining to the Act5C gene in Drosophila melanogaster.  $ grep Act5C dmel-all-r6.19.gtf   The output is nearly unintelligible since there is so much of it. Let’s send the output of that grep command to head so we can just take a peek at the first line. The | operator lets us send output from one command to the next:  $ grep Act5C dmel-all-r6.19.gtf | head -n 1 X\tFlyBase\tgene\t5900861\t5905399\t.\t+\t.\tgene_id &quot;FBgn0000042&quot;; gene_symbol &quot;Act5C&quot;;   Nice work, we sent the output of grep to head. Let’s try counting the number of entries for Act5C with wc -l. We can do the same trick to send grep’s output to wc -l:  $ grep Act5C dmel-all-r6.19.gtf | wc -l 46   note This is just the same as redirecting output to a file, then reading the number of lines from that file.  Writing commands using pipes How many files are there in the “fastq” directory we made earlier? (Use the shell to do this.) [Click for Solution] Solution ls fastq/ | wc -l Output of ls is one line per item, when chaining commands together like this, so counting lines gives the number of files.  Reading from compressed files Let’s compress one of our files using gzip. $ gzip gene_association.fb zcat acts like cat, except that it can read information from .gz (compressed) files. Using zcat, can you write a command to take a look at the top few lines of the gene_association.fb.gz file (without decompressing the file itself)? [Click for Solution] Solution zcat gene_association.fb.gz | head or for Mac: zcat &lt; gene_association.fb.gz | head zcat works a little differently on Macs. You'll need to use &lt; to explicitly input the file for zcat. The head command without any options shows the first 10 lines of a file.  Key Points The * wildcard is used as a placeholder to match any text that follows a pattern.Redirect a command’s output to a file with &gt;.Commands can be chained with | ","version":"Next","tagName":"h2"},{"title":"Moving around and looking at things","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/moving_looking/","content":"","keywords":"","version":"Next"},{"title":"System Architecture​","type":1,"pageTitle":"Moving around and looking at things","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/moving_looking/#system-architecture","content":" File Systems: The NYU HPC clusters have multiple file systems for user’s files. Each file system is configured differently to serve a different purpose.  Space\tEnvironment Variable\tSpace Purpose\tFlushed\tAllocation (per user)/home\t$HOME\tProgram development space; storing small files you want to keep long term, e.g. source code, scripts.\tNO\t20 GB /scratch\t$SCRATCH\tComputational workspace. Best suited to large, infrequent reads and writes.\tYES. Files not accessed for 60 days are deleted.\t5 TB /archive\t$ARCHIVE\tLong-term storage\tNO\t2 TB /vast\t$VAST\tFlash memory for high I/O workflows\tYES. Files not accessed for 60 days are deleted.\t2 TB  Please see HPC Storage for more details.  Right now, all we see is something that looks like this:  [NetID@login-1 ~]$   The dollar sign is a prompt, which shows us that the shell is waiting for input; your shell may use a different character as a prompt and may add information before the prompt. When typing commands, either from these lessons or from other sources, do not type the prompt, only the commands that follow it.  Type the command whoami, then press the Enter key (sometimes marked Return) to send the command to the shell. The command’s output is the ID of the current user, i.e., it shows us who the shell thinks we are:  $ whoami &lt;NetID&gt;   More specifically, when we type whoami in the shell:  finds a program called whoami,runs that program,displays that program’s output, thendisplays a new prompt to tell us that it’s ready for more commands.  Next, let’s find out where we are by running a command called pwd (which stands for “print working directory”). (“Directory” is another word for “folder”). At any moment, our current working directory (where we are) is the directory that the computer assumes we want to run commands in unless we explicitly specify something else. Here, the computer’s response is /home/&lt;NetID&gt;, which is &lt;NetID&gt;'s home directory. Note that the location of your home directory may differ from system to system.  $ pwd /home/&lt;NetID&gt;   So, we know where we are. How do we look and see what’s in our current directory?  $ ls   ls prints the names of the files and directories in the current directory in alphabetical order, arranged neatly into columns.  Differences between remote and local system Open a second terminal window on your local computer and run the ls command without logging in remotely. What differences do you see? [click to see the solution] SolutionYou probably see something like this: Output Application Documents Library Music Public Desktop Downloads Movies Pictures In addition you should also note that the preamble before the prompt ($) is different. This is very important for making sure you know what system you are issuing commands on when in the shell.  If nothing shows up when you run ls, it means that nothing’s there. Let’s make a directory for us to play with.  mkdir &lt;new directory name&gt; makes a new directory with that name in your current location. Notice that this command required two pieces of input: the actual name of the command (mkdir) and an argument that specifies the name of the directory you wish to create.  $ mkdir documents   Let’s ls again. What do we see?  Our folder is there, awesome. What if we wanted to go inside it and do stuff there? We will use the cd (change directory) command to move around. Let’s cd into our new documents folder.  $ cd documents $ pwd ~/documents   What is the ~ character? When using the shell, ~ is a shortcut that represents /home/&lt;NetID&gt;.  Now that we know how to use cd, we can go anywhere. That’s a lot of responsibility. What happens if we get “lost” and want to get back to where we started?  To go back to your home directory, the following three commands will work:  $ cd /home/&lt;NetID&gt; $ cd ~ $ cd   A quick note on the structure of a UNIX (Linux/Mac/Android/Solaris/etc) filesystem. Directories and absolute paths (i.e. exact position in the system) are always prefixed with a /. / by itself is the “root” or base directory.  Let’s go there now, look around, and then return to our home directory.  $ cd / $ ls $ cd ~ bin dev initrd local mnt proc root scratch tmp work boot etc lib localscratch nix project run srv usr cvmfs home lib64 media opt ram sbin sys var   The “home” directory is the one where we generally want to keep all of our files. Other folders on a UNIX OS contain system files, and get modified and changed as you install new software or upgrade your OS.  There are several other useful shortcuts you should be aware of.  . represents your current directory.. represents the “parent” directory of your current locationWhile typing nearly anything, you can have bash try to autocomplete what you are typing by pressing the tab key.  Let’s try these out now:  $ cd ./documents $ pwd $ cd .. $ pwd /home/&lt;NetID&gt;/documents /home/&lt;NetID&gt;   Many commands also have multiple behaviours that you can invoke with command line ‘flags.’ What is a flag? It’s generally just your command followed by a - and the name of the flag (sometimes it’s – followed by the name of the flag). You follow the flag(s) with any additional arguments you might need.  We’re going to demonstrate a couple of these “flags” using ls.  Show hidden files with -a. Hidden files are files that begin with ., these files will not appear otherwise, but that doesn’t mean they aren’t there! “Hidden” files are not hidden for security purposes, they are usually just config files and other tempfiles that the user doesn’t necessarily need to see all the time.  $ ls -a . .. .bash_logout .bash_profile .bashrc documents .emacs .mozilla .ssh   Notice how both . and .. are visible as hidden files. Show files, their size in bytes, date last modified, permissions, and other things with -l.  $ ls -l drwxr-xr-x 2 &lt;NetID&gt; tc001 4096 Jan 14 17:31 documents   This is a lot of information to take in at once and we will explain it all later! ls -l is extremely useful, and tells you almost everything you need to know about your files without actually looking at them.  We can also use multiple flags at the same time!  $ ls -l -a [yourUsername@gra-login1 ~]$ ls -la total 36 drwx--S--- 5 &lt;NetID&gt; tc001 4096 Nov 28 09:58 . drwxr-x--- 3 root tc001 4096 Nov 28 09:40 .. -rw-r--r-- 1 &lt;NetID&gt; tc001 18 Dec 6 2016 .bash_logout -rw-r--r-- 1 &lt;NetID&gt; tc001 193 Dec 6 2016 .bash_profile -rw-r--r-- 1 &lt;NetID&gt; tc001 231 Dec 6 2016 .bashrc drwxr-sr-x 2 &lt;NetID&gt; tc001 4096 Nov 28 09:58 documents -rw-r--r-- 1 &lt;NetID&gt; tc001 334 Mar 3 2017 .emacs drwxr-xr-x 4 &lt;NetID&gt; tc001 4096 Aug 2 2016 .mozilla drwx--S--- 2 &lt;NetID&gt; tc001 4096 Nov 28 09:58 .ssh   Flags generally precede any arguments passed to a UNIX command. ls actually takes an extra argument that specifies a directory to look into. When you use flags and arguments together, the syntax (how it’s supposed to be typed) generally looks something like this:  $ command &lt;flags/options&gt; &lt;arguments&gt;   So using ls -l -a on a different directory than the one we’re in would look something like:  $ ls -l -a ~/documents drwxr-sr-x 2 &lt;NetID&gt; tc001 4096 Nov 28 09:58 . drwx--S--- 5 &lt;NetID&gt; tc001 4096 Nov 28 09:58 ..   ","version":"Next","tagName":"h2"},{"title":"Where to go for help?​","type":1,"pageTitle":"Moving around and looking at things","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/moving_looking/#where-to-go-for-help","content":" How did I know about the -l and -a options? Is there a manual we can look at for help when we need help? There is a very helpful manual for most UNIX commands: man (if you’ve ever heard of a “man page” for something, this is what it is).  $ man ls LS(1) User Commands LS(1) NAME ls - list directory contents SYNOPSIS ls [OPTION]... [FILE]... DESCRIPTION List information about the FILEs (the current directory by default). Sort entries alphabetically if none of -cftuvSUX nor --sort is specified. Mandatory arguments to long options are mandatory for short options too.   To navigate through the man pages, you may use the up and down arrow keys to move line-by-line, or try the spacebar and b keys to skip up and down by full page. Quit the man pages by typing q.  Alternatively, most commands you run will have a --help option that displays addition information For instance, with ls:  $ ls --help Usage: ls [OPTION]... [FILE]... List information about the FILEs (the current directory by default). Sort entries alphabetically if none of -cftuvSUX nor --sort is specified. Mandatory arguments to long options are mandatory for short options too. -a, --all do not ignore entries starting with . -A, --almost-all do not list implied . and .. --author with -l, print the author of each file -b, --escape print C-style escapes for nongraphic characters --block-size=SIZE scale sizes by SIZE before printing them; e.g., '--block-size=M' prints sizes in units of 1,048,576 bytes; see SIZE format below -B, --ignore-backups do not list implied entries ending with ~ # further output omitted for clarity   Unsupported command-line options If you try to use an option that is not supported, ls and other programs will print an error message similar to this: [remote]$ ls -j Error ls: invalid option -- 'j' Try 'ls --help' for more information.  ","version":"Next","tagName":"h2"},{"title":"File System Challenge Questions​","type":1,"pageTitle":"Moving around and looking at things","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/moving_looking/#file-system-challenge-questions","content":" Looking at documentation Looking at the man page for ls or using ls --help, what does the -h (--human-readable) option do? [Click for Solution] Solution When used with the -l option, use unit suffixes: Byte, Kilobyte, Megabyte, Gigabyte, Terabyte and Petabyte in order to reduce the number of digits to four or fewer using base 2 for sizes. This option is not defined in IEEE Std 1003.1-2008 (“POSIX.1”).  Absolute vs Relative Paths Starting from /Users/amanda/data/, which of the following commands could Amanda use to navigate to her home directory, which is /Users/amanda? cd .cd /cd /home/amandacd ../..cd ~cd homecd ~/data/..cdcd .. [Click for Solution] Solution No: . stands for the current directory.No: / stands for the root directory.No: Amanda’s home directory is /Users/amanda.No: this goes up two levels, i.e. ends in /Users.Yes: ~ stands for the user’s home directory, in this case /Users/amanda.No: this would navigate into a directory home in the current directory if it exists.Yes: unnecessarily complicated, but correct.Yes: shortcut to go back to the user’s home directory.Yes: goes up one level.  Relative Path Resolution Using the filesystem diagram below, if pwd displays /Users/thing, what will ls -F ../backup display? ../backup: No such file or directory2012-12-01 2013-01-08 2013-01-272012-12-01/ 2013-01-08/ 2013-01-27/original/ pnas_final/ pnas_sub/ [Click for Solution] Solution No: there is a directory backup in /Users.No: this is the content of Users/thing/backup, but with .. we asked for one level further up.No: see previous explanation.Yes: ../backup/ refers to /Users/backup/.  ls Reading Comprehension Assuming a directory structure as in the above figure, if pwd displays /Users/backup, and -r tells ls to display things in reverse order, what command will display: Output pnas_sub/ pnas_final/ original/ ls pwdls -r -Fls -r -F /Users/backupEither #2 or #3 above, but not #1. [Click for Solution] Solution No: pwd is not the name of a directory.Yes: ls without directory argument lists files and directories in the current directory.Yes: uses the absolute path explicitly.Correct: see explanations above.  Exploring More ls Arguments What does the command ls do when used with the -l and -h arguments? Some of its output is about properties that we do not cover in this lesson (such as file permissions and ownership), but the rest should be useful nevertheless. [Click for Solution] Solution The -l arguments makes ls use a long listing format, showing not only the file/directory names but also additional information such as the file size and the time of its last modification. The -h argument makes the file size “human readable”, i.e. display something like 5.3K instead of 5369.  Listing Recursively and by Time The command ls -R lists the contents of directories recursively, i.e., lists their sub-directories, sub-sub-directories, and so on in alphabetical order at each level. The command ls -t lists things by time of last change, with most recently changed files or directories first. In what order does ls -R -t display things? Hint: ls -l uses a long listing format to view timestamps. [Click for Solution] Solution The directories are listed alphabetical at each level, the files/directories in each directory are sorted by time of last change.  Key Points Your current directory is referred to as the working directory.To change directories, use cd.To view files, use ls.You can view help for a command with man command or command --help.Hit tab to autocomplete whatever you’re currently typing. ","version":"Next","tagName":"h2"},{"title":"Scripts, variables, and loops","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/scripts_variables_loops/","content":"","keywords":"","version":"Next"},{"title":"Writing a Script​","type":1,"pageTitle":"Scripts, variables, and loops","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/scripts_variables_loops/#writing-a-script","content":" So how do we write a shell script, exactly? It turns out we can do this with a text editor. Start editing a file called demo.sh (to recap, we can do this with nano demo.sh). The .sh is the standard file extension for shell scripts that most people use (you may also see .bash used).  Our shell script will have two parts:  On the very first line, add #!/bin/bash. The #! (pronounced “hash-bang”) tells our computer what program to run our script with. In this case, we are telling it to run our script with our command-line shell (what we’ve been doing everything in so far). If we wanted our script to be run with something else, like Perl, we could use #!/usr/bin/perl instead.Now, anywhere below the first line, add echo &quot;Our script worked!&quot;. When our script runs, echo will happily print out &quot;Our script worked!&quot;.  Our file should now look like this:  #!/bin/bash echo &quot;Our script worked!&quot;   Ready to run our program? Let’s try running it:  $ demo.sh bash: demo.sh: command not found...   Strangely enough, Bash can’t find our script. As it turns out, Bash will only look in certain directories for scripts to run. To run anything else, we need to tell Bash exactly where to look. To run a script that we wrote ourselves, we need to specify the full path to the file, followed by the filename. We could do this one of two ways:  with our absolute path /home/yourNetID/demo.shwith the relative path ./demo.sh  $ ./demo.sh bash: ./demo.sh: Permission denied   There’s one last thing we need to do. Before a file can be run, it needs 'permission' to run. We'll get a better understanding of Linux file permissions in the next section that will allow us to finally run our script.  ","version":"Next","tagName":"h2"},{"title":"Permissions​","type":1,"pageTitle":"Scripts, variables, and loops","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/scripts_variables_loops/#permissions","content":" Let’s look at our file’s permissions with ls -l:  $ ls -l -rw-rw-r-- 1 yourNetID users 12534006 Jan 16 18:50 bash-lesson.tar.gz -rw-rw-r-- 1 yourNetID users 40 Jan 16 19:41 demo.sh -rw-rw-r-- 1 yourNetID users 77426528 Jan 16 18:50 dmel-all-r6.19.gtf -rw-r--r-- 1 yourNetID users 721242 Jan 25 2016 dmel_unique_protein_is... drwxrwxr-x 2 yourNetID users 4096 Jan 16 19:16 fastq -rw-r--r-- 1 yourNetID users 1830516 Jan 25 2016 gene_association.fb.gz -rw-rw-r-- 1 yourNetID users 15 Jan 16 19:17 test.txt -rw-rw-r-- 1 yourNetID users 245 Jan 16 19:24 word_counts.txt   That’s a huge amount of output: a full listing of everything in the directory. Let’s see if we can understand what each field of a given row represents, working from the left to right.  What each column of the output above means: File/Directory PermissionsReferencesOwnerGroupSize of itemTime last modifiedFilename This column contains a block of subcolumns that define the permissions for a file or directory given in each row. The permissions are shown for three user types to perform three actions each. The user types are: user (u): This refers to your permissions for this file/directory.group (g): This refers to the permissions for people in the same group as this file/directory. You will see the group in the 4th column.other (o): This refers to the permissions for all other users. The actions are: read (r): This refers to the permission to read this file.write (w): This refers to the permission to write to this file.execute (x): This refers to the permission to execute this file. The following table show what each of the subcolumns refer to and their possible values: directory\tuser read\tuser write\tuser execute\tgroup read\tgroup write\tgroup execute\tother read\tother write\tother executed or -\tr or -\tw or -\tx or -\tr or -\tw or -\tx or -\tr or -\tw or -\tx or - If there is a - in the directory column, the row refers to a file. If it contains a d, the row refers to a directory. The following columns behave in a similar manner. If they contain a -, the associated action is not allowed for the associated user type.  ","version":"Next","tagName":"h2"},{"title":"Changing Permissions​","type":1,"pageTitle":"Scripts, variables, and loops","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/scripts_variables_loops/#changing-permissions","content":" As previously mentioned, in Unix a file has three basic permissions, each of which can be set for three types of user. Those three permission also have a numeric value:  Read permission (“r”) - numeric value 4.Write permission (“w”) - numeric value 2.Execute permission (“x”) - numeric value 1.  note When applied to a directory, execute permission refers to whether the directory can be entered with cd.  You'll need to use the chmod command to modify permissions. You grant permissions with chmod who+what file and revoke them with chmod who-what file. (Notice that the first has + and the second -). Here, “who” is some combination of “u”, “g”, and “o”, and “what” is some combination of “r”, “w”, and “x”. Leaving out the who part of the command applies it to all user types.  So, to set execute permission we use:  $ chmod +x demo.sh $ ls -l -rw-rw-r-- 1 yourNetID users 12534006 Jan 16 18:50 bash-lesson.tar.gz -rwxrwxr-x 1 yourNetID users 40 Jan 16 19:41 demo.sh -rw-rw-r-- 1 yourNetID users 77426528 Jan 16 18:50 dmel-all-r6.19.gtf -rw-r--r-- 1 yourNetID users 721242 Jan 25 2016 dmel_unique_protein_is... drwxrwxr-x 2 yourNetID users 4096 Jan 16 19:16 fastq -rw-r--r-- 1 yourNetID users 1830516 Jan 25 2016 gene_association.fb.gz -rw-rw-r-- 1 yourNetID users 15 Jan 16 19:17 test.txt -rw-rw-r-- 1 yourNetID users 245 Jan 16 19:24 word_counts.txt   ","version":"Next","tagName":"h2"},{"title":"Executing Script​","type":1,"pageTitle":"Scripts, variables, and loops","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/scripts_variables_loops/#executing-script","content":" Now that we have executable permissions for that file, we can run it.  $ ./demo.sh   Our script worked! Fantastic, we’ve written our first program!  ","version":"Next","tagName":"h2"},{"title":"Comments​","type":1,"pageTitle":"Scripts, variables, and loops","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/scripts_variables_loops/#comments","content":" Before we go any further, let’s learn how to take notes inside our program using comments. A comment is indicated by the # character, followed by whatever we want. Comments do not get run. Let’s try out some comments in the console, then add one to our script!  # This won't show anything.   Now let's try adding this to our script with nano. Edit your script to look something like this:  #!/bin/bash # This is a comment... they are nice for making notes! echo &quot;Our script worked!&quot;   When we run our script, the output should be unchanged from before!  ","version":"Next","tagName":"h2"},{"title":"Shell variables​","type":1,"pageTitle":"Scripts, variables, and loops","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/scripts_variables_loops/#shell-variables","content":" One important concept that we’ll need to cover are shell variables. Variables are a great way of saving information under a name you can access later. In programming languages like Python and R, variables can store pretty much anything you can think of. In the shell, they usually just store text. The best way to understand how they work is to see them in action.  To set a variable, simply type in a name containing only letters, numbers, and underscores, followed by an = and whatever you want to put in the variable. Shell variable names are often uppercase by convention (but do not have to be).  $ VAR=&quot;This is our variable&quot;   To use a variable, prefix its name with a $ sign. Note that if we want to simply check what a variable is, we should use echo (or else the shell will try to run the contents of a variable).  $ echo $VAR This is our variable   Let’s try setting a variable in our script and then recalling its value as part of a command. We’re going to make it so our script runs wc -l on whichever file we specify with FILE.  Our script:  #!/bin/bash # set our variable to the name of our GTF file FILE=dmel-all-r6.19.gtf # call wc -l on our file wc -l $FILE   $ ./demo.sh 542048 dmel-all-r6.19.gtf   What if we wanted to do our little wc -l script on other files without having to change $FILE every time we want to use it? There is actually a special shell variable we can use in scripts that allows us to use arguments in our scripts (arguments are extra information that we can pass to our script, like the -l in wc -l).  To use the first argument to a script, use $1 (the second argument is $2, and so on). Let’s change our script to run wc -l on $1 instead of $FILE. Note that we can also pass all of the arguments using $@ (not going to use it in this lesson, but it’s something to be aware of).  Our script:  #!/bin/bash # call wc -l on our first argument wc -l $1   $ ./demo.sh dmel_unique_protein_isoforms_fb_2016_01.tsv 22129 dmel_unique_protein_isoforms_fb_2016_01.tsv   Nice! One thing to be aware of when using variables: they are all treated as pure text. How do we save the output of an actual command like ls -l?  First, a demonstration of what doesn’t work:  $ TEST=ls -l -bash: -l: command not found   What does work? We need to surround any command with $(command):  $ TEST=$(ls -l) $ echo $TEST total 90372 -rw-rw-r-- 1 jeff jeff 12534006 Jan 16 18:50 bash-lesson.tar.gz -rwxrwxr-x. 1 jeff jeff 40 Jan 1619:41 demo.sh -rw-rw-r-- 1 jeff jeff 77426528 Jan 16 18:50 dmel-all-r6.19.gtf -rw-r--r-- 1 jeff jeff 721242 Jan 25 2016 dmel_unique_protein_isoforms_fb_2016_01.tsv drwxrwxr-x. 2 jeff jeff 4096 Jan 16 19:16 fastq -rw-r--r-- 1 jeff jeff 1830516 Jan 25 2016 gene_association.fb.gz -rw-rw-r-- 1 jeff jeff 15 Jan 16 19:17 test.txt -rw-rw-r-- 1 jeff jeff 245 Jan 16 19:24 word_counts.txt   note Everything got printed on the same line. This is a feature, not a bug, as it allows us to use $(commands) inside lines of script without triggering line breaks (which would end our line of code and execute it prematurely).  ","version":"Next","tagName":"h2"},{"title":"Loops​","type":1,"pageTitle":"Scripts, variables, and loops","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/scripts_variables_loops/#loops","content":" To end our lesson on scripts, we are going to learn how to write a for-loop to execute a lot of commands at once. This will let us do the same string of commands on every file in a directory (or other stuff of that nature).  for-loops generally have the following syntax:  #!/bin/bash for VAR in first second third do echo $VAR done   When a for-loop gets run, the loop will run once for everything following the word in. In each iteration, the variable $VAR is set to a particular value for that iteration. In this case it will be set to first during the first iteration, second on the second, and so on. During each iteration, the code between do and done is performed.  Let’s run the script we just wrote (I saved mine as loop.sh).  $ chmod +x loop.sh $ ./loop.sh first second third   What if we wanted to loop over a shell variable, such as every file in the current directory? Shell variables work perfectly in for-loops. In this example, we’ll save the result of ls and loop over each file:  #!/bin/bash FILES=$(ls) for VAR in $FILES do echo $VAR done   $ ./loop.sh bash-lesson.tar.gz demo.sh dmel_unique_protein_isoforms_fb_2016_01.tsv dmel-all-r6.19.gtf fastq gene_association.fb.gz loop.sh test.txt word_counts.txt   There’s a shortcut to run on all files of a particular type, say all .gz files:  #!/bin/bash for VAR in *.gz do echo $VAR done   bash-lesson.tar.gz gene_association.fb.gz   Writing our own scripts and loops cd to our fastq directory from earlier and write a loop to print off the name and top 4 lines of every fastq file in that directory. Is there a way to only run the loop on fastq files ending in _1.fastq? [Click for Solution] Solution Create the following script in a file called head_all.sh #!/bin/bash for FILE in *.fasatq do echo $FILE head -n 4 $FILE done The for line could be modified to be for FILE in *_1.fastq to achieve the second aim.  Concatenating variables Concatenating (i.e. mashing together) variables is quite easy to do. Add whatever you want to concatenate to the beginning or end of the shell variable after enclosing it in {} characters. $ FILE=stuff.txt $ echo ${FILE}.example stuff.txt.example Can you write a script that prints off the name of every file in a directory with .processed added to it? [Click for Solution] Solution Create the following script in a file called process.sh: #!/bin/bash for FILE in * do echo ${FILE}.processed done note This will also print directories appended with .processed. To truly only get files and not directories, we need to modify this to use the find command to give us only files in the current directory: #!/bin/bash for FILE in $(find . -maxdepth 1 -type f) do echo ${FILE}.processed done but this will have the side effect of listing hidden files too. We can fix this by making a small change to the find command: #!/bin/bash for FILE in $(find . -maxdepth 1 -type f ! -name &quot;.*&quot;) do echo ${FILE}.processed done We've added ! -name &quot;.*&quot; to the find command. It means not (!) a name that starts with .. As you can see, programming is often iterative in more ways than one.   Special permissions What if we want to give different sets of users different permissions. chmod actually accepts special numeric codes instead of stuff like chmod +x, as we mentioned above. Again, the numeric codes are as follows: read = 4, write = 2, execute = 1. For each user we will assign permissions based on the sum of these permissions (must be between 7 and 0). Let’s make an example file and give everyone permission to do everything with it. $ touch example $ ls -l example -rw-r--r-- 1 yourNetID users 0 May 30 14:50 example $ chmod 777 example $ ls -l example -rwxrwxrwx 1 yourNetID users 0 May 30 14:50 example How might we give ourselves permission to do everything with a file, but allow no one else to do anything with it. [Click for Solution] Solution $ chmod 700 example $ ls -l example -rwx------ 1 yourNetID users 0 May 30 14:50 example We want all permissions, so: 4 (read) + 2 (write) + 1 (execute) = 7 for user (first position), no permissions, i.e. 0, for group (second position) and other (third position).  Key Points A shell script is just a list of bash commands in a text file.To make a shell script file executable, run chmod +x script.sh. ","version":"Next","tagName":"h2"},{"title":"DLP Interpretation Guide","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/dlp/dlp/","content":"","keywords":"","version":"Next"},{"title":"Viewing results from the DLP report​","type":1,"pageTitle":"DLP Interpretation Guide","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/dlp/dlp/#viewing-results-from-the-dlp-report","content":" This query, the most basic, fetches the first 100 flagged items in the report.  SELECT quote, info_type.name, info_type.sensitivity_score.score, likelihood FROM `your_table_name` LIMIT 100   Each row of the report contains a great deal of metadata on where the potentially sensitive metadata was found, as well as metadata on the DLP scan itself, but here we select only the following four columns:  quote: the span of text that was flagged as sensitive infoinfo_type.name: the type of sensitive infoInfo_type.sensitivity_score.score: the sensitivity level (LOW, MODERATE, or HIGH)likelihood: the confidence with which the DLP tool has flagged the item (POSSIBLE, LIKELY, or VERY_LIKELY)  The results should look something like this. As you can see, the same piece of text may be flagged multiple times with different types, depending on the results of DLP’s auto-detection algorithms.  To see more results, you can adjust the value of the LIMIT clause or remove it entirely. Alternatively, use some of the sample queries below to view targeted subsets of the data.  ","version":"Next","tagName":"h2"},{"title":"Sample Queries: selecting a subset of flagged items​","type":1,"pageTitle":"DLP Interpretation Guide","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/dlp/dlp/#sample-queries-selecting-a-subset-of-flagged-items","content":" Select only high-sensitivity items  SELECT quote, info_type.name, info_type.sensitivity_score.score, likelihood FROM `your_table_name` WHERE info_type.sensitivity_score.score = &quot;SENSITIVITY_HIGH&quot; LIMIT 100   Select only items that are high-sensitivity and have a likelihood higher than “possible”  SELECT quote, info_type.name, info_type.sensitivity_score.score, likelihood FROM `your_table_name` WHERE info_type.sensitivity_score.score = &quot;SENSITIVITY_HIGH&quot; AND likelihood != &quot;POSSIBLE&quot; LIMIT 100   Select all items, sorted by type  SELECT quote, info_type.name, info_type.sensitivity_score.score, likelihood FROM `your_table_name` ORDER BY info_type.name   Select all items of type PERSON_NAME, ordered alphabetically  SELECT quote, info_type.name, info_type.sensitivity_score.score, likelihood FROM `your_table_name` WHERE info_type.name = &quot;PERSON_NAME&quot; ORDER BY quote  ","version":"Next","tagName":"h2"},{"title":"About SRDE, projects and getting started","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/faq/basics/","content":"","keywords":"","version":"Next"},{"title":"What is the SRDE?​","type":1,"pageTitle":"About SRDE, projects and getting started","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/faq/basics/#what-is-the-srde","content":" NYU Secure Research Data Environment (SRDE) is a centralized secure computing platform designed to support research projects that require storage, sharing and analysis of high risk datasets. The team provides researchers with consultations and resources to comply with security requirements of research grants and Data Use Agreements. SRDE resources intend to meet the security controls outlined in the NIST 800-171 to safeguard Controlled Unclassified Information (CUI).  Technical description Please refer to our technical description here  ","version":"Next","tagName":"h2"},{"title":"Who can have an SRDE project?​","type":1,"pageTitle":"About SRDE, projects and getting started","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/faq/basics/#who-can-have-an-srde-project","content":" Access to SRDE is available to NYU researchers and sponsored members of their research team (i.e. co-investigators, research assistants, external collaborators).  IRB Waiver Requirement A project must be reviewed and approved (or receive a waiver) by the University's Institutional Review Board (IRB) in order to have an SRDE.  ","version":"Next","tagName":"h2"},{"title":"How do I sign up for an SRDE?​","type":1,"pageTitle":"About SRDE, projects and getting started","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/faq/basics/#how-do-i-sign-up-for-an-srde","content":" Fill out our intake form to provide more information about your project: Secure Research Data Environment intake form. Once we have received your form, the team will review the information and will contact you to schedule the consultation.  ","version":"Next","tagName":"h2"},{"title":"How much will the SRDE cost?​","type":1,"pageTitle":"About SRDE, projects and getting started","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/faq/basics/#how-much-will-the-srde-cost","content":" The cost is dependent on the needs of the project, such as size of the data and the type of machine needed for the analysis. Google Cloud has a calculator that will help estimate the costs: https://cloud.google.com/products/calculator  ","version":"Next","tagName":"h2"},{"title":"My data is not high risk, are there other options available?​","type":1,"pageTitle":"About SRDE, projects and getting started","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/faq/basics/#my-data-is-not-high-risk-are-there-other-options-available","content":" There are several options available depending on the data risk classification and the needs of the project. There are resources provided by the university such as research project space, cloud computing, etc. You can check out many of these services on the HPC Support Site. If you are unsure on how to proceed, a consultation with the SRDE team will help determine the best path forward.  ","version":"Next","tagName":"h2"},{"title":"What does the SRDE team need from me for the consultation?​","type":1,"pageTitle":"About SRDE, projects and getting started","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/faq/basics/#what-does-the-srde-team-need-from-me-for-the-consultation","content":" To help get things started it would be beneficial to submit an intake form with any related data governance documentation (files can be attached to the form), including, but not limited to, data use agreement, OSP/IRB documents, and project information. ","version":"Next","tagName":"h2"},{"title":"Writing and Reading Files","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/writing_reading_files/","content":"","keywords":"","version":"Next"},{"title":"Creating and Editing Text Files​","type":1,"pageTitle":"Writing and Reading Files","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/writing_reading_files/#creating-and-editing-text-files","content":" When working on an HPC system, we will frequently need to create or edit text files. Text is one of the simplest computer file formats, defined as a simple sequence of text lines.  What if we want to make a file? There are a few ways of doing this, the easiest of which is simply using a text editor. For this lesson, we are going to use nano, since it’s more intuitive than many other terminal text editors.  To create or edit a file, type nano &lt;filename&gt;, on the terminal, where &lt;filename&gt; is the name of the file. If the file does not already exist, it will be created. Let’s make a new file now, type whatever you want in it, and save it.  $ nano draft.txt     Nano defines a number of shortcut keys (prefixed by the Control or Ctrl key) to perform actions such as saving the file or exiting the editor. Here are the shortcut keys for a few common actions:  Ctrl+O — save the file (into a current name or a new name).Ctrl+X — exit the editor. If you have not saved your file upon exiting, nano will ask you if you want to save.Ctrl+K — cut (“kill”) a text line. This command deletes a line and saves it on a clipboard. If repeated multiple times without any interruption (key typing or cursor movement), it will cut a chunk of text lines.Ctrl+U — paste the cut text line (or lines). This command can be repeated to paste the same text elsewhere.  Option\tExplanationCtrl + O\tSave the changes Ctrl + X\tExit nano Ctrl + K\tCut single line Ctrl + U\tPaste the text  Using vim as a text editor From time to time, you may encounter the vim text editor. Although vim isn’t the easiest or most user-friendly of text editors, you’ll be able to find it on any system and it has many more features than nano. vim has several modes, a “command” mode (for doing big operations, like saving and quitting) and an “insert” mode. You can switch to insert mode with the i key, and command mode with Esc. In insert mode, you can type more or less normally. In command mode there are a few commands you should be aware of: :q! — quit, without saving:wq — save and quitdd — cut/delete a liney — paste a line  Do a quick check to confirm our file was created.  $ ls draft.txt   ","version":"Next","tagName":"h2"},{"title":"Reading Files​","type":1,"pageTitle":"Writing and Reading Files","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/writing_reading_files/#reading-files","content":" Let’s read the file we just created now. There are a few different ways of doing this, one of which is reading the entire file with cat.  $ cat draft.txt It's not &quot;publish or perish&quot; any more, it's &quot;share and thrive&quot;.   By default, cat prints out the content of the given file. Although cat may not seem like an intuitive command with which to read files, it stands for “concatenate”. Giving it multiple file names will print out the contents of the input files in the order specified in the cat’s invocation. For example:  $ cat draft.txt draft.txt It's not &quot;publish or perish&quot; any more, it's &quot;share and thrive&quot;. It's not &quot;publish or perish&quot; any more, it's &quot;share and thrive&quot;.   Reading Multiple Text Files Create two more files using nano, giving them different names such as chap1.txt and chap2.txt. Then use a single cat command to read and print the contents of draft.txt, chap1.txt, and chap2.txt.  ","version":"Next","tagName":"h2"},{"title":"Creating Directory​","type":1,"pageTitle":"Writing and Reading Files","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/writing_reading_files/#creating-directory","content":" We’ve successfully created a file. What about a directory? We’ve actually done this before, using mkdir.  $ mkdir files $ mkdir documents $ ls documents files draft.txt   ","version":"Next","tagName":"h2"},{"title":"Moving, Renaming, Copying Files​","type":1,"pageTitle":"Writing and Reading Files","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/writing_reading_files/#moving-renaming-copying-files","content":" Moving — We will move draft.txt to the files directory with mv (“move”) command. The same syntax works for both files and directories: mv &lt;file/directory&gt; &lt;new-location&gt;  $ mv draft.txt files $ cd files $ ls draft.txt   Command\tExplanationmv dummy_file.txt test_file.txt\tRenames dummy_file.txt as test_file.txt mv subdir new_subdir\tRenames the directory “subdir” to a new directory “new_subdir”  Renaming — draft.txt isn’t a very descriptive name. How do we go about changing it? It turns out that mv is also used to rename files and directories. Although this may not seem intuitive at first, think of it as moving a file to be stored under a different name. The syntax is quite similar to moving files: mv oldName newName  $ mv draft.txt newname.testfile $ ls newname.testfile   File extensions are arbitrary In the last example, we changed both a file’s name and extension at the same time. On UNIX systems, file extensions (like .txt) are arbitrary. A file is a .txt file only because we say it is. Changing the name or extension of the file will never change a file’s contents, so you are free to rename things as you wish. With that in mind, however, file extensions are a useful tool for keeping track of what type of data it contains. A .txt file typically contains text, for instance.  Copying — What if we want to copy a file, instead of simply renaming or moving it? Use cp command (an abbreviated name for “copy”). This command has two different uses that work in the same way as mv:  Copy to same directory (copied file is renamed): cp file newFilenameCopy to other directory (copied file retains original name): cp file directory  You can also combine these two operations in one command to copy a file to a different directory with a new name: cp file directory/newFilename  Command\tExplanationcp test_file1.txt test_file2.txt\tCopies a duplicate copy of test_file1.txt with the new name test_file2.txt cp -r subdir subdir2\tRecursively copies the directory “subdir” to a new directory “subdir2”. That is, a new directory “subdir2” is created, and each file and directory under “subdir” is replicated in “subdir2”.  Let’s try this out:  $ cp newname.testfile copy.testfile $ ls newname.testfile copy.testfile $ cp newname.testfile .. $ cd .. $ ls files documents newname.testfile   ","version":"Next","tagName":"h2"},{"title":"Removing files​","type":1,"pageTitle":"Writing and Reading Files","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/writing_reading_files/#removing-files","content":" We’ve begun to clutter up our workspace with all of the directories and files we’ve been making. Let’s learn how to get rid of them. One important note before we start… when you delete a file on UNIX systems, they are gone forever. There is no “recycle bin” or “trash”. Once a file is deleted, it is gone, never to return. So be very careful when deleting files.  Files are deleted with rm file [moreFiles]. To delete the newname.testfile in our current directory:  $ ls files documents newname.testfile $ rm newname.testfile $ ls files documents   That was simple enough. Directories are deleted in a similar manner using rmdir if the directory is empty or rm -r (the -r option stands for ‘recursive’) if the directory has contents.  $ ls files documents $ rmdir documents $ rmdir files rmdir: failed to remove `files/': Directory not empty $ ls files $ rm -r files $ ls   Command\tExplanationrm dummy_file.txt\tRemove a file rm -i dummy_file.txt\tIf you use -i you will be prompted for confirmation before each file is deleted. rm -f serious_file.txt\tForcibly removes a file without asking, regardless of its permissions (provided you own the file). rmdir subdir/\tRemoves “subdir” if it is already empty. Otherwise, the command fails. rm -r subdir/\tRecursively deletes the directory “subdir” and everything in it. Use it with care!  What happened? As it turns out, rmdir is unable to remove directories that have stuff in them. To delete a directory and everything inside it, we will use a special variant of rm, rm -rf directory. This is probably the scariest command on UNIX- it will force delete a directory and all of its contents without prompting. ALWAYS double check your typing before using it… if you leave out the arguments, it will attempt to delete everything on your file system that you have permission to delete. So when deleting directories be very, very careful.  What happens when you use rm -rf accidentally Steam is a major online sales platform for PC video games with over 125 million users. Despite this, it hasn’t always had the most stable or error-free code. In January 2015, user kevyin on GitHub reported that Steam’s Linux client had deleted every file on his computer. It turned out that one of the Steam programmers had added the following line: rm -rf &quot;$STEAMROOT/&quot;*. Due to the way that Steam was set up, the variable $STEAMROOT was never initialized, meaning the statement evaluated to rm -rf /*. This coding error in the Linux client meant that Steam deleted every single file on a computer when run in certain scenarios (including connected external hard drives). Moral of the story: be very careful when using rm -rf!  ","version":"Next","tagName":"h2"},{"title":"Looking at files​","type":1,"pageTitle":"Writing and Reading Files","url":"/rts-docs-dev/pr-preview/pr-115/docs/hpc/tutorial_intro_shell_hpc/writing_reading_files/#looking-at-files","content":" Sometimes it’s not practical to read an entire file with cat. The file might be way too large, take a long time to open, or maybe we want to only look at a certain part of the file. As an example, we are going to look at a large and complex file type used in bioinformatics, a .gtf file. The GTF2 format is commonly used to describe the location of genetic features in a genome.  Let’s grab and unpack a set of demo files for use later. To do this, we’ll use wget.  $ wget https://nyuhpc.github.io/hpc-shell/files/bash-lesson.tar.gz   Problems with wget? wget is a stand-alone application for downloading things over HTTP/HTTPS and FTP/FTPS connections, and it does the job admirably — when it is installed. Some operating systems instead come with cURL, which is the command-line interface to libcurl, a powerful library for programming interactions with remote resources over a wide variety of network protocols. If you have curl but not wget, then try this command instead: $ curl -O https://nyuhpc.github.io/hpc-shell/files/bash-lesson.tar.gz For very large downloads, you might consider using Aria2, which has support for downloading the same file from multiple mirrors. You have to install it separately, but if you have it, try this to get it faster than your neighbors: $ aria2c https://nyuhpc.github.io/hpc-shell/files/bash-lesson.tar.gz Install wget Linux: Debian, Ubuntu, Mint: sudo apt install wgetCentOS, Red Hat: sudo yum install wget or zypper install wgetFedora: sudo dnf install wget macOS: brew install wgetWindows: Download the Wget executable (wget.exe) from a reliable source.Place the wget.exe file in a directory that's included in your system's PATH environment variable (e.g., C:\\Windows\\System32).Open a command prompt and verify the installation by running wget --version Install cURL Linux: curl is packaged for every major distribution. You can install it through the usual means. Debian, Ubuntu, Mint: sudo apt install curlCentOS, Red Hat: sudo yum install curl or zypper install curlFedora: sudo dnf install curl macOS: curl is preinstalled on macOS. If you must have the latest version you can brew install it, but only do so if the stock version has failed you.Windows: curl comes preinstalled for the Windows 10 command line.For earlier Windows systems, you can download the executable directly; run it in place.curl comes preinstalled in Git for Windows and Windows Subsystem for Linux.On Cygwin, run the setup program again and select the curl package to install it. Install Aria2 Linux: every major distribution has an aria2 package. Install it by the usual means. Debian, Ubuntu, Mint: sudo apt install aria2CentOS, Red Hat: sudo yum install aria2 or zypper install aria2Fedora: sudo dnf install aria2 macOS: aria2c is available through homebrew: brew install aria2Windows: you have the following 2 options: download the latest release and run aria2c in place.Use the Windows Subsystem for Linux  You’ll commonly encounter .tar.gz archives while working in UNIX. To extract the files from a .tar.gz file, we run the command tar -xvf filename.tar.gz:  $ tar -xvf bash-lesson.tar.gz dmel-all-r6.19.gtf dmel_unique_protein_isoforms_fb_2016_01.tsv gene_association.fb SRR307023_1.fastq SRR307023_2.fastq SRR307024_1.fastq SRR307024_2.fastq SRR307025_1.fastq SRR307025_2.fastq SRR307026_1.fastq SRR307026_2.fastq SRR307027_1.fastq SRR307027_2.fastq SRR307028_1.fastq SRR307028_2.fastq SRR307029_1.fastq SRR307029_2.fastq SRR307030_1.fastq SRR307030_2.fastq   Unzipping files We just unzipped a .tar.gz file for this example. What if we run into other file formats that we need to unzip? Just use the handy reference below: gunzip extracts the contents of .gz filesunzip extracts the contents of .zip filestar -xvf extracts the contents of .tar.gz, .tgz and .tar.bz2 files  That is a lot of files! One of these files, dmel-all-r6.19.gtf is extremely large, and contains every annotated feature in the Drosophila melanogaster genome. It’s a huge file. What happens if we run cat on it? (Press Ctrl + C to stop it).  So, cat is a really bad option when reading big files… it scrolls through the entire file far too quickly! What are the alternatives? Try all of these out and see which ones you like best!  head file: Print the top 10 lines in a file to the console. You can control the number of lines you see with the -n numberOfLines flag.tail file: Same as head, but prints the last 10 lines in a file to the console.less file: Opens a file and display as much as possible on-screen. You can scroll with Enter or the arrow keys on your keyboard. Press q to close the viewer.  Out of cat, head, tail, and less, which method of reading files is your favourite? Why?  Key Points Use nano to create or edit text files from a terminal.Use cat file1 [file2 ...] to print the contents of one or more files to the terminal.Use mv old dir to move a file or directory old to another directory dir.Use mv old new to rename a file or directory old to a new name.Use cp old new to copy a file under a new name or location.Use cp old dir copies a file old into a directory dir.Use rm old to delete (remove) a file.File extensions are entirely arbitrary on UNIX systems. ","version":"Next","tagName":"h2"},{"title":"Environment and Roles","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/faq/env_roles/","content":"","keywords":"","version":"Next"},{"title":"Who is the Data Steward?​","type":1,"pageTitle":"Environment and Roles","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/faq/env_roles/#who-is-the-data-steward","content":" The data steward is an individual who is responsible for the ingress and egress of the data. The data steward is usually an IT administrator of the school the PI is associated with, they should be familiar with data sets and classification and may even cosign the Data Use Agreement (DUA) associated with the project.  Data Steward role If there is no such person, the project PI will need to assign the role to someone who will NOT be analyzing the data in the SRDE since this role does not have access to the research workspace in the SRDE (to enforce separation of duties). The SRDE team will provide role-based training for the data steward.  ","version":"Next","tagName":"h2"},{"title":"Will other project users have access to my files?​","type":1,"pageTitle":"Environment and Roles","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/faq/env_roles/#will-other-project-users-have-access-to-my-files","content":" Each user will have access to two drives within the SRDE workspace: home and scratch. The home drive is private and the scratch drive is shared. Any files for collaboration with other project team members on the workspace should be placed or copied over to the scratch drive.  ","version":"Next","tagName":"h2"},{"title":"What kind of software is available on the SRDE?​","type":1,"pageTitle":"Environment and Roles","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/faq/env_roles/#what-kind-of-software-is-available-on-the-srde","content":" Some statistical analysis software packages are preinstalled on the SRDE, such as Stata and MATLAB, other software is added on a case-by-case basis, dependent on the security and compatibility of the software with the SRDE.  ","version":"Next","tagName":"h2"},{"title":"How long will the data stay in the SRDE?​","type":1,"pageTitle":"Environment and Roles","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/faq/env_roles/#how-long-will-the-data-stay-in-the-srde","content":" Project lifecycle will be determined between the PI and the SRDE Team during the intake interview. ","version":"Next","tagName":"h2"},{"title":"Using the SRDE","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/faq/using_srde/","content":"","keywords":"","version":"Next"},{"title":"Will I receive training on how to use the SRDE?​","type":1,"pageTitle":"Using the SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/faq/using_srde/#will-i-receive-training-on-how-to-use-the-srde","content":" Absolutely! Once your SRDE is set up the SRDE team will schedule onboarding sessions for the research workspace users and a separate one for the data steward. In addition, all users will have access to the User Guide and SRDE support team for troubleshooting and guidance.  ","version":"Next","tagName":"h2"},{"title":"Why is the SRDE in a terminal, is there a screen layout?​","type":1,"pageTitle":"Using the SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/faq/using_srde/#why-is-the-srde-in-a-terminal-is-there-a-screen-layout","content":" At this time the SRDE is command-line only. We are working on an updated version with a graphical user interface (GUI) for projects with the need for it.  ","version":"Next","tagName":"h2"},{"title":"How do I export a file?​","type":1,"pageTitle":"Using the SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/faq/using_srde/#how-do-i-export-a-file","content":" All egress is done by the Data Steward. In order to have a file exported, please follow instructions in the SRDE User Guide to place the file in the export folder in the research workspace, then alert your Data Steward that there is a file ready for export.  ","version":"Next","tagName":"h2"},{"title":"Can I upload my own (non-sensitive) files to the SRDE?​","type":1,"pageTitle":"Using the SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/faq/using_srde/#can-i-upload-my-own-non-sensitive-files-to-the-srde","content":" In order to maintain the integrity of the data management flow into and out of the SRDE all data going into the environment needs to go through the Data Steward. This ensures both that the Data Steward is aware of all external data entering or leaving the environment and can confirm compliance with the DUA, as well as providing a single path for audit logging. To have your files uploaded to the environment, you can provide them to the Data Steward who can then upload them to the ingress bucket for your retrieval. ","version":"Next","tagName":"h2"},{"title":"Secure Research Data Environment (SRDE)","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/getting_started/eligibility_accounts/","content":"","keywords":"","version":"Next"},{"title":"SRDE Eligibility​","type":1,"pageTitle":"Secure Research Data Environment (SRDE)","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/getting_started/eligibility_accounts/#srde-eligibility","content":" A project must be reviewed and approved by the University's Institutional Review Board (IRB) (or receive a waiver) in order to have an SRDE. Access to SRDE is available to NYU researchers and sponsored members of their research team (i.e. co-investigators, research assistants, external collaborators). SRDE project owners and requesters must be in one of the eligible positions to act as a project Primary Investigator (PI), as outlined by the NYU IRB policy. This includes Tenured/Tenure Track Faculty, Continuing Contract faculty, Honorific Research Faculty, Professional Research Personnel, and Emeriti and retired faculty. The SRDE project owner should be the same as the PI of the IRB protocol or waiver.  Users on a research project must have valid and active NYU NetID credentials, including external, non-NYU collaborators, and be listed on the IRB protocol. Non-NYU Researchers/Collaborators need to obtain an affiliate status to obtain access. A full-time NYU faculty member must sponsor a non-NYU collaborator for affiliate status. Please see instructions for affiliate management (NYU NetID login is required to follow the link). A Data Steward must be a faculty member or university employee.  A Data Steward must be designated for each research project using SRDE and will be solely responsible for the transport of data for the project’s SRDE. Only users approved according to the Data Provider’s requirements are permitted to access project workspaces and related resources. These individuals are required to go through our on-boarding process, which includes signing a User Agreement and agreeing to our Terms of Use.  ","version":"Next","tagName":"h2"},{"title":"Requesting an SRDE Project​","type":1,"pageTitle":"Secure Research Data Environment (SRDE)","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/getting_started/eligibility_accounts/#requesting-an-srde-project","content":" The SRDE form contains details about the project, such as if the project requires IRB (Institutional Review Board) approval, technical requirements (such as data storage and software). The form will be submitted to the SRDE team for review.  Link to the Secure Research Data Environment Intake Form.  After you submit the intake form, the SRDE team will review the submitted documents and will respond to schedule the consultation. ","version":"Next","tagName":"h2"},{"title":"Start here!","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/getting_started/intro/","content":"Start here! Welcome to the Secure Research Data Environment documentation! If you do not have an active project, please proceed to the next section that explains the eligibility criteria and how you may request one. If you are an active user, you can proceed to one of the categories on the left. tip If you are looking to use REDCap, proceed to REDCap","keywords":"","version":"Next"},{"title":"REDCap at NYU","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/redcap/redcap/","content":"REDCap at NYU REDCap is a Research Electronic Data Capture tool, originally developed in Vanderbilt University. The REDCap Consortium, composed of many active institutional partners from overall the world, utilizes and supports the REDCap ecosystem. For more information on using REDCap at NYU, please proceed to https://sites.google.com/nyu.edu/redcap (VPN required).","keywords":"","version":"Next"},{"title":"Support","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/support/support/","content":"Support Please email your questions to: srde-support@nyu.edu","keywords":"","version":"Next"},{"title":"Best Practices","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/best_practices/","content":"","keywords":"","version":"Next"},{"title":"Shared Files​","type":1,"pageTitle":"Best Practices","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/best_practices/#shared-files","content":" Each user on the research workspace has their own home directory, as well as access to the top-level /shared partition.  ","version":"Next","tagName":"h2"},{"title":"Shared data files​","type":1,"pageTitle":"Best Practices","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/best_practices/#shared-data-files","content":" It is recommended to keep datasets under the /shared partition, especially if they are large. This is more efficient than each researcher making their own copy from the ingress bucket, and ensures all experiments are consistent with each other.  ","version":"Next","tagName":"h3"},{"title":"Shared code files​","type":1,"pageTitle":"Best Practices","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/best_practices/#shared-code-files","content":" Code files should also be stored under the /shared partition whenever possible. You can use a local git repo to keep a version history of your codebase, and to avoid conflicts from multiple developers working on the same file at once. To create a repo,  cd /shared/code git init   And then, after adding or modifying files,  git add * git commit -m “log message describing your change”   The git repo, with its full version history, can be exported alongside your results for transparency and reproducibility. ","version":"Next","tagName":"h3"},{"title":"Data Access","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/data_access/","content":"Data Access After you are connected to the Workspace Host using SSH per the instructions in the previous section, you can access data that has been placed in the workspace ingress bucket by your Data Steward. You can use the gsutil ls and cp commands to copy the data into your home directory using the steps described below. Use the following command to see list of folders in your workspace: gsutil ls As shown above, there are several folders. Data that has been transferred into the workspace is in the Ingress folder. Use the following command to list the objects in the ingress folder, replacing the path with your project’s path: gsutil ls gs://your-workspace-ingress-path List the contents of the folder with the timestamp corresponding to the date the data was transferred into the workspace, and you will see the files that were uploaded: gsutil ls gs://your-workspace-ingress-path/data-timestamp-folder To copy the files into your home directory use gsutil cp command (use period at end to copy to your home directory): gsutil cp gs://your-workspace-ingress-path/data-timestamp-folder/filename . ","keywords":"","version":"Next"},{"title":"Managing Data Transfer","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/data_transfers/","content":"","keywords":"","version":"Next"},{"title":"Data Ingestion process​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/data_transfers/#data-ingestion-process","content":" Ingesting data into the secure environment is a two-step process; First the Data Steward must upload the data onto the staging GCP Storage Bucket and then “push” the data into the secure Workspace environment.  ","version":"Next","tagName":"h2"},{"title":"Uploading Data to the Staging Area​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/data_transfers/#uploading-data-to-the-staging-area","content":" Option1: Using the Web Console Interface​  Log into GCP console, set project to your staging project (i.e. srde-staging-dev), and navigate on the side panel to Cloud Storage -&gt; Buckets:  Navigate to your research workspace’s corresponding Staging Ingress bucket:  Copy data to the Staging Ingress bucket:  Option2: Using the CLI​  Follow the instructions in section 2 to install and configure gcloud on your workstation. Once this is done, run the following command to find your workspace’s bucket:  gsutil ls | fgrep [Workspace Name]   The workspace name will be given to you by the SRDE team after your workspace has been provisioned. The command above should output two buckets– one will be for data ingest (ingress) and the other will be for data egress:  nyu10003@cloudshell:~ (srde-staging-dev-cedd)$ gsutil ls | fgrep example gs://nyu-us-east4-example-staging-egress-9d94/ gs://nyu-us-east4-example-staging-ingress-4bd9/   To ingest data into the SRDE, run the following command to copy individual files into the ingress bucket:  gsutil cp [FILENAME] gs://[INGRESS BUCKET]   So for instance, the following command would copy an individual text file (1661-0.txt) into the example ingress bucket:  gsutil cp 1661-0.txt gs://nyu-us-east4-example-staging-ingress-4bd9/   To copy a folder, you need to add -r after cp:  gsutil cp -r [FOLDER] gs://[INGRESS BUCKET]   We would use the following command to copy a folder named dataset into the example ingress bucket:  gsutil cp -r dataset gs://nyu-us-east4-example-staging-ingress-4bd9/   ","version":"Next","tagName":"h3"},{"title":"Push Data to the Research Workspace Using Airflow​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/data_transfers/#push-data-to-the-research-workspace-using-airflow","content":" Once the data is in the Staging Ingress bucket, navigate to Cloud Composer and click on Airflow:  In Airflow you will see the DAG workflows for your project. If you do not see any DAGs, contact srde-support@nyu.edu with subject line “Missing Airflow Permissions”  Once you see the workflows for your project, pick the one named [project-id]_Ingress_1_Staging_to_Workspace, which will bring you to the DAG page. On the DAG page, click on the “play” button at the top right to trigger the DAG:  The DAG may take a few minutes to run. You can see its progress in the status display on the bottom left.  The display shows a list of tasks executed by the DAG. A light green square will appear next to the task when it is running, and turn dark green when it is complete. When all tasks have finished successfully, the DAG is done.  Researchers will now be able to see the data in the ingress bucket in the research project workspace.  Access policy for Data Stewards Data stewards do not have access to the research project workspace.  Instructions for researchers who need to access the ingested data in the research workspace are found in the Data Access section of this document.  ","version":"Next","tagName":"h3"},{"title":"Data Egress Process​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/data_transfers/#data-egress-process","content":" To transport data out of the SRDE project workspace, research team members copy files to be exported to the 'export' folder in the Researcher Workspace Egress bucket, sample command below:  After the files have been copied to the export folder in the egress bucket within the workspace, researchers will notify the Data Steward that they are ready to export. The Data Steward will first move the files to the Staging Egress folder and scan them using the Data Loss Prevention API, a tool for automatically detecting sensitive data types. Next, they will check the generated report and either pass the inspection or fail it. Passing the inspection moves the data onwards to the Research Data Egress project for external sharing. Failing the inspection blocks the export.  ","version":"Next","tagName":"h2"},{"title":"Push the data from the Research Workspace to Staging​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/data_transfers/#push-the-data-from-the-research-workspace-to-staging","content":" First, run Egress DAG #1 to move files to the Staging Egress folder. Follow the same instructions as above to navigate to the Airflow page.  Once on the Airflow page, find the DAG named [project-id]_Egress_1_Workspace_to_Staging_Inspection.  Once on the DAG page, follow the steps to trigger the DAG, as instructed above. This DAG executes several tasks:  An archive copy of the export files is created within the workspace.The export files are moved to the staging environment.A DLP inspection is run to scan the exported files for sensitive data. The DLP scan may take some time to run, so wait for all tasks to be marked as successful (dark green) before proceeding.  ","version":"Next","tagName":"h3"},{"title":"Check the DLP inspection report​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/data_transfers/#check-the-dlp-inspection-report","content":" After Stage 1 is successfully completed, the DLP inspection findings are written to BigQuery. To examine results, navigate to BigQuery by going to Google console webpage, typing BigQuery on the search bar, and selecting it from the list.  Once in BigQuery, on the Explorer tab on the left, click on the corresponding project, then on the table that corresponds to the scan that was done. The name will contain the UTC date and time of the scan, using the format dlp_YYYY-MM-DD-HHMMSS. You can verify the report’s creation time under the “Details” tab.  Select “Query &gt; In new tab” to examine the results. The following default query will return a sample of 1000 results:  SELECT * FROM “table_id” LIMIT 1000   For more information on querying the DLP report, see the DLP Interpretation Guide (TODO Add section on DLP Interp. guide!)  Click on Run to run the query and review the results of the scan. After running the query you will see the results on the lower half of the window:  ","version":"Next","tagName":"h3"},{"title":"Pass or fail the inspection​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/data_transfers/#pass-or-fail-the-inspection","content":" Once the results are reviewed, the Data Steward approves or denies movement to the external egress bucket. They navigate back to the Airflow page and choose one of the following options:  If DLP scan results are NOT approved, Data Steward fails the data export by running Egress_2_Staging_Fail_inspection. Once on the DAG page, follow the steps to trigger the DAG, as instructed above. The data will be fully deleted from staging, and only the archived copy will remain in the workspace.If DLP scan results ARE approved, Data Steward passes the data export by running Egress_3_Staging_Pass_Inspection. Once on the DAG page, follow the steps to trigger the DAG, as instructed above. The data will be transferred to the project’s external egress bucket, where the researchers will be able to access and share it. After the final egress DAG completes successfully, the Data Steward should notify the researchers either a) that their data is available in the external egress bucket or b) that their data export was denied and why.  ","version":"Next","tagName":"h3"},{"title":"Moving Files to Export​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/data_transfers/#moving-files-to-export","content":" You can use the gsutil cp command to copy data from your home directory to the Egress export folder in the workspace using the following steps. Use the gsutil ls command to see the list of folders in your workspace. Copy your file into the Egress folder, adding /export/yourfilename to the Egress folder path:  gsutil cp data_file.txt gs://egress_bucket_path/export/data_file.txt     ","version":"Next","tagName":"h2"},{"title":"Auto-Inspection​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/data_transfers/#auto-inspection","content":" When files are added to the export folder, they are automatically scanned for sensitive data using the Data Loss Prevention API. This is the same tool that the Data Steward will use to examine your exported data and approve or deny the export, so you should review the results of auto-inspection carefully. Before notifying the Data Steward that an export is ready, make sure that the DLP inspection does not detect sensitive info, or that if it does, you are aware of the items it flags and can explain why they are false alarms.  The DLP scan is automatically triggered by any new file in the export folder. It may take several minutes to run. When it is complete, a summary file will be written back to the “dlp” folder in the egress bucket.  gsutil ls gs://egress_bucket_path/dlp   Within this folder, a folder is created for each exported file, and within that are dated summary reports for each version.  gsutil ls gs://egress_bucket_path/dlp/data_file.txt/   You should see a file of the format dlp_results_YYYY-MM-DD-HHMMSS corresponding to approximately when you added the file to the export folder. Note that the scan takes about a minute to pick up new files, and may behave oddly if you upload several versions very close together.  To see the summary file contents, use the command:  gsutil cat gs://egress_bucket_path/dlp/data_file.txt/dlp_results_YYYY-MM-DD-HHMMSS   If sensitive information is detected, you will see it listed by type and count  If no sensitive information is detected, you will see a clean scan report. Double-check that the “Processed bytes” and “Total estimated bytes” approximately line up with the size of your file–if both values are 0 it is likely that there was an error in the scan. ","version":"Next","tagName":"h2"},{"title":"Troubleshooting","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/troubleshooting/","content":"Troubleshooting Coming soon!","keywords":"","version":"Next"},{"title":"Connecting to SRDE","type":0,"sectionRef":"#","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/","content":"","keywords":"","version":"Next"},{"title":"Connecting through Google Cloud Console​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/#connecting-through-google-cloud-console","content":" Navigate to Google Cloud Console https://console.cloud.google.com/welcome and login with your NetID. Click the Select a project drop-down list at the top left corner of the page. In the Select a project window that appears, search and select the bastion project using the provided project ID (ex. test-dev1-bastion-1234).    Once selected, navigate to the VM Instances page via the Navigation menu (Menu in the top left corner of the page ) &gt; Compute Engine &gt; VM Instances. A running Bastion instance will be visible in the page as shown below:    ssh to the Bastion instance by clicking on the SSH button, a new SSH-in-browser tab will appear with a restricted CLI ( Command line interface ) connected to the instance. We are now inside the Bastion Host.    Now we can ssh to our workspace host by using the workspace internal IP address 10.0.0.2:  ssh 10.0.0.2   This will open the workspace CLI, with access to the workspace host having the computing needs to work on our data.  ","version":"Next","tagName":"h2"},{"title":"Connecting through Google Cloud Shell​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/#connecting-through-google-cloud-shell","content":" Navigate to https://shell.cloud.google.com/ while logged in using your NetID.  ","version":"Next","tagName":"h2"},{"title":"Setting project and zone​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/#setting-project-and-zone","content":" Note - Ask your SRDE administrator for the appropriate GCP PROJECT_ID and ZONE_NAME. Replace the values in the two commands below and run them  gcloud config set project PROJECT_ID gcloud config set compute/zone ZONE_NAME   ","version":"Next","tagName":"h3"},{"title":"Confirm settings​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/#confirm-settings","content":" Before proceeding, confirm that the project and zone match your GCP project ID and zone:  gcloud config list [compute] region = us-east4 zone = us-east4-a [core] account = netid@nyu.edu disable_usage_reporting = False project = test-dev1-bastion-1234 Your active configuration is: [default]   ","version":"Next","tagName":"h3"},{"title":"Generate SSH keys​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/#generate-ssh-keys","content":" Unused keys expire! Google Cloud Shell will delete your files, including generated SSH keys, if they are not accessed for 120 days. If this happens you will need to generate them again.  The simplest way to generate SSH keys is to delegate the key generation to gcloud. In order to trigger key creation, run the following command.  note Ignore the result of this command. It will most likely print errors to the output console.  gcloud compute ssh bastion-vm   You will be prompted to enter an SSH passphrase. This is optional, however it is recommended for additional user security.  The above command should log you into the bastion VM. You will see a prompt like:  -bash-4.4$”   Before proceeding, exit back to your local machine  exit   Then make sure the above step created two keys in your ssh home directory (~/.ssh) as shown below:  ls ~/.ssh     Start the ssh-agent on your local machine  eval `ssh-agent -s`   Add the google_compute_engine key to your ssh session  ssh-add ~/.ssh/google_compute_engine   Connect to the instance with gcloud using the –ssh-flag-”-A” flag  note This command uses the default project and zone set above.  gcloud compute ssh bastion-vm --ssh-flag=&quot;-A&quot; --tunnel-through-iap   ","version":"Next","tagName":"h3"},{"title":"Add SSH key to session​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/#add-ssh-key-to-session","content":" Run the following command to add the google_compute_engine key to the current session:ssh  ssh-add -L   Connect to the workstation-vm  ssh 10.0.0.2   ","version":"Next","tagName":"h3"},{"title":"Future logins​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/#future-logins","content":" After the initial login, you will not need to regenerate the SSH keys, but you will need the rest of the command sequence from “Start the SSH agent”. On your local machine:  eval `ssh-agent -s` ssh-add ~/.ssh/google_compute_engine gcloud compute ssh bastion-vm --ssh-flag=&quot;-A&quot; --tunnel-through-iap --project=PROJECT_ID   And then on the bastion VM:  ssh 10.0.0.2   ","version":"Next","tagName":"h3"},{"title":"Connecting on MacOS/Linux​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/#connecting-on-macoslinux","content":" ","version":"Next","tagName":"h2"},{"title":"Install gcloud CLI​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/#install-gcloud-cli","content":" Follow the official guidelines to install the latest version of gcloud CLI locally on your computer.  note After completing the gcloud installation, verify that the gcloud binary is in your $PATH environment variable.  ","version":"Next","tagName":"h3"},{"title":"Configure local gcloud settings​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/#configure-local-gcloud-settings","content":" Run the following command. It generates a link as shown below  gcloud auth login --no-launch-browser     Copy the link and open your chrome browser in incognito mode to perform user sign in.Username is your NYU NetID email address. For e.g. netid@nyu.edu  You will be redirected to the NYU SSO page and MFA verification through Duo Push. After successfully logging in, you will be asked to allow google SDK to access your account as shown below    Pressing the “Allow” button on this page will present the authorization code. Copy the code and paste it in the terminal. If this step is successful, you should see this text printed to the console. You are now logged in as [netid@nyu.edu].  ","version":"Next","tagName":"h3"},{"title":"Connect to the workspace​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/#connect-to-the-workspace","content":" Follow the same instructions for connecting with Google Cloud Shell above, starting from section on setting project and zone above.  ","version":"Next","tagName":"h3"},{"title":"Connecting on Windows 10/11​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/#connecting-on-windows-1011","content":" ","version":"Next","tagName":"h2"},{"title":"Start and Configure SSH-Agent Service​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/#start-and-configure-ssh-agent-service","content":" Using an elevated PowerShell window (run as admin), execute the following command to install the SSH-Agent service and configure it to start automatically when you log into your machine:  Get-Service ssh-agent | Set-Service -StartupType Automatic -PassThru | Start-Service     ","version":"Next","tagName":"h3"},{"title":"Install gcloud CLI​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/#install-gcloud-cli-1","content":" Download the [Google Cloud CLI installer] (https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe) and run the installer    Alternatively, run the following command to download and install:  (New-Object Net.WebClient).DownloadFile(&quot;https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe&quot;, &quot;$env:Temp\\GoogleCloudSDKInstaller.exe&quot;) &amp; $env:Temp\\GoogleCloudSDKInstaller.exe   ","version":"Next","tagName":"h3"},{"title":"Install Git​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/#install-git","content":" Download the Git Bash setup from the official website: https://git-scm.com/ and run the installer  ","version":"Next","tagName":"h3"},{"title":"Install Putty​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/#install-putty","content":" Download and install Putty from this link https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html  Post installation verify that the Putty authentication agent is installed and available  For 64-bit installer, you will find this executable at C:/Program Files/PuTTY/pageant.exe  ","version":"Next","tagName":"h3"},{"title":"Install Python (>version 3.0)​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/#install-python-version-30","content":" Install Python from the official website:https://www.python.org/downloads/  Remember to check “Add python to the environment path.” ***add screenshot  Make sure it's installed and available on PATH. On many systems Python comes pre-installed, you can try running the python command to start the Python interpreter to check and see if it is already installed.    On windows you can also try the py command which is a launcher which is more likely to work. If it is installed you will see a response which will include the version number, for example:    ","version":"Next","tagName":"h3"},{"title":"Logging in:​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/#logging-in","content":" Authenticate gcloud by starting a new session of command line or powershell. initialize and login to gcloud with your account (you will be redirected to the browser for authentication)  gcloud auth login       Run Git Bash and start the ssh-agent on your local machine  eval `ssh-agent -s`     Add the SSH key to agent by running  pageant.exe     The app runs in the background. you can find it in the tray.  Right click the icon and select &quot;Add Key&quot;. Add the google_compute_engine key with the PPK extension (~/.ssh/google_compute_engine) to your agent:  :::Skip this step in the future Go to the Pageant shortcut icon from the Windows Start Menu or your desktop.  Right click on the icon, and click on Properties. (If Properties is not an option on the menu, click on Open file location, then right click on the Pageant icon, and click on Properties)  :::    From the Shortcut tab, edit the Target field. Leave the path to pageant.exe intact. After that path, add the path to your Google .ppk key file.  Critical The key path should be outside the quotation marks. i  Here’s an example:  &quot;C:\\Program Files\\PuTTY\\pageant.exe&quot; C:\\Users\\Sam\\.ssh\\google_compute_engine.ppk     ","version":"Next","tagName":"h3"},{"title":"SSH into the bastion VM from Git Bash​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/#ssh-into-the-bastion-vm-from-git-bash","content":" tip Ask your SRDE administrator for the appropriate GCP project ID.  Replace gcp-project-id with that information in the below command:  export PROJECT_ID=gcp-project-id; gcloud compute ssh bastion-vm --ssh-flag=&quot;-A&quot; --zone=us-east4-a --tunnel-through-iap --project=${PROJECT_ID}     When SSHing to bastion in the git bash window, a new terminal in putty appears with the bastion connection  A PuTTY security alert window may pop up to accept the host key, click on Accept  ","version":"Next","tagName":"h3"},{"title":"Add SSH key to session​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/pr-preview/pr-115/docs/srde/user_guide/connecting/#add-ssh-key-to-session-1","content":" Run ssh-add to add the google_compute_engine key to the current session  ssh-add -L   Connect to the workstation-vm  ssh 10.0.0.2    ","version":"Next","tagName":"h3"}],"options":{"id":"default"}}