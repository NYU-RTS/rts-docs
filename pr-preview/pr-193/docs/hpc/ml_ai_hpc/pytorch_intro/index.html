<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-hpc/ml_ai_hpc/pytorch_intro" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.1">
<title data-rh="true">Single-GPU Training with PyTorch | Connecting researchers to computational resources.</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://services.rt.nyu.edu/pr-preview/pr-193/img/NYU.svg"><meta data-rh="true" name="twitter:image" content="https://services.rt.nyu.edu/pr-preview/pr-193/img/NYU.svg"><meta data-rh="true" property="og:url" content="https://services.rt.nyu.edu/pr-preview/pr-193/docs/hpc/ml_ai_hpc/pytorch_intro/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Single-GPU Training with PyTorch | Connecting researchers to computational resources."><meta data-rh="true" name="description" content="It is important to optimize your script for the single-GPU case before moving to multi-GPU training. This is because as you request more resources, your queue time increases. We also want to avoid wasting resources by running code that is not optimized."><meta data-rh="true" property="og:description" content="It is important to optimize your script for the single-GPU case before moving to multi-GPU training. This is because as you request more resources, your queue time increases. We also want to avoid wasting resources by running code that is not optimized."><link data-rh="true" rel="icon" href="/pr-preview/pr-193/img/NYU.ico"><link data-rh="true" rel="canonical" href="https://services.rt.nyu.edu/pr-preview/pr-193/docs/hpc/ml_ai_hpc/pytorch_intro/"><link data-rh="true" rel="alternate" href="https://services.rt.nyu.edu/pr-preview/pr-193/docs/hpc/ml_ai_hpc/pytorch_intro/" hreflang="en"><link data-rh="true" rel="alternate" href="https://services.rt.nyu.edu/pr-preview/pr-193/docs/hpc/ml_ai_hpc/pytorch_intro/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Single-GPU Training with PyTorch","item":"https://services.rt.nyu.edu/pr-preview/pr-193/docs/hpc/ml_ai_hpc/pytorch_intro"}]}</script><link rel="alternate" type="application/rss+xml" href="/pr-preview/pr-193/blog/rss.xml" title="Connecting researchers to computational resources. RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/pr-preview/pr-193/blog/atom.xml" title="Connecting researchers to computational resources. Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2DXB3BDH70"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-2DXB3BDH70",{anonymize_ip:!0})</script><link rel="stylesheet" href="/pr-preview/pr-193/assets/css/styles.f70cced0.css">
<script src="/pr-preview/pr-193/assets/js/runtime~main.0824a3e3.js" defer="defer"></script>
<script src="/pr-preview/pr-193/assets/js/main.bdfd79c4.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/pr-preview/pr-193/img/NYU.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_bHSa" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/pr-preview/pr-193/"><div class="navbar__logo"><img src="/pr-preview/pr-193/img/NYU.svg" alt="NYU torch logo" class="themedComponent_ypQ_ themedComponent--light_qQSo"><img src="/pr-preview/pr-193/img/NYU.svg" alt="NYU torch logo" class="themedComponent_ypQ_ themedComponent--dark_m0FH"></div><b class="navbar__title text--truncate">Research Technology Services</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/pr-preview/pr-193/docs/genai/getting_started/intro/">Pythia</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/pr-preview/pr-193/docs/hpc/getting_started/intro/">HPC</a><a class="navbar__item navbar__link" href="/pr-preview/pr-193/docs/cloud/getting_started/intro/">Cloud</a><a class="navbar__item navbar__link" href="/pr-preview/pr-193/docs/srde/getting_started/intro/">SRDE</a><a href="https://hsrn.nyu.edu/docs/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">HSRN<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_i9eD"><use href="#theme-svg-external-link"></use></svg></a><a class="navbar__item navbar__link" href="/pr-preview/pr-193/blog/">Announcements</a><div class="toggle_QBuD colorModeToggle_y_72"><button class="clean-btn toggleButton_lwp1 toggleButtonDisabled_HT4F" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_Djgd lightToggleIcon_DYGl"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_Djgd darkToggleIcon_IJRp"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_Djgd systemToggleIcon_dFXK"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_ASv1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_zMx_"><div class="docsWrapper_vlUk"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_nMwe" type="button"></button><div class="docRoot_wnou"><aside class="theme-doc-sidebar-container docSidebarContainer_JrTY"><div class="sidebarViewport_wVN_"><div class="sidebar_Zo3G"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_XvAV"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OTIz menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-193/docs/hpc/getting_started/intro/"><span title="Getting Started" class="categoryLinkLabel_IRz3">Getting Started</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OTIz menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-193/docs/hpc/connecting_to_hpc/connecting_to_hpc/"><span title="Setup" class="categoryLinkLabel_IRz3">Setup</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OTIz menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-193/docs/hpc/storage/intro_and_data_management/"><span title="Storage &amp; Data transfers" class="categoryLinkLabel_IRz3">Storage &amp; Data transfers</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OTIz menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-193/docs/hpc/datasets/intro/"><span title="Datasets" class="categoryLinkLabel_IRz3">Datasets</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OTIz menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-193/docs/hpc/submitting_jobs/slurm_submitting_jobs/"><span title="Submitting jobs" class="categoryLinkLabel_IRz3">Submitting jobs</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OTIz menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-193/docs/hpc/tools_and_software/intro/"><span title="Tools &amp; Software" class="categoryLinkLabel_IRz3">Tools &amp; Software</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OTIz menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-193/docs/hpc/containers/intro/"><span title="Containers" class="categoryLinkLabel_IRz3">Containers</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_OTIz menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/pr-preview/pr-193/docs/hpc/ml_ai_hpc/intro/"><span title="ML/AI on HPC" class="categoryLinkLabel_IRz3">ML/AI on HPC</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/pr-preview/pr-193/docs/hpc/ml_ai_hpc/intro/"><span title="Machine Learning on HPC" class="linkLabel_RgMO">Machine Learning on HPC</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/pr-preview/pr-193/docs/hpc/ml_ai_hpc/pytorch_intro/"><span title="Single-GPU Training with PyTorch" class="linkLabel_RgMO">Single-GPU Training with PyTorch</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/pr-preview/pr-193/docs/hpc/ml_ai_hpc/pytorch_dpp/"><span title="Multi-GPU Training with PyTorch: Distributed Data Parallel (DDP)" class="linkLabel_RgMO">Multi-GPU Training with PyTorch: Distributed Data Parallel (DDP)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/pr-preview/pr-193/docs/hpc/ml_ai_hpc/pytorch_lightning/"><span title="pytorch_lightning" class="linkLabel_RgMO">pytorch_lightning</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/pr-preview/pr-193/docs/hpc/ml_ai_hpc/pytorch_fsdp/"><span title="pytorch_fsdp" class="linkLabel_RgMO">pytorch_fsdp</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/pr-preview/pr-193/docs/hpc/ml_ai_hpc/tensorflow/"><span title="tensorflow" class="linkLabel_RgMO">tensorflow</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/pr-preview/pr-193/docs/hpc/ml_ai_hpc/run_hf_model/"><span title="Run a Hugging Face model" class="linkLabel_RgMO">Run a Hugging Face model</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/pr-preview/pr-193/docs/hpc/ml_ai_hpc/llm_fine_tuning/"><span title="Fine tune LLMs on HPC" class="linkLabel_RgMO">Fine tune LLMs on HPC</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OTIz menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-193/docs/hpc/ood/ood_intro/"><span title="Open OnDemand (OOD)" class="categoryLinkLabel_IRz3">Open OnDemand (OOD)</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/pr-preview/pr-193/docs/hpc/spec_sheet/"><span title="Torch Spec Sheet" class="linkLabel_RgMO">Torch Spec Sheet</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/pr-preview/pr-193/docs/hpc/system_status/"><span title="Greene System Status" class="linkLabel_RgMO">Greene System Status</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OTIz menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-193/docs/hpc/tutorial_intro_shell_hpc/intro/"><span title="Tutorial: Intro to Shell for HPC" class="categoryLinkLabel_IRz3">Tutorial: Intro to Shell for HPC</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OTIz menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-193/docs/hpc/tutorial_intro_hpc/intro_hpc/"><span title="Tutorial: Intro to HPC" class="categoryLinkLabel_IRz3">Tutorial: Intro to HPC</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_OTIz menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-193/docs/hpc/support/support/"><span title="Support" class="categoryLinkLabel_IRz3">Support</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_KK_H"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_K3iM"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_p7J4"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol__lxn"><div class="docItemContainer_T58M"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_mYRa" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/pr-preview/pr-193/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_einq"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">ML/AI on HPC</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Single-GPU Training with PyTorch</span></li></ul></nav><div class="tocCollapsible_FsOG theme-doc-toc-mobile tocMobile_e85Q"><button type="button" class="clean-btn tocCollapsibleButton_R0hy">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Single-GPU Training with PyTorch</h1></header>
<p>It is important to optimize your script for the single-GPU case before moving to multi-GPU training. This is because as you request more resources, your queue time increases. We also want to avoid wasting resources by running code that is not optimized.</p>
<p>Here we train a CNN on the MNIST dataset using a single GPU as an example. We profile the code and make performance improvements.</p>
<p>This tutorial uses PyTorch but the steps are similar for TensorFlow. See our <a href="/pr-preview/pr-193/docs/hpc/ml_ai_hpc/tensorflow/">TensorFlow</a> page for details.</p>
<h2 class="anchor anchorWithStickyNavbar_nzaP" id="step-1-activate-the-environment">Step 1: Activate the Environment<a href="#step-1-activate-the-environment" class="hash-link" aria-label="Direct link to Step 1: Activate the Environment" title="Direct link to Step 1: Activate the Environment" translate="no">​</a></h2>
<p>For simplicity we will use a pre-installed Conda environment. Run these commands to activate the environment:</p>
<div class="language-bash codeBlockContainer_Gtvh theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_q3dc"><pre tabindex="0" class="prism-code language-bash codeBlock_ggay thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_Y19p"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">$ </span><span class="token function" style="color:hsl(221, 87%, 60%)">ssh</span><span class="token plain"> </span><span class="token operator" style="color:hsl(221, 87%, 60%)">&lt;</span><span class="token plain">YourNetID</span><span class="token operator" style="color:hsl(221, 87%, 60%)">&gt;</span><span class="token plain">@adroit.princeton.edu</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">$ module load anaconda3/2023.9</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">$ conda activate /home/jdh4/.conda/envs/torch-env</span><br></span></code></pre></div></div>
<p>Watch a <a href="https://www.youtube.com/watch?v=wqTgM-Wq4YY&amp;t=296s" target="_blank" rel="noopener noreferrer">video</a> that covers everything on this page for single-GPU training with <a href="https://researchcomputing.princeton.edu/python-profiling" target="_blank" rel="noopener noreferrer">profiling Python</a> using <code>line_profiler</code>.</p>
<h2 class="anchor anchorWithStickyNavbar_nzaP" id="step-2-run-and-profile-the-script">Step 2: Run and Profile the Script<a href="#step-2-run-and-profile-the-script" class="hash-link" aria-label="Direct link to Step 2: Run and Profile the Script" title="Direct link to Step 2: Run and Profile the Script" translate="no">​</a></h2>
<div class="language-text codeBlockContainer_Gtvh theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_q3dc"><pre tabindex="0" class="prism-code language-text codeBlock_ggay thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_Y19p"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">fix link</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">First, inspect the script ([see script](mnist_classify.py)) by running these commands:</span><br></span></code></pre></div></div>
<div class="language-bash codeBlockContainer_Gtvh theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_q3dc"><pre tabindex="0" class="prism-code language-bash codeBlock_ggay thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_Y19p"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token punctuation" style="color:hsl(119, 34%, 47%)">(</span><span class="token plain">torch-env</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">)</span><span class="token plain"> $ </span><span class="token builtin class-name" style="color:hsl(35, 99%, 36%)">cd</span><span class="token plain"> multi_gpu_training/01_single_gpu</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">(</span><span class="token plain">torch-env</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">)</span><span class="token plain"> $ </span><span class="token function" style="color:hsl(221, 87%, 60%)">cat</span><span class="token plain"> mnist_classify.py</span><br></span></code></pre></div></div>
<p>We will profile the <code>train</code> function using <code>line_profiler</code> (see line 39) by adding the following decorator:</p>
<div class="language-python codeBlockContainer_Gtvh theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_q3dc"><pre tabindex="0" class="prism-code language-python codeBlock_ggay thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_Y19p"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token decorator annotation punctuation" style="color:hsl(119, 34%, 47%)">@profile</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token keyword" style="color:hsl(301, 63%, 40%)">def</span><span class="token plain"> </span><span class="token function" style="color:hsl(221, 87%, 60%)">train</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">(</span><span class="token plain">args</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"> model</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"> device</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"> train_loader</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"> optimizer</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"> epoch</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">)</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">:</span><br></span></code></pre></div></div>
<p>Next, download the data while on the login node since the compute nodes do not have internet access:</p>
<div class="language-text codeBlockContainer_Gtvh theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_q3dc"><pre tabindex="0" class="prism-code language-text codeBlock_ggay thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_Y19p"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">(torch-env) $ python download_mnist.py</span><br></span></code></pre></div></div>
<p>Below is the Slurm script:</p>
<div class="language-bash codeBlockContainer_Gtvh theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_q3dc"><pre tabindex="0" class="prism-code language-bash codeBlock_ggay thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_Y19p"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token shebang important" style="color:hsl(230, 8%, 24%);font-weight:bold">#!/bin/bash</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)">#SBATCH --job-name=mnist         # create a short name for your job</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)">#SBATCH --nodes=1                # node count</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)">#SBATCH --ntasks=1               # total number of tasks across all nodes</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)">#SBATCH --cpus-per-task=1        # cpu-cores per task (&gt;1 if multi-threaded tasks)</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)">#SBATCH --mem=8G                 # total memory per node (4 GB per cpu-core is default)</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)">#SBATCH --gres=gpu:1             # number of gpus per node</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)">#SBATCH --time=00:05:00          # total run time limit (HH:MM:SS)</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)">#SBATCH --mail-type=begin        # send email when job begins</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)">#SBATCH --mail-type=end          # send email when job ends</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)"># which gpu node was used</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token builtin class-name" style="color:hsl(35, 99%, 36%)">echo</span><span class="token plain"> </span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;Running on host&quot;</span><span class="token plain"> </span><span class="token variable" style="color:hsl(221, 87%, 60%)">$(</span><span class="token variable function" style="color:hsl(221, 87%, 60%)">hostname</span><span class="token variable" style="color:hsl(221, 87%, 60%)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)"># print the slurm environment variables sorted by name</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token function" style="color:hsl(221, 87%, 60%)">printenv</span><span class="token plain"> </span><span class="token operator" style="color:hsl(221, 87%, 60%)">|</span><span class="token plain"> </span><span class="token function" style="color:hsl(221, 87%, 60%)">grep</span><span class="token plain"> </span><span class="token parameter variable" style="color:hsl(221, 87%, 60%)">-i</span><span class="token plain"> slurm </span><span class="token operator" style="color:hsl(221, 87%, 60%)">|</span><span class="token plain"> </span><span class="token function" style="color:hsl(221, 87%, 60%)">sort</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">module purge</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">module load anaconda3/2023.9</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">conda activate /home/jdh4/.conda/envs/torch-env</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">kernprof </span><span class="token parameter variable" style="color:hsl(221, 87%, 60%)">-o</span><span class="token plain"> </span><span class="token variable" style="color:hsl(221, 87%, 60%)">${SLURM_JOBID}</span><span class="token plain">.lprof </span><span class="token parameter variable" style="color:hsl(221, 87%, 60%)">-l</span><span class="token plain"> mnist_classify.py </span><span class="token parameter variable" style="color:hsl(221, 87%, 60%)">--epochs</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token number" style="color:hsl(35, 99%, 36%)">3</span><br></span></code></pre></div></div>
<p><code>kernprof</code> is a profiler that wraps Python. Adroit has two different A100 nodes. Learn how to choose <a href="https://researchcomputing.princeton.edu/systems/adroit#gpus" target="_blank" rel="noopener noreferrer">specific nodes</a>.</p>
<p>Finally, submit the job while specifying the reservation:</p>
<div class="language-bash codeBlockContainer_Gtvh theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_q3dc"><pre tabindex="0" class="prism-code language-bash codeBlock_ggay thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_Y19p"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token punctuation" style="color:hsl(119, 34%, 47%)">(</span><span class="token plain">torch-env</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">)</span><span class="token plain"> $ sbatch </span><span class="token parameter variable" style="color:hsl(221, 87%, 60%)">--reservation</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token plain">multigpu job.slurm</span><br></span></code></pre></div></div>
<p>You should find that the code runs in about 20-40 seconds with 1 CPU-core depending on which A100 GPU node was used:</p>
<div class="language-text codeBlockContainer_Gtvh theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_q3dc"><pre tabindex="0" class="prism-code language-text codeBlock_ggay thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_Y19p"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">$ seff 1937315</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">Job ID: 1937315</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">Cluster: adroit</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">User/Group: aturing/cses</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">State: COMPLETED (exit code 0)</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">Cores: 1</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">CPU Utilized: 00:00:36</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">CPU Efficiency: 94.74% of 00:00:38 core-walltime</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">Job Wall-clock time: 00:00:38</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">Memory Utilized: 593.32 MB</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">Memory Efficiency: 7.24% of 8.00 GB</span><br></span></code></pre></div></div>
<p>For jobs that run for longer than 1 minute, one should use the <code>jobstats</code> command instead of <code>seff</code>. Use <code>shistory -n</code> to see which node was used or look in the <code>slurm-#######.out</code> file.</p>
<p>Some variation in the run time is expected when multiple users are running on the same node. Also, the two A100 GPU nodes are not equal:</p>
<table><thead><tr><th>hostname</th><th>CPU</th><th>GPU</th></tr></thead><tbody><tr><td>adroit-h11g1</td><td>Intel Xeon Gold 6442Y @ 2.6GHz</td><td>NVIDIA A100 80GB PCIe</td></tr><tr><td>adroit-h11g2</td><td>Intel Xeon Gold 6342  @ 2.8GHz</td><td>NVIDIA A100-PCIE-40GB</td></tr></tbody></table>
<h2 class="anchor anchorWithStickyNavbar_nzaP" id="step-3-analyze-the-profiling-data">Step 3: Analyze the Profiling Data<a href="#step-3-analyze-the-profiling-data" class="hash-link" aria-label="Direct link to Step 3: Analyze the Profiling Data" title="Direct link to Step 3: Analyze the Profiling Data" translate="no">​</a></h2>
<p>We installed <a href="https://researchcomputing.princeton.edu/python-profiling" target="_blank" rel="noopener noreferrer">line_profiler</a> into the Conda environment and profiled the code. To analyze the profiling data:</p>
<div class="language-text codeBlockContainer_Gtvh theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_q3dc"><pre tabindex="0" class="prism-code language-text codeBlock_ggay thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_Y19p"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">(torch-env) $ python -m line_profiler -rmt *.lprof </span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">Timer unit: 1e-06 s</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">Total time: 30.8937 s</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">File: mnist_classify.py</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">Function: train at line 39</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">Line #      Hits         Time  Per Hit   % Time  Line Contents</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">==============================================================</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    39                                           @profile</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    40                                           def train(args, model, device, train_loader, optimizer, epoch):</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    41         3        213.1     71.0      0.0      model.train()</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    42      2817   26106124.7   9267.3     84.5      for batch_idx, (data, target) in enumerate(train_loader):</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    43      2814     286242.0    101.7      0.9          data, target = data.to(device), target.to(device)</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    44      2814     296440.2    105.3      1.0          optimizer.zero_grad()</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    45      2814    1189206.1    422.6      3.8          output = model(data)</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    46      2814      81578.6     29.0      0.3          loss = F.nll_loss(output, target)</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    47      2814    1979990.2    703.6      6.4          loss.backward()</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    48      2814     841861.9    299.2      2.7          optimizer.step()</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    49      2814       2095.3      0.7      0.0          if batch_idx % args.log_interval == 0:</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    50       564       1852.9      3.3      0.0              print(&#x27;Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}&#x27;.format(</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    51       282       2218.6      7.9      0.0                  epoch, batch_idx * len(data), len(train_loader.dataset),</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    52       282     105753.3    375.0      0.3                  100. * batch_idx / len(train_loader), loss.item()))</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    53       282        119.2      0.4      0.0              if args.dry_run:</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    54                                                           break</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"> 30.89 seconds - mnist_classify.py:39 - train</span><br></span></code></pre></div></div>
<p>The slowest line is number 42 which consumes 84.5% of the time in the training function. That line involves <code>train_loader</code> which is the data loader for the training set. Are you surprised that the data loader is the slowest step and not the forward pass or calculation of the gradients? Can we improve on this?</p>
<h3 class="anchor anchorWithStickyNavbar_nzaP" id="examine-your-gpu-utilization">Examine Your GPU Utilization<a href="#examine-your-gpu-utilization" class="hash-link" aria-label="Direct link to Examine Your GPU Utilization" title="Direct link to Examine Your GPU Utilization" translate="no">​</a></h3>
<p>Use tools like <a href="https://researchcomputing.princeton.edu/support/knowledge-base/job-stats#jobstats" target="_blank" rel="noopener noreferrer">jobstats</a>, <a href="https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing#gpudash" target="_blank" rel="noopener noreferrer">gpudash</a> and <a href="https://researchcomputing.princeton.edu/support/knowledge-base/job-stats#stats.rc" target="_blank" rel="noopener noreferrer">stats.rc</a> to measure your GPU utilization. You can also do this on a <a href="https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing#gpu-utilization" target="_blank" rel="noopener noreferrer">compute node in real time</a>.</p>
<p>Note that GPU utilization as measured using nvidia-smi is only a measure of the fraction of the time that a GPU kernel is running on the GPU. It says nothing about how many CUDA cores are being used or how efficiently the GPU kernels have been written. However, for codes used by large communities, one can generally associate GPU utilization with overall GPU efficiency. For a more accurate measure of GPU utilization, use <a href="https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing#profiling" target="_blank" rel="noopener noreferrer">Nsight Systems or Nsight Compute</a> to measure the occupancy.</p>
<h2 class="anchor anchorWithStickyNavbar_nzaP" id="step-4-work-through-the-performance-tuning-guide">Step 4: Work through the Performance Tuning Guide<a href="#step-4-work-through-the-performance-tuning-guide" class="hash-link" aria-label="Direct link to Step 4: Work through the Performance Tuning Guide" title="Direct link to Step 4: Work through the Performance Tuning Guide" translate="no">​</a></h2>
<p>Make sure you optimize the single GPU case before going to multiple GPUs by working through the <a href="https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html" target="_blank" rel="noopener noreferrer">Performance Tuning Guide</a>.</p>
<h2 class="anchor anchorWithStickyNavbar_nzaP" id="step-5-optimize-your-script">Step 5: Optimize Your Script<a href="#step-5-optimize-your-script" class="hash-link" aria-label="Direct link to Step 5: Optimize Your Script" title="Direct link to Step 5: Optimize Your Script" translate="no">​</a></h2>
<p>One technique that was discussed in the <a href="https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html" target="_blank" rel="noopener noreferrer">Performance Tuning Guide</a> was using multiple CPU-cores to speed-up <a href="https://en.wikipedia.org/wiki/Extract,_transform,_load" target="_blank" rel="noopener noreferrer">ETL</a>. Let&#x27;s put this into practice.</p>
<p><img decoding="async" loading="lazy" src="https://www.telesens.co/wp-content/uploads/2019/04/img_5ca4eff975d80.png" alt="multiple_workers" class="img_mZZa"></p>
<p><em>Credit for image above is <a href="https://www.telesens.co/2019/04/04/distributed-data-parallel-training-using-pytorch-on-aws/" target="_blank" rel="noopener noreferrer">here</a>.</em></p>
<p>In <code>mnist_classify.py</code>, change <code>num_workers</code> from 1 to 8. And then in <code>job.slurm</code> change <code>--cpus-per-task</code> from 1 to 8. Then run the script again and note the speed-up:</p>
<div class="language-text codeBlockContainer_Gtvh theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_q3dc"><pre tabindex="0" class="prism-code language-text codeBlock_ggay thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_Y19p"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">(torch-env) $ sbatch --reservation=multigpu job.slurm</span><br></span></code></pre></div></div>
<p>How did the profiling data change? Watch the <a href="https://www.youtube.com/watch?v=wqTgM-Wq4YY&amp;t=296s" target="_blank" rel="noopener noreferrer">video</a> for the solution. For consistency between the Slurm script and PyTorch script, one can use:</p>
<div class="language-python codeBlockContainer_Gtvh theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_q3dc"><pre tabindex="0" class="prism-code language-python codeBlock_ggay thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_Y19p"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token keyword" style="color:hsl(301, 63%, 40%)">import</span><span class="token plain"> os</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">.</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">.</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">.</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    cuda_kwargs </span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token plain"> </span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">{</span><span class="token string" style="color:hsl(119, 34%, 47%)">&#x27;num_workers&#x27;</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">:</span><span class="token plain"> </span><span class="token builtin" style="color:hsl(119, 34%, 47%)">int</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">(</span><span class="token plain">os</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">.</span><span class="token plain">environ</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">[</span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;SLURM_CPUS_PER_TASK&quot;</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">]</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">)</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">.</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">.</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">.</span><br></span></code></pre></div></div>
<p>Several environment variables are set in the Slurm script. These can be referenced by the PyTorch script as demonstrated above. To see all of the available environment variables that are set in the Slurm script, add this line to <code>job.slurm</code>:</p>
<div class="language-text codeBlockContainer_Gtvh theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_q3dc"><pre tabindex="0" class="prism-code language-text codeBlock_ggay thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_Y19p"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">printenv | sort</span><br></span></code></pre></div></div>
<p>Consider these external data loading libraries: <a href="https://github.com/libffcv/ffcv" target="_blank" rel="noopener noreferrer">ffcv</a> and <a href="https://developer.nvidia.com/dali" target="_blank" rel="noopener noreferrer">NVIDIA DALI</a>.</p>
<h2 class="anchor anchorWithStickyNavbar_nzaP" id="summary">Summary<a href="#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary" translate="no">​</a></h2>
<p>It is essential to optimize your code before going to multi-GPU training since the inefficiencies will only be magnified otherwise. The more GPUs you request in a Slurm job, the longer you will wait for the job to run. If you can get your work done using an optimized script running on a single GPU then proceed that way. Do not use multiple GPUs if your GPU efficiency is low. The average GPU efficiency on Della is around 50%.</p>
<p>Next, we focus on scaling the code to multiple GPUs (go to <a href="/pr-preview/pr-193/docs/hpc/ml_ai_hpc/pytorch_dpp/">next section</a>).</p>
<h2 class="anchor anchorWithStickyNavbar_nzaP" id="how-was-the-conda-environment-made">How was the Conda environment made?<a href="#how-was-the-conda-environment-made" class="hash-link" aria-label="Direct link to How was the Conda environment made?" title="Direct link to How was the Conda environment made?" translate="no">​</a></h2>
<p>Your <code>/home</code> directory on Adroit probably has a capacity of 9.3 GB. To store Conda environments in another location see <a href="https://researchcomputing.princeton.edu/support/knowledge-base/checkquota" target="_blank" rel="noopener noreferrer">this page</a>. See the Research Computing knowledge base on <a href="https://researchcomputing.princeton.edu/support/knowledge-base/pytorch" target="_blank" rel="noopener noreferrer">PyTorch</a> for installation directions.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/NYU-RTS/rts-docs/blob/main/docs/hpc/08_ml_ai_hpc/02_pytorch_intro.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_liBE" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_si7g"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/pr-preview/pr-193/docs/hpc/ml_ai_hpc/intro/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Machine Learning on HPC</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/pr-preview/pr-193/docs/hpc/ml_ai_hpc/pytorch_dpp/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Multi-GPU Training with PyTorch: Distributed Data Parallel (DDP)</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_CFJu thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#step-1-activate-the-environment" class="table-of-contents__link toc-highlight">Step 1: Activate the Environment</a></li><li><a href="#step-2-run-and-profile-the-script" class="table-of-contents__link toc-highlight">Step 2: Run and Profile the Script</a></li><li><a href="#step-3-analyze-the-profiling-data" class="table-of-contents__link toc-highlight">Step 3: Analyze the Profiling Data</a><ul><li><a href="#examine-your-gpu-utilization" class="table-of-contents__link toc-highlight">Examine Your GPU Utilization</a></li></ul></li><li><a href="#step-4-work-through-the-performance-tuning-guide" class="table-of-contents__link toc-highlight">Step 4: Work through the Performance Tuning Guide</a></li><li><a href="#step-5-optimize-your-script" class="table-of-contents__link toc-highlight">Step 5: Optimize Your Script</a></li><li><a href="#summary" class="table-of-contents__link toc-highlight">Summary</a></li><li><a href="#how-was-the-conda-environment-made" class="table-of-contents__link toc-highlight">How was the Conda environment made?</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Contact</div><ul class="footer__items clean-list"><li class="footer__item"><a href="mailto:hpc@nyu.edu" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email HPC support<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_i9eD"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="mailto:hsrn-support@nyu.edu" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email HSRN support<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_i9eD"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="mailto:research-cloud-support@nyu.edu" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email Research Cloud support<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_i9eD"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="mailto:srde-support@nyu.edu" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email Secure Research Data Environment support<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_i9eD"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="mailto:genai-support@nyu.edu" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email general GenAI support<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_i9eD"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="mailto:genai-research-support@nyu.edu" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email GenAI for research support<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_i9eD"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Links</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://guides.nyu.edu/" target="_blank" rel="noopener noreferrer" class="footer__link-item">NYU Libraries Research Guides<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_i9eD"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://sites.google.com/nyu.edu/forc-camp/home" target="_blank" rel="noopener noreferrer" class="footer__link-item">FORC camp<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_i9eD"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://docs.google.com/forms/d/e/1FAIpQLSeHnmkPdR_IvWnT6a7U_V3RpfmQrpS8hjxI11FNnsZMlrBa4g/viewform" target="_blank" rel="noopener noreferrer" class="footer__link-item">Feedback</a></li><li class="footer__item"><a class="footer__link-item" href="/pr-preview/pr-193/blog/">Announcements</a></li><li class="footer__item"><a href="https://github.com/NYU-RTS/rts-docs" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_i9eD"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Made with 💜 in NYC!</div></div></div></footer></div>
</body>
</html>