"use strict";(self.webpackChunkrts_docs_dev=self.webpackChunkrts_docs_dev||[]).push([["8281"],{51807:function(e,n,r){r.r(n),r.d(n,{frontMatter:()=>o,toc:()=>d,default:()=>h,metadata:()=>t,assets:()=>l,contentTitle:()=>i});var t=JSON.parse('{"id":"hpc/ml_ai_hpc/pytorch_on_hpc_mulit_gpu","title":"Multi-GPU Training with PyTorch: Distributed Data Parallel (DDP)","description":"This was adaped from Princeton University Multi-GPU Training with PyTorch","source":"@site/docs/hpc/08_ml_ai_hpc/03_pytorch_on_hpc_mulit_gpu.md","sourceDirName":"hpc/08_ml_ai_hpc","slug":"/hpc/ml_ai_hpc/pytorch_on_hpc_mulit_gpu","permalink":"/pr-preview/pr-193/docs/hpc/ml_ai_hpc/pytorch_on_hpc_mulit_gpu","draft":false,"unlisted":false,"editUrl":"https://github.com/NYU-RTS/rts-docs/blob/main/docs/hpc/08_ml_ai_hpc/03_pytorch_on_hpc_mulit_gpu.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{},"sidebar":"hpcSidebar","previous":{"title":"Machine Learning on HPC","permalink":"/pr-preview/pr-193/docs/hpc/ml_ai_hpc/pytorch_intro"},"next":{"title":"Run a Hugging Face model","permalink":"/pr-preview/pr-193/docs/hpc/ml_ai_hpc/run_hf_model"}}'),a=r(47259),s=r(55511);let o={},i="Multi-GPU Training with PyTorch: Distributed Data Parallel (DDP)",l={},d=[{value:"Overall Idea of Distributed Data Parallel",id:"overall-idea-of-distributed-data-parallel",level:2},{value:"Main changes needed in going from single-GPU to multi-GPU training with DDP",id:"main-changes-needed-in-going-from-single-gpu-to-multi-gpu-training-with-ddp",level:2},{value:"Simple DDP Script",id:"simple-ddp-script",level:2},{value:"Job Arrays",id:"job-arrays",level:3},{value:"--ntasks-per-node versus --ntasks",id:"--ntasks-per-node-versus---ntasks",level:3},{value:"What is <code>local_rank</code>?",id:"what-is-local_rank",level:2},{value:"Total number of tasks equals total number of GPUs",id:"total-number-of-tasks-equals-total-number-of-gpus",level:2},{value:"Full Example of DDP",id:"full-example-of-ddp",level:2},{value:"Memory issues",id:"memory-issues",level:2}];function c(e){let n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"multi-gpu-training-with-pytorch-distributed-data-parallel-ddp",children:"Multi-GPU Training with PyTorch: Distributed Data Parallel (DDP)"})}),"\n",(0,a.jsx)(n.admonition,{type:"info",children:(0,a.jsxs)(n.p,{children:["This was adaped from ",(0,a.jsx)(n.a,{href:"https://github.com/PrincetonUniversity/multi_gpu_training",children:"Princeton University Multi-GPU Training with PyTorch"})]})}),"\n",(0,a.jsx)(n.p,{children:"One should always first try to use only a single GPU for training. This maximizes efficiency. There are two common reasons for using multiple GPUs when training neural networks:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"the execution time is too long with a single GPU"}),"\n",(0,a.jsx)(n.li,{children:"the model is too large to fit on a single GPU"}),"\n"]}),"\n",(0,a.jsx)(n.admonition,{type:"tip",children:(0,a.jsxs)(n.p,{children:["The more GPUs you request for a Slurm job, the longer the queue time will be. Learn how to conduct a ",(0,a.jsx)(n.a,{href:"https://researchcomputing.princeton.edu/support/knowledge-base/scaling-analysis",children:"scaling analysis"})," to find the optimal number of GPUs."]})}),"\n",(0,a.jsx)(n.h2,{id:"overall-idea-of-distributed-data-parallel",children:"Overall Idea of Distributed Data Parallel"}),"\n",(0,a.jsx)(n.p,{children:"The single-program, multiple data (SPMD) paradigm is used. That is, the model is copied to each of the GPUs. The input data is divided between the GPUs evenly. After the gradients have been computed they are averaged across all the GPUs. This is done in a way that all replicas have numerically identical values for the average gradients. The weights are then updated and once again they are identical by construction. The process then repeats with new mini-batches sent to the GPUs."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:"https://www.telesens.co/wp-content/uploads/2019/04/img_5ca570946ee1c.png",alt:"ddp"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsxs)(n.em,{children:["Credit for the image above is ",(0,a.jsx)(n.a,{href:"https://www.telesens.co/wp-content/uploads/2019/04/img_5ca570946ee1c.png",children:"here"}),"."]})}),"\n",(0,a.jsx)(n.p,{children:"If you would like more background then read through these PyTorch pages:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://pytorch.org/tutorials/intermediate/ddp_tutorial.html",children:"Getting Started with Distributed Data Parallel"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html",children:"See the docs on DPP"})}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Here are some webpages and videos:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://www.youtube.com/watch?v=TibQO_xv1zc",children:"YouTube Video"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://gist.github.com/TengdaHan/1dd10d335c7ca6f13810fff41e809904",children:"GitHub Gist"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://info.gwdg.de/wiki/doku.php?id=wiki:hpc:pytorch_on_the_hpc_clusters",children:"GWDG Webpage"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://sagemaker.readthedocs.io/en/stable/api/training/sdp_versions/latestsmd_data_parallel_pytorch.html",children:"SageMaker Webpage"})}),"\n"]}),"\n",(0,a.jsx)(n.admonition,{type:"warning",children:(0,a.jsxs)(n.p,{children:["Do not use ",(0,a.jsx)(n.code,{children:"DataParallel"})," in PyTorch for anything since it gives poor performance relative to ",(0,a.jsx)(n.code,{children:"DistributedDataParallel"}),"."]})}),"\n",(0,a.jsx)(n.h2,{id:"main-changes-needed-in-going-from-single-gpu-to-multi-gpu-training-with-ddp",children:"Main changes needed in going from single-GPU to multi-GPU training with DDP"}),"\n",(0,a.jsx)(n.p,{children:"This completely new piece is needed to form the process group:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def setup(rank, world_size):\n    # initialize the process group\n    dist.init_process_group("nccl", rank=rank, world_size=world_size)\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Note that ",(0,a.jsx)(n.code,{children:"dist.init_process_group()"})," is blocking. That means the code waits until all processes have reached that line and the command is successfully executed before going on. One should prefer ",(0,a.jsx)(n.code,{children:"nccl"})," over ",(0,a.jsx)(n.code,{children:"gloo"})," since the latter will use TCP by first moving tensor to CPU."]}),"\n",(0,a.jsx)(n.p,{children:"For the single-GPU training:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"model = Net().to(device)\noptimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n"})}),"\n",(0,a.jsx)(n.p,{children:"For multi-GPU training with DPP:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"model = Net().to(local_rank)\nddp_model = DDP(model, device_ids=[local_rank])\noptimizer = optim.Adadelta(ddp_model.parameters(), lr=args.lr)\n"})}),"\n",(0,a.jsxs)(n.p,{children:["More on ",(0,a.jsx)(n.code,{children:"local_rank"})," below. In short, this is the GPU index."]}),"\n",(0,a.jsx)(n.p,{children:"One also needs to ensure that a different batch is sent to each GPU:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'train_sampler = torch.utils.data.distributed.DistributedSampler(dataset1, num_replicas=world_size, rank=rank)\n    train_loader = torch.utils.data.DataLoader(dataset1, batch_size=args.batch_size, sampler=train_sampler, \\\n                                               num_workers=int(os.environ["SLURM_CPUS_PER_TASK"]), pin_memory=True)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"simple-ddp-script",children:"Simple DDP Script"}),"\n",(0,a.jsx)(n.p,{children:"The following can be used as a simple use case of DPP:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom socket import gethostname\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc = nn.Linear(42, 3)\n\n    def forward(self, x):\n        x = self.fc(x)\n        x = F.relu(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\nrank          = int(os.environ["SLURM_PROCID"])\nworld_size    = int(os.environ["WORLD_SIZE"])\ngpus_per_node = int(os.environ["SLURM_GPUS_ON_NODE"])\nassert gpus_per_node == torch.cuda.device_count()\nprint(f"Hello from rank {rank} of {world_size} on {gethostname()} where there are" \\\n      f" {gpus_per_node} allocated GPUs per node.", flush=True)\n\ndist.init_process_group("nccl", rank=rank, world_size=world_size)\nif rank == 0: print(f"Group initialized? {dist.is_initialized()}", flush=True)\n\nlocal_rank = rank - gpus_per_node * (rank // gpus_per_node)\ntorch.cuda.set_device(local_rank)\n\nmodel = Net().to(local_rank)\nddp_model = DDP(model, device_ids=[local_rank])\n\nddp_model.eval()\nwith torch.no_grad():\n  data = torch.rand(1, 42)\n  data = data.to(local_rank)\n  output = ddp_model(data)\n  print(f"host: {gethostname()}, rank: {rank}, output: {output}")\n\ndist.destroy_process_group()\n'})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"SLURM_PROCID"})," is a Slurm environment variable and it varies from 0 to N - 1, where N is the number of tasks running under ",(0,a.jsx)(n.code,{children:"srun"}),".  For instance, consider the abbreviated Slurm script below:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n...\nsrun python myscript.py\n"})}),"\n",(0,a.jsxs)(n.p,{children:["The Python interpreter will be launched 8 times (2 x 4) and each of the 8 tasks will have a different value of ",(0,a.jsx)(n.code,{children:"SLURM_PROCID"})," from\nthe set 0, 1, 2, 3, 4, 5, 6, 7."]}),"\n",(0,a.jsx)(n.p,{children:"Below is a full Slurm script for using DDP for Della (GPU) where there are 2 GPUs per node:"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"run-full.SBATCH:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=2\n#SBATCH --time=1:00:00\n#SBATCH --mem=2GB\n#SBATCH --gres=gpu:2\n#SBATCH --job-name=torch\n\nmodule purge\n\nexport MASTER_ADDR=$(hostname)\nexport MASTER_PORT=12345\nexport WORLD_SIZE=$SLURM_NTASKS\nexport RANK=$SLURM_PROCID\n\nsrun singularity exec --nv \\\n	    --overlay /scratch/NetID/pytorch-example/my_pytorch.ext3:ro \\\n	    /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif\\\n	    /bin/bash -c "source /ext3/env.sh; python simple_ddp.py"\n'})}),"\n",(0,a.jsxs)(n.p,{children:["In the script above, ",(0,a.jsx)(n.code,{children:"MASTER_PORT"}),", ",(0,a.jsx)(n.code,{children:"MASTER_ADDR"})," and ",(0,a.jsx)(n.code,{children:"WORLD_SIZE"})," are set. The three are later used to create the DDP process group. The total number of GPUs allocated to the job must be equal to ",(0,a.jsx)(n.code,{children:"WORLD_SIZE"})," -- this is satisfied above since nodes times ntasks-per-node is ",(0,a.jsx)(n.code,{children:"2 x 2 = 4"})," and number of GPUs allocated is nodes times gpus_per_node which is also ",(0,a.jsx)(n.code,{children:"2 x 2 = 4"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["This uses the overlay file for PyTorch we created in ",(0,a.jsx)(n.a,{href:"/pr-preview/pr-193/docs/hpc/containers/singularity_with_conda",children:"Singularity with Conda"})]}),"\n",(0,a.jsx)(n.h3,{id:"job-arrays",children:"Job Arrays"}),"\n",(0,a.jsxs)(n.p,{children:["For a job array, all jobs of the array have the same value of ",(0,a.jsx)(n.code,{children:"SLURM_JOBID"}),". Because of this, it is wise to modify ",(0,a.jsx)(n.code,{children:"MASTER_PORT"}),".\nHere is one possibility:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"export MASTER_PORT=$(expr 10000 + $(echo -n $SLURM_JOBID | tail -c 4) + $SLURM_ARRAY_TASK_ID)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"--ntasks-per-node-versus---ntasks",children:"--ntasks-per-node versus --ntasks"}),"\n",(0,a.jsxs)(n.p,{children:["Be sure to use ",(0,a.jsx)(n.code,{children:"--ntasks-per-node"})," and not ",(0,a.jsx)(n.code,{children:"--ntasks"})," in your Slurm script."]}),"\n",(0,a.jsxs)(n.h2,{id:"what-is-local_rank",children:["What is ",(0,a.jsx)(n.code,{children:"local_rank"}),"?"]}),"\n",(0,a.jsxs)(n.p,{children:["The indices of the GPUs on each node of your Slurm allocation begin at 0 and end at N - 1, where N is the total number of GPUs in your allocation on each node. Consider the case of 2 nodes and 8 tasks with 4 GPUs per node. The process ranks will be 0, 1, 2, 3 on the first node and 4, 5, 6, 7 on the second node while the GPU indices will be 0, 1, 2, 3 on the first and 0, 1, 2, 3 on the second. Thus, one cannot make calls such as ",(0,a.jsx)(n.code,{children:"data.to(rank)"})," since this will fail on the second node where there is a mismatch between the process ranks and the GPU indices. To deal with this a local rank is introduced:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'rank = int(os.environ["SLURM_PROCID"])\ngpus_per_node = int(os.environ["SLURM_GPUS_ON_NODE"])\nlocal_rank = rank - gpus_per_node * (rank // gpus_per_node)\n'})}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"local_rank"})," should be used everywhere in your script except when initializing the DDP process group where ",(0,a.jsx)(n.code,{children:"rank"})," should be used. In Python, one uses the  ",(0,a.jsx)(n.code,{children:"//"})," operator for integer division. For example, ",(0,a.jsx)(n.code,{children:"1 / 2 = 0.5"})," while ",(0,a.jsx)(n.code,{children:"1 // 2 = 0"}),"."]}),"\n",(0,a.jsx)(n.h1,{id:"ddp-and-slurm",children:"DDP and Slurm"}),"\n",(0,a.jsx)(n.h2,{id:"total-number-of-tasks-equals-total-number-of-gpus",children:"Total number of tasks equals total number of GPUs"}),"\n",(0,a.jsxs)(n.p,{children:["When using DDP, the total number of tasks must equal the total number of allocated GPUs. Therefore, if ",(0,a.jsx)(n.code,{children:"--ntasks-per-node=<N>"})," then you must have ",(0,a.jsx)(n.code,{children:"--gres=gpu:<N>"}),". Here are two examples:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"#SBATCH --ntasks-per-node=2\n#SBATCH --gres=gpu:2\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"#SBATCH --ntasks-per-node=4\n#SBATCH --gres=gpu:4\n"})}),"\n",(0,a.jsx)(n.p,{children:"You should take all of the GPUs on a node before going to multiple nodes. Never do one GPU per node for multinode jobs."}),"\n",(0,a.jsx)(n.h2,{id:"full-example-of-ddp",children:"Full Example of DDP"}),"\n",(0,a.jsx)(n.p,{children:"Below is an example Slurm script for DDP:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n#SBATCH --job-name=ddp-torch     # create a short name for your job\n#SBATCH --nodes=2                # node count\n#SBATCH --ntasks-per-node=2      # total number of tasks per node\n#SBATCH --cpus-per-task=8        # cpu-cores per task (>1 if multi-threaded tasks)\n#SBATCH --mem=32G                # total memory per node (4 GB per cpu-core is default)\n#SBATCH --gres=gpu:2             # number of gpus per node\n#SBATCH --time=00:05:00          # total run time limit (HH:MM:SS)\n\nexport MASTER_PORT=$(expr 10000 + $(echo -n $SLURM_JOBID | tail -c 4))\nexport WORLD_SIZE=$(($SLURM_NNODES * $SLURM_NTASKS_PER_NODE))\necho "WORLD_SIZE="$WORLD_SIZE\n\nmaster_addr=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)\nexport MASTER_ADDR=$master_addr\necho "MASTER_ADDR="$MASTER_ADDR\n\nsrun singularity exec --nv \\\n	    --overlay /scratch/NetID/pytorch-example/my_pytorch.ext3:ro \\\n	    /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif\\\n	    /bin/bash -c "source /ext3/env.sh; python download_data.py; python mnist_classify_ddp.py --epoch\ns=2"\n'})}),"\n",(0,a.jsxs)(n.p,{children:["The script above uses 2 nodes with 2 tasks per node and therefore 2 GPUs per node. This yields a total of 4 processes and each process can use 8 CPU-cores for data loading. An allocation of 4 GPUs is substantial so the queue time may be long. In all cases make sure that the GPUs are being used efficiently by monitoring the ",(0,a.jsx)(n.a,{href:"https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing",children:"GPU utilization"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"Below is the original single-GPU Python script modified to use DDP:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\n\nimport os\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom socket import gethostname\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\ndef train(args, model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n            if args.dry_run:\n                break\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\ndef setup(rank, world_size):\n    # initialize the process group\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef main():\n    # Training settings\n    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n                        help='number of epochs to train (default: 14)')\n    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n                        help='learning rate (default: 1.0)')\n    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n                        help='Learning rate step gamma (default: 0.7)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--dry-run', action='store_true', default=False,\n                        help='quickly check a single pass')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n                        help='how many batches to wait before logging training status')\n    parser.add_argument('--save-model', action='store_true', default=False,\n                        help='For Saving the current Model')\n    args = parser.parse_args()\n    use_cuda = not args.no_cuda and torch.cuda.is_available()\n\n    torch.manual_seed(args.seed)\n    \n    train_kwargs = {'batch_size': args.batch_size}\n    test_kwargs = {'batch_size': args.test_batch_size}\n    if use_cuda:\n        cuda_kwargs = {'num_workers': int(os.environ[\"SLURM_CPUS_PER_TASK\"]),\n                       'pin_memory': True,\n                       'shuffle': True}\n        train_kwargs.update(cuda_kwargs)\n        test_kwargs.update(cuda_kwargs)\n\n    transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n        ])\n    dataset1 = datasets.MNIST('data', train=True, download=False,\n                       transform=transform)\n    dataset2 = datasets.MNIST('data', train=False,\n                       transform=transform)\n \n    world_size    = int(os.environ[\"WORLD_SIZE\"])\n    rank          = int(os.environ[\"SLURM_PROCID\"])\n    gpus_per_node = int(os.environ[\"SLURM_GPUS_ON_NODE\"])\n    assert gpus_per_node == torch.cuda.device_count()\n    print(f\"Hello from rank {rank} of {world_size} on {gethostname()} where there are\" \\\n          f\" {gpus_per_node} allocated GPUs per node.\", flush=True)\n\n    setup(rank, world_size)\n    if rank == 0: print(f\"Group initialized? {dist.is_initialized()}\", flush=True)\n\n    local_rank = rank - gpus_per_node * (rank // gpus_per_node)\n    torch.cuda.set_device(local_rank)\n    print(f\"host: {gethostname()}, rank: {rank}, local_rank: {local_rank}\")\n\n    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset1, num_replicas=world_size, rank=rank)\n    train_loader = torch.utils.data.DataLoader(dataset1, batch_size=args.batch_size, sampler=train_sampler, \\\n                                               num_workers=int(os.environ[\"SLURM_CPUS_PER_TASK\"]), pin_memory=True)\n    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n\n    model = Net().to(local_rank)\n    ddp_model = DDP(model, device_ids=[local_rank])\n    optimizer = optim.Adadelta(ddp_model.parameters(), lr=args.lr)\n\n    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n    for epoch in range(1, args.epochs + 1):\n        train(args, ddp_model, local_rank, train_loader, optimizer, epoch)\n        if rank == 0: test(ddp_model, local_rank, test_loader)\n        scheduler.step()\n\n    if args.save_model and rank == 0:\n        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n\n    dist.destroy_process_group()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsxs)(n.p,{children:["In the script above the number of workers is taken directly from the value of ",(0,a.jsx)(n.code,{children:"--cpus-per-task"})," which is set in the Slurm script:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"cuda_kwargs = {'num_workers': int(os.environ[\"SLURM_CPUS_PER_TASK\"]), 'pin_memory': True, 'shuffle': True}\n"})}),"\n",(0,a.jsxs)(n.p,{children:["It also relies on the script ",(0,a.jsx)(n.a,{href:"https://github.com/PrincetonUniversity/multi_gpu_training/blob/main/01_single_gpu/download_data.py",children:"download_data.py"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import torchvision\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n# compute nodes do not have internet so download the data in advance\n\n_ = torchvision.datasets.MNIST(root='data',\n                               train=True,\n                               transform=None,\n                               target_transform=None,\n                               download=True)\n"})}),"\n",(0,a.jsx)(n.p,{children:"Execute the commands below to run the example above:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"[NetID@log-1 full-ddp-test]$ sbatch run-full.SBATCH\n"})}),"\n",(0,a.jsx)(n.h2,{id:"memory-issues",children:"Memory issues"}),"\n",(0,a.jsxs)(n.p,{children:["Use ",(0,a.jsx)(n.code,{children:"gradient_as_bucket_view=True"})," when making the DDP model to decrease the required memory by 1/3."]})]})}function h(e={}){let{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},55511:function(e,n,r){r.d(n,{R:()=>o,x:()=>i});var t=r(96363);let a={},s=t.createContext(a);function o(e){let n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);