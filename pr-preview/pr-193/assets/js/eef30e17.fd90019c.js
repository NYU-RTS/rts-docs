"use strict";(self.webpackChunkrts_docs_dev=self.webpackChunkrts_docs_dev||[]).push([["2222"],{54065:function(t,e,n){n.r(e),n.d(e,{frontMatter:()=>p,toc:()=>a,default:()=>h,metadata:()=>r,assets:()=>s,contentTitle:()=>o});var r=JSON.parse('{"id":"hpc/ml_ai_hpc/pytorch_lightning","title":"pytorch_lightning","description":"","source":"@site/docs/hpc/08_ml_ai_hpc/04_pytorch_lightning.md","sourceDirName":"hpc/08_ml_ai_hpc","slug":"/hpc/ml_ai_hpc/pytorch_lightning","permalink":"/pr-preview/pr-193/docs/hpc/ml_ai_hpc/pytorch_lightning","draft":false,"unlisted":false,"editUrl":"https://github.com/NYU-RTS/rts-docs/blob/main/docs/hpc/08_ml_ai_hpc/04_pytorch_lightning.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{},"sidebar":"hpcSidebar","previous":{"title":"Multi-GPU Training with PyTorch: Distributed Data Parallel (DDP)","permalink":"/pr-preview/pr-193/docs/hpc/ml_ai_hpc/pytorch_dpp"},"next":{"title":"pytorch_fsdp","permalink":"/pr-preview/pr-193/docs/hpc/ml_ai_hpc/pytorch_fsdp"}}'),i=n(47259),c=n(55511);let p={},o,s={},a=[];function l(t){return(0,i.jsx)(i.Fragment,{})}function h(t={}){let{wrapper:e}={...(0,c.R)(),...t.components};return e?(0,i.jsx)(e,{...t,children:(0,i.jsx)(l,{...t})}):l(t)}},55511:function(t,e,n){n.d(e,{R:()=>p,x:()=>o});var r=n(96363);let i={},c=r.createContext(i);function p(t){let e=r.useContext(c);return r.useMemo(function(){return"function"==typeof t?t(e):{...e,...t}},[e,t])}function o(t){let e;return e=t.disableParentContext?"function"==typeof t.components?t.components(i):t.components||i:p(t.components),r.createElement(c.Provider,{value:e},t.children)}}}]);