"use strict";(self.webpackChunkrts_docs_dev=self.webpackChunkrts_docs_dev||[]).push([["808"],{45115:function(e,n,t){t.r(n),t.d(n,{frontMatter:()=>s,toc:()=>h,default:()=>d,metadata:()=>i,assets:()=>c,contentTitle:()=>a});var i=JSON.parse('{"id":"hpc/ml_ai_hpc/pytorch_intro","title":"Single-GPU Training with PyTorch","description":"It is important to optimize your script for the single-GPU case before moving to multi-GPU training. This is because as you request more resources, your queue time increases. We also want to avoid wasting resources by running code that is not optimized.","source":"@site/docs/hpc/08_ml_ai_hpc/02_pytorch_intro.md","sourceDirName":"hpc/08_ml_ai_hpc","slug":"/hpc/ml_ai_hpc/pytorch_intro","permalink":"/pr-preview/pr-193/docs/hpc/ml_ai_hpc/pytorch_intro","draft":false,"unlisted":false,"editUrl":"https://github.com/NYU-RTS/rts-docs/blob/main/docs/hpc/08_ml_ai_hpc/02_pytorch_intro.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{},"sidebar":"hpcSidebar","previous":{"title":"Machine Learning on HPC","permalink":"/pr-preview/pr-193/docs/hpc/ml_ai_hpc/intro"},"next":{"title":"Multi-GPU Training with PyTorch: Distributed Data Parallel (DDP)","permalink":"/pr-preview/pr-193/docs/hpc/ml_ai_hpc/pytorch_dpp"}}'),r=t(47259),o=t(55511);let s={},a="Single-GPU Training with PyTorch",c={},h=[{value:"Step 1: Activate the Environment",id:"step-1-activate-the-environment",level:2},{value:"Step 2: Run and Profile the Script",id:"step-2-run-and-profile-the-script",level:2},{value:"Step 3: Analyze the Profiling Data",id:"step-3-analyze-the-profiling-data",level:2},{value:"Examine Your GPU Utilization",id:"examine-your-gpu-utilization",level:3},{value:"Step 4: Work through the Performance Tuning Guide",id:"step-4-work-through-the-performance-tuning-guide",level:2},{value:"Step 5: Optimize Your Script",id:"step-5-optimize-your-script",level:2},{value:"Summary",id:"summary",level:2},{value:"How was the Conda environment made?",id:"how-was-the-conda-environment-made",level:2}];function l(e){let n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"single-gpu-training-with-pytorch",children:"Single-GPU Training with PyTorch"})}),"\n",(0,r.jsx)(n.p,{children:"It is important to optimize your script for the single-GPU case before moving to multi-GPU training. This is because as you request more resources, your queue time increases. We also want to avoid wasting resources by running code that is not optimized."}),"\n",(0,r.jsx)(n.p,{children:"Here we train a CNN on the MNIST dataset using a single GPU as an example. We profile the code and make performance improvements."}),"\n",(0,r.jsxs)(n.p,{children:["This tutorial uses PyTorch but the steps are similar for TensorFlow. See our ",(0,r.jsx)(n.a,{href:"/pr-preview/pr-193/docs/hpc/ml_ai_hpc/tensorflow",children:"TensorFlow"})," page for details."]}),"\n",(0,r.jsx)(n.h2,{id:"step-1-activate-the-environment",children:"Step 1: Activate the Environment"}),"\n",(0,r.jsx)(n.p,{children:"For simplicity we will use a pre-installed Conda environmnet. Run these commands to activate the environment:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ ssh <YourNetID>@adroit.princeton.edu\n$ module load anaconda3/2023.9\n$ conda activate /home/jdh4/.conda/envs/torch-env\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Watch a ",(0,r.jsx)(n.a,{href:"https://www.youtube.com/watch?v=wqTgM-Wq4YY&t=296s",children:"video"})," that covers everything on this page for single-GPU training with ",(0,r.jsx)(n.a,{href:"https://researchcomputing.princeton.edu/python-profiling",children:"profiling Python"})," using ",(0,r.jsx)(n.code,{children:"line_profiler"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"step-2-run-and-profile-the-script",children:"Step 2: Run and Profile the Script"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"fix link\nFirst, inspect the script ([see script](mnist_classify.py)) by running these commands:\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"(torch-env) $ cd multi_gpu_training/01_single_gpu\n(torch-env) $ cat mnist_classify.py\n"})}),"\n",(0,r.jsxs)(n.p,{children:["We will profile the ",(0,r.jsx)(n.code,{children:"train"})," function using ",(0,r.jsx)(n.code,{children:"line_profiler"})," (see line 39) by adding the following decorator:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"@profile\ndef train(args, model, device, train_loader, optimizer, epoch):\n"})}),"\n",(0,r.jsx)(n.p,{children:"Next, download the data while on the login node since the compute nodes do not have internet access:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"(torch-env) $ python download_mnist.py\n"})}),"\n",(0,r.jsx)(n.p,{children:"Below is the Slurm script:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n#SBATCH --job-name=mnist         # create a short name for your job\n#SBATCH --nodes=1                # node count\n#SBATCH --ntasks=1               # total number of tasks across all nodes\n#SBATCH --cpus-per-task=1        # cpu-cores per task (>1 if multi-threaded tasks)\n#SBATCH --mem=8G                 # total memory per node (4 GB per cpu-core is default)\n#SBATCH --gres=gpu:1             # number of gpus per node\n#SBATCH --time=00:05:00          # total run time limit (HH:MM:SS)\n#SBATCH --mail-type=begin        # send email when job begins\n#SBATCH --mail-type=end          # send email when job ends\n\n# which gpu node was used\necho "Running on host" $(hostname)\n\n# print the slurm environment variables sorted by name\nprintenv | grep -i slurm | sort\n\nmodule purge\nmodule load anaconda3/2023.9\nconda activate /home/jdh4/.conda/envs/torch-env\n\nkernprof -o ${SLURM_JOBID}.lprof -l mnist_classify.py --epochs=3\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"kernprof"})," is a profiler that wraps Python. Adroit has two different A100 nodes. Learn how to choose ",(0,r.jsx)(n.a,{href:"https://researchcomputing.princeton.edu/systems/adroit#gpus",children:"specific nodes"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"Finally, submit the job while specifying the reservation:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"(torch-env) $ sbatch --reservation=multigpu job.slurm\n"})}),"\n",(0,r.jsx)(n.p,{children:"You should find that the code runs in about 20-40 seconds with 1 CPU-core depending on which A100 GPU node was used:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"$ seff 1937315\nJob ID: 1937315\nCluster: adroit\nUser/Group: aturing/cses\nState: COMPLETED (exit code 0)\nCores: 1\nCPU Utilized: 00:00:36\nCPU Efficiency: 94.74% of 00:00:38 core-walltime\nJob Wall-clock time: 00:00:38\nMemory Utilized: 593.32 MB\nMemory Efficiency: 7.24% of 8.00 GB\n"})}),"\n",(0,r.jsxs)(n.p,{children:["For jobs that run for longer than 1 minute, one should use the ",(0,r.jsx)(n.code,{children:"jobstats"})," command instead of ",(0,r.jsx)(n.code,{children:"seff"}),". Use ",(0,r.jsx)(n.code,{children:"shistory -n"})," to see which node was used or look in the ",(0,r.jsx)(n.code,{children:"slurm-#######.out"})," file."]}),"\n",(0,r.jsx)(n.p,{children:"Some variation in the run time is expected when multiple users are running on the same node. Also, the two A100 GPU nodes are not equal:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"hostname"}),(0,r.jsx)(n.th,{children:"CPU"}),(0,r.jsx)(n.th,{children:"GPU"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"adroit-h11g1"}),(0,r.jsx)(n.td,{children:"Intel Xeon Gold 6442Y @ 2.6GHz"}),(0,r.jsx)(n.td,{children:"NVIDIA A100 80GB PCIe"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"adroit-h11g2"}),(0,r.jsx)(n.td,{children:"Intel Xeon Gold 6342  @ 2.8GHz"}),(0,r.jsx)(n.td,{children:"NVIDIA A100-PCIE-40GB"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"step-3-analyze-the-profiling-data",children:"Step 3: Analyze the Profiling Data"}),"\n",(0,r.jsxs)(n.p,{children:["We installed ",(0,r.jsx)(n.a,{href:"https://researchcomputing.princeton.edu/python-profiling",children:"line_profiler"})," into the Conda environment and profiled the code. To analyze the profiling data:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"(torch-env) $ python -m line_profiler -rmt *.lprof \nTimer unit: 1e-06 s\n\nTotal time: 30.8937 s\nFile: mnist_classify.py\nFunction: train at line 39\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    39                                           @profile\n    40                                           def train(args, model, device, train_loader, optimizer, epoch):\n    41         3        213.1     71.0      0.0      model.train()\n    42      2817   26106124.7   9267.3     84.5      for batch_idx, (data, target) in enumerate(train_loader):\n    43      2814     286242.0    101.7      0.9          data, target = data.to(device), target.to(device)\n    44      2814     296440.2    105.3      1.0          optimizer.zero_grad()\n    45      2814    1189206.1    422.6      3.8          output = model(data)\n    46      2814      81578.6     29.0      0.3          loss = F.nll_loss(output, target)\n    47      2814    1979990.2    703.6      6.4          loss.backward()\n    48      2814     841861.9    299.2      2.7          optimizer.step()\n    49      2814       2095.3      0.7      0.0          if batch_idx % args.log_interval == 0:\n    50       564       1852.9      3.3      0.0              print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n    51       282       2218.6      7.9      0.0                  epoch, batch_idx * len(data), len(train_loader.dataset),\n    52       282     105753.3    375.0      0.3                  100. * batch_idx / len(train_loader), loss.item()))\n    53       282        119.2      0.4      0.0              if args.dry_run:\n    54                                                           break\n\n 30.89 seconds - mnist_classify.py:39 - train\n"})}),"\n",(0,r.jsxs)(n.p,{children:["The slowest line is number 42 which consumes 84.5% of the time in the training function. That line involves ",(0,r.jsx)(n.code,{children:"train_loader"})," which is the data loader for the training set. Are you surprised that the data loader is the slowest step and not the forward pass or calculation of the gradients? Can we improve on this?"]}),"\n",(0,r.jsx)(n.h3,{id:"examine-your-gpu-utilization",children:"Examine Your GPU Utilization"}),"\n",(0,r.jsxs)(n.p,{children:["Use tools like ",(0,r.jsx)(n.a,{href:"https://researchcomputing.princeton.edu/support/knowledge-base/job-stats#jobstats",children:"jobstats"}),", ",(0,r.jsx)(n.a,{href:"https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing#gpudash",children:"gpudash"})," and ",(0,r.jsx)(n.a,{href:"https://researchcomputing.princeton.edu/support/knowledge-base/job-stats#stats.rc",children:"stats.rc"})," to measure your GPU utilization. You can also do this on a ",(0,r.jsx)(n.a,{href:"https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing#gpu-utilization",children:"compute node in real time"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["Note that GPU utilization as measured using nvidia-smi is only a measure of the fraction of the time that a GPU kernel is running on the GPU. It says nothing about how many CUDA cores are being used or how efficiently the GPU kernels have been written. However, for codes used by large communities, one can generally associate GPU utilization with overall GPU efficiency. For a more accurate measure of GPU utilization, use ",(0,r.jsx)(n.a,{href:"https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing#profiling",children:"Nsight Systems or Nsight Compute"})," to measure the occupancy."]}),"\n",(0,r.jsx)(n.h2,{id:"step-4-work-through-the-performance-tuning-guide",children:"Step 4: Work through the Performance Tuning Guide"}),"\n",(0,r.jsxs)(n.p,{children:["Make sure you optimize the single GPU case before going to multiple GPUs by working through the ",(0,r.jsx)(n.a,{href:"https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html",children:"Performance Tuning Guide"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"step-5-optimize-your-script",children:"Step 5: Optimize Your Script"}),"\n",(0,r.jsxs)(n.p,{children:["One technique that was discussed in the ",(0,r.jsx)(n.a,{href:"https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html",children:"Performance Tuning Guide"})," was using multiple CPU-cores to speed-up ",(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Extract,_transform,_load",children:"ETL"}),". Let's put this into practice."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://www.telesens.co/wp-content/uploads/2019/04/img_5ca4eff975d80.png",alt:"multiple_workers"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.em,{children:["Credit for image above is ",(0,r.jsx)(n.a,{href:"https://www.telesens.co/2019/04/04/distributed-data-parallel-training-using-pytorch-on-aws/",children:"here"}),"."]})}),"\n",(0,r.jsxs)(n.p,{children:["In ",(0,r.jsx)(n.code,{children:"mnist_classify.py"}),", change ",(0,r.jsx)(n.code,{children:"num_workers"})," from 1 to 8. And then in ",(0,r.jsx)(n.code,{children:"job.slurm"})," change ",(0,r.jsx)(n.code,{children:"--cpus-per-task"})," from 1 to 8. Then run the script again and note the speed-up:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"(torch-env) $ sbatch --reservation=multigpu job.slurm\n"})}),"\n",(0,r.jsxs)(n.p,{children:["How did the profiling data change? Watch the ",(0,r.jsx)(n.a,{href:"https://www.youtube.com/watch?v=wqTgM-Wq4YY&t=296s",children:"video"})," for the solution. For consistency between the Slurm script and PyTorch script, one can use:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import os\n...\n    cuda_kwargs = {'num_workers': int(os.environ[\"SLURM_CPUS_PER_TASK\"]),\n...\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Several environment variables are set in the Slurm script. These can be referenced by the PyTorch script as demonstrated above. To see all of the available environment variables that are set in the Slurm script, add this line to ",(0,r.jsx)(n.code,{children:"job.slurm"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"printenv | sort\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Consider these external data loading libraries: ",(0,r.jsx)(n.a,{href:"https://github.com/libffcv/ffcv",children:"ffcv"})," and ",(0,r.jsx)(n.a,{href:"https://developer.nvidia.com/dali",children:"NVIDIA DALI"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"It is essential to optimize your code before going to multi-GPU training since the inefficiencies will only be magnified otherwise. The more GPUs you request in a Slurm job, the longer you will wait for the job to run. If you can get your work done using an optimized script running on a single GPU then proceed that way. Do not use multiple GPUs if your GPU efficiency is low. The average GPU efficiency on Della is around 50%."}),"\n",(0,r.jsxs)(n.p,{children:["Next, we focus on scaling the code to multiple GPUs (go to ",(0,r.jsx)(n.a,{href:"/pr-preview/pr-193/docs/hpc/ml_ai_hpc/pytorch_dpp",children:"next section"}),")."]}),"\n",(0,r.jsx)(n.h2,{id:"how-was-the-conda-environment-made",children:"How was the Conda environment made?"}),"\n",(0,r.jsxs)(n.p,{children:["Your ",(0,r.jsx)(n.code,{children:"/home"})," directory on Adroit probably has a capacity of 9.3 GB. To store Conda environments in another location see ",(0,r.jsx)(n.a,{href:"https://researchcomputing.princeton.edu/support/knowledge-base/checkquota",children:"this page"}),". See the Research Computing knowledge base on ",(0,r.jsx)(n.a,{href:"https://researchcomputing.princeton.edu/support/knowledge-base/pytorch",children:"PyTorch"})," for installation directions."]})]})}function d(e={}){let{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},55511:function(e,n,t){t.d(n,{R:()=>s,x:()=>a});var i=t(96363);let r={},o=i.createContext(r);function s(e){let n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);