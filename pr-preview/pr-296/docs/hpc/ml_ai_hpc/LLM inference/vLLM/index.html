<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-hpc/ml_ai_hpc/LLM inference/vLLM" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">High-performance LLM inference with vLLM | Connecting researchers to computational resources.</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://services.rt.nyu.edu/pr-preview/pr-296/img/NYU.svg"><meta data-rh="true" name="twitter:image" content="https://services.rt.nyu.edu/pr-preview/pr-296/img/NYU.svg"><meta data-rh="true" property="og:url" content="https://services.rt.nyu.edu/pr-preview/pr-296/docs/hpc/ml_ai_hpc/LLM inference/vLLM/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="High-performance LLM inference with vLLM | Connecting researchers to computational resources."><meta data-rh="true" name="description" content="What is vLLM?"><meta data-rh="true" property="og:description" content="What is vLLM?"><link data-rh="true" rel="icon" href="/pr-preview/pr-296/img/NYU.ico"><link data-rh="true" rel="canonical" href="https://services.rt.nyu.edu/pr-preview/pr-296/docs/hpc/ml_ai_hpc/LLM inference/vLLM/"><link data-rh="true" rel="alternate" href="https://services.rt.nyu.edu/pr-preview/pr-296/docs/hpc/ml_ai_hpc/LLM inference/vLLM/" hreflang="en"><link data-rh="true" rel="alternate" href="https://services.rt.nyu.edu/pr-preview/pr-296/docs/hpc/ml_ai_hpc/LLM inference/vLLM/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"High-performance LLM inference with vLLM","item":"https://services.rt.nyu.edu/pr-preview/pr-296/docs/hpc/ml_ai_hpc/LLM inference/vLLM"}]}</script><link rel="alternate" type="application/rss+xml" href="/pr-preview/pr-296/blog/rss.xml" title="Connecting researchers to computational resources. RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/pr-preview/pr-296/blog/atom.xml" title="Connecting researchers to computational resources. Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2DXB3BDH70"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-2DXB3BDH70",{anonymize_ip:!0})</script><link rel="stylesheet" href="/pr-preview/pr-296/assets/css/styles.f829a653.css">
<script src="/pr-preview/pr-296/assets/js/runtime~main.5f441af6.js" defer="defer"></script>
<script src="/pr-preview/pr-296/assets/js/main.95ac3745.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/pr-preview/pr-296/img/NYU.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_TUNJ" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/pr-preview/pr-296/"><div class="navbar__logo"><img src="/pr-preview/pr-296/img/NYU.svg" alt="NYU torch logo" class="themedComponent_adik themedComponent--light_G91j"><img src="/pr-preview/pr-296/img/NYU.svg" alt="NYU torch logo" class="themedComponent_adik themedComponent--dark_ZN7y"></div><b class="navbar__title text--truncate">Research Technology Services</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/pr-preview/pr-296/docs/genai/getting_started/intro/">Pythia</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/pr-preview/pr-296/docs/hpc/getting_started/intro/">HPC</a><a class="navbar__item navbar__link" href="/pr-preview/pr-296/docs/cloud/getting_started/intro/">Cloud</a><a class="navbar__item navbar__link" href="/pr-preview/pr-296/docs/srde/getting_started/intro/">SRDE</a><a href="https://hsrn.nyu.edu/docs/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">HSRN<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_NVDT"><use href="#theme-svg-external-link"></use></svg></a><a class="navbar__item navbar__link" href="/pr-preview/pr-296/blog/">Announcements</a><div class="toggle_jqKN colorModeToggle_gY5u"><button class="clean-btn toggleButton_lieL toggleButtonDisabled_sjb2" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_AmaU lightToggleIcon_PUFc"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_AmaU darkToggleIcon_woI4"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_AmaU systemToggleIcon_QZRn"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_gKxF"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_necp"><div class="docsWrapper_wAz_"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_QBOB" type="button"></button><div class="docRoot_gn_g"><aside class="theme-doc-sidebar-container docSidebarContainer_RCWh"><div class="sidebarViewport_BocM"><div class="sidebar_w3s6"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_EeVB"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-296/docs/hpc/getting_started/intro/"><span title="Getting Started" class="categoryLinkLabel_vKI6">Getting Started</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-296/docs/hpc/connecting_to_hpc/connecting_to_hpc/"><span title="Setup" class="categoryLinkLabel_vKI6">Setup</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-296/docs/hpc/storage/intro_and_data_management/"><span title="Storage &amp; Data transfers" class="categoryLinkLabel_vKI6">Storage &amp; Data transfers</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-296/docs/hpc/datasets/intro/"><span title="Datasets" class="categoryLinkLabel_vKI6">Datasets</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-296/docs/hpc/submitting_jobs/slurm_submitting_jobs/"><span title="Submitting jobs" class="categoryLinkLabel_vKI6">Submitting jobs</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-296/docs/hpc/tools_and_software/intro/"><span title="Tools &amp; Software" class="categoryLinkLabel_vKI6">Tools &amp; Software</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-296/docs/hpc/containers/intro/"><span title="Containers" class="categoryLinkLabel_vKI6">Containers</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/pr-preview/pr-296/docs/hpc/ml_ai_hpc/intro/"><span title="ML/AI on HPC" class="categoryLinkLabel_vKI6">ML/AI on HPC</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/pr-preview/pr-296/docs/hpc/ml_ai_hpc/intro/"><span title="Machine Learning (ML) and Artificial Intelligence (AI) on HPC" class="linkLabel_BOGb">Machine Learning (ML) and Artificial Intelligence (AI) on HPC</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/pr-preview/pr-296/docs/hpc/ml_ai_hpc/pytorch_intro/"><span title="Single-GPU Training with PyTorch" class="linkLabel_BOGb">Single-GPU Training with PyTorch</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/pr-preview/pr-296/docs/hpc/ml_ai_hpc/pytorch_dpp/"><span title="Multi-GPU Training with PyTorch: Distributed Data Parallel (DDP)" class="linkLabel_BOGb">Multi-GPU Training with PyTorch: Distributed Data Parallel (DDP)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/pr-preview/pr-296/docs/hpc/ml_ai_hpc/tensorflow/"><span title="TensorFlow" class="linkLabel_BOGb">TensorFlow</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/pr-preview/pr-296/docs/hpc/ml_ai_hpc/llm_fine_tuning/"><span title="Fine tune LLMs on HPC" class="linkLabel_BOGb">Fine tune LLMs on HPC</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/pr-preview/pr-296/docs/hpc/ml_ai_hpc/LLM inference/llm_inferenceoverview/"><span title="LLM inference" class="categoryLinkLabel_vKI6">LLM inference</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pr-preview/pr-296/docs/hpc/ml_ai_hpc/LLM inference/llm_inferenceoverview/"><span title="Overview" class="linkLabel_BOGb">Overview</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/pr-preview/pr-296/docs/hpc/ml_ai_hpc/LLM inference/run_hf_model/"><span title="Basic LLM Inference with Hugging Face transformers" class="linkLabel_BOGb">Basic LLM Inference with Hugging Face transformers</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/pr-preview/pr-296/docs/hpc/ml_ai_hpc/LLM inference/vLLM/"><span title="High-performance LLM inference with vLLM" class="linkLabel_BOGb">High-performance LLM inference with vLLM</span></a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-296/docs/hpc/ood/ood_intro/"><span title="Open OnDemand (OOD)" class="categoryLinkLabel_vKI6">Open OnDemand (OOD)</span></a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/pr-preview/pr-296/docs/hpc/spec_sheet/"><span title="Torch Spec Sheet" class="linkLabel_BOGb">Torch Spec Sheet</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/pr-preview/pr-296/docs/hpc/system_status/"><span title="Greene System Status" class="linkLabel_BOGb">Greene System Status</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-296/docs/hpc/tutorial_intro_shell_hpc/intro/"><span title="Tutorial: Intro to Shell for HPC" class="categoryLinkLabel_vKI6">Tutorial: Intro to Shell for HPC</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-296/docs/hpc/tutorial_intro_hpc/intro_hpc/"><span title="Tutorial: Intro to HPC" class="categoryLinkLabel_vKI6">Tutorial: Intro to HPC</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_HmkA menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/pr-preview/pr-296/docs/hpc/support/support/"><span title="Support" class="categoryLinkLabel_vKI6">Support</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_M8Os"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_dZwW"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_cg7U"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_l9oI"><div class="docItemContainer_ENRc"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_N0Nx" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/pr-preview/pr-296/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_CnUo"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">ML/AI on HPC</span></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">LLM inference</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">High-performance LLM inference with vLLM</span></li></ul></nav><div class="tocCollapsible_lcDd theme-doc-toc-mobile tocMobile_o_ql"><button type="button" class="clean-btn tocCollapsibleButton_A397">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>High-performance LLM inference with <code>vLLM</code></h1></header>
<h2 class="anchor anchorTargetStickyNavbar_wKjT" id="what-is-vllm">What is vLLM?<a href="#what-is-vllm" class="hash-link" aria-label="Direct link to What is vLLM?" title="Direct link to What is vLLM?" translate="no">â€‹</a></h2>
<p><a href="https://docs.vllm.ai/en/latest/" target="_blank" rel="noopener noreferrer" class=""><code>vLLM</code></a> is an easy-to-use library for LLM inference and serving which support a wide variety of models with optimized kernels ensuring optimal utilization of GPUs.</p>
<h2 class="anchor anchorTargetStickyNavbar_wKjT" id="why-vllm">Why <code>vLLM</code>?<a href="#why-vllm" class="hash-link" aria-label="Direct link to why-vllm" title="Direct link to why-vllm" translate="no">â€‹</a></h2>
<p>We tested <code>vLLM</code> and <code>llama-cpp</code> (the inference framework behind <code>ollama</code>) on Torch, and found <code>vLLM</code> performs better on Torch with the following model and token configuration:</p>
<p>Model: <code>Qwen2.5-7B-Instruct</code></p>
<p>Prompt Tokens: <code>512</code></p>
<p>Output Tokens: <code>256</code></p>
<table><thead><tr><th>Inference Server</th><th>Peak Throughput</th><th>Median Latency(ms)</th><th>Recommendation</th></tr></thead><tbody><tr><td><code>vLLM</code></td><td>~4689.6</td><td>~48.0</td><td>Best for Batch/Research</td></tr><tr><td><code>llama-cpp</code></td><td>~115.0</td><td>~280.0</td><td>Best for Single User</td></tr></tbody></table>
<h2 class="anchor anchorTargetStickyNavbar_wKjT" id="vllm-installation-instructions">vLLM Installation Instructions<a href="#vllm-installation-instructions" class="hash-link" aria-label="Direct link to vLLM Installation Instructions" title="Direct link to vLLM Installation Instructions" translate="no">â€‹</a></h2>
<p>Create a <code>vLLM</code> directory in your /scratch directory, then install the vLLM image:</p>
<div class="language-text codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-text codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">apptainer pull docker://vllm/vllm-openai:latest</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_wKjT" id="avoid-filling-up-your-home-directory">Avoid filling up your <code>$HOME</code> directory<a href="#avoid-filling-up-your-home-directory" class="hash-link" aria-label="Direct link to avoid-filling-up-your-home-directory" title="Direct link to avoid-filling-up-your-home-directory" translate="no">â€‹</a></h3>
<p>To avoid exceeding your <code>$HOME</code> quota (50GB) and inode limits (30,000 files), you should redirect <code>vLLM</code>&#x27;s cache and Hugging Face&#x27;s model downloads to your scratch space:</p>
<div class="language-sh codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-sh codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token builtin class-name" style="color:hsl(35, 99%, 36%)">export</span><span class="token plain"> </span><span class="token assign-left variable" style="color:hsl(221, 87%, 60%)">HF_HOME</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token plain">/scratch/</span><span class="token environment constant" style="color:hsl(35, 99%, 36%)">$USER</span><span class="token plain">/hf_cache</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token builtin class-name" style="color:hsl(35, 99%, 36%)">export</span><span class="token plain"> </span><span class="token assign-left variable" style="color:hsl(221, 87%, 60%)">VLLM_CACHE_ROOT</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token plain">/scratch/</span><span class="token environment constant" style="color:hsl(35, 99%, 36%)">$USER</span><span class="token plain">/vllm_cache</span><br></span></code></pre></div></div>
<p>You should run this to configure <code>vLLM</code> to always use your <code>$SCRATCH</code> storage for consistent use:</p>
<div class="language-sh codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-sh codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token builtin class-name" style="color:hsl(35, 99%, 36%)">echo</span><span class="token plain"> </span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;export HF_HOME=/scratch/\</span><span class="token string environment constant" style="color:hsl(35, 99%, 36%)">$USER</span><span class="token string" style="color:hsl(119, 34%, 47%)">/hf_cache&quot;</span><span class="token plain"> </span><span class="token operator" style="color:hsl(221, 87%, 60%)">&gt;&gt;</span><span class="token plain"> ~/.bashrc</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token builtin class-name" style="color:hsl(35, 99%, 36%)">echo</span><span class="token plain"> </span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;export VLLM_CACHE_ROOT=/scratch/\</span><span class="token string environment constant" style="color:hsl(35, 99%, 36%)">$USER</span><span class="token string" style="color:hsl(119, 34%, 47%)">/vllm_cache&quot;</span><span class="token plain"> </span><span class="token operator" style="color:hsl(221, 87%, 60%)">&gt;&gt;</span><span class="token plain"> ~/.bashrc</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-note admonition_nT0D alert alert--secondary"><div class="admonitionHeading_v1P0"><span class="admonitionIcon_Vpqq"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_kdNk"><p>Files on <code>$SCRATCH</code> are not backed up and will be deleted after 60 days of inactivity. Always keep your source code and .slurm scripts in <code>$HOME</code>!</p></div></div>
<h2 class="anchor anchorTargetStickyNavbar_wKjT" id="run-vllm">Run vLLM<a href="#run-vllm" class="hash-link" aria-label="Direct link to Run vLLM" title="Direct link to Run vLLM" translate="no">â€‹</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_wKjT" id="online-serving-openai-compatible-api">Online Serving (OpenAI-Compatible API)<a href="#online-serving-openai-compatible-api" class="hash-link" aria-label="Direct link to Online Serving (OpenAI-Compatible API)" title="Direct link to Online Serving (OpenAI-Compatible API)" translate="no">â€‹</a></h3>
<p><code>vLLM</code> implements the OpenAI API protocol, allowing it to be a drop-in replacement for applications using OpenAI&#x27;s services. By default, it starts the server at <code>http://localhost:8000</code>. You can specify the address with <code>--host</code> and <code>--port</code> arguments.
<strong>In Terminal 1:</strong>
Start  vLLM server (In this example we use Qwen model):</p>
<div class="language-sh codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-sh codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">apptainer </span><span class="token builtin class-name" style="color:hsl(35, 99%, 36%)">exec</span><span class="token plain"> </span><span class="token parameter variable" style="color:hsl(221, 87%, 60%)">--nv</span><span class="token plain"> vllm-openai_latest.sif vllm serve </span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;Qwen/Qwen2.5-0.5B-Instruct&quot;</span><br></span></code></pre></div></div>
<p>When you see:</p>
<div class="language-text codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-text codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">Application startup complete.</span><br></span></code></pre></div></div>
<p>Open another terminal and log in to the same computing node as in terminal 1.</p>
<p><strong>In Terminal 2</strong></p>
<div class="language-sh codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-sh codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token function" style="color:hsl(221, 87%, 60%)">curl</span><span class="token plain"> http://localhost:8000/v1/chat/completions </span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    </span><span class="token parameter variable" style="color:hsl(221, 87%, 60%)">-H</span><span class="token plain"> </span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;Content-Type: application/json&quot;</span><span class="token plain"> </span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">    </span><span class="token parameter variable" style="color:hsl(221, 87%, 60%)">-d</span><span class="token plain"> </span><span class="token string" style="color:hsl(119, 34%, 47%)">&#x27;{</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token string" style="color:hsl(119, 34%, 47%)">        &quot;model&quot;: &quot;Qwen/Qwen2.5-0.5B-Instruct&quot;,</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token string" style="color:hsl(119, 34%, 47%)">        &quot;messages&quot;: [</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token string" style="color:hsl(119, 34%, 47%)">            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Your prompt...&quot;}</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token string" style="color:hsl(119, 34%, 47%)">        ]</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token string" style="color:hsl(119, 34%, 47%)">    }&#x27;</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_wKjT" id="offline-inference">Offline Inference<a href="#offline-inference" class="hash-link" aria-label="Direct link to Offline Inference" title="Direct link to Offline Inference" translate="no">â€‹</a></h3>
<p>If you need to process a large dataset at once without setting up a server, you can use <code>vLLM</code>&#x27;s LLM class.
For example, the following code downloads the <code>facebook/opt-125m</code> model from HuggingFace and runs it in <code>vLLM</code> using the default configuration.</p>
<div class="language-python codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-python codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token keyword" style="color:hsl(301, 63%, 40%)">from</span><span class="token plain"> vllm </span><span class="token keyword" style="color:hsl(301, 63%, 40%)">import</span><span class="token plain"> LLM</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)"># Initialize the vLLM engine.</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">llm </span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token plain"> LLM</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">(</span><span class="token plain">model</span><span class="token operator" style="color:hsl(221, 87%, 60%)">=</span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;facebook/opt-125m&quot;</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">)</span><br></span></code></pre></div></div>
<p>After initializing the LLM instance, use the available APIs to perform model inference.</p>
<h3 class="anchor anchorTargetStickyNavbar_wKjT" id="sglang-a-simple-option-for-offline-batch-inference">SGLang: A Simple Option for Offline Batch Inference<a href="#sglang-a-simple-option-for-offline-batch-inference" class="hash-link" aria-label="Direct link to SGLang: A Simple Option for Offline Batch Inference" title="Direct link to SGLang: A Simple Option for Offline Batch Inference" translate="no">â€‹</a></h3>
<p>For cases where users only want to run batch inference and do not need an HTTP endpoint, SGLang provides a much simpler offline engine API compared to running a full vLLM server. It is particularly suitable for dataset processing, evaluation pipelines, and one-off large-scale inference jobs.
For more details and examples, see the official SGLang offline engine documentation here: <a href="https://docs.sglang.io/basic_usage/offline_engine_api.html" target="_blank" rel="noopener noreferrer" class="">https://docs.sglang.io/basic_usage/offline_engine_api.html</a></p>
<h2 class="anchor anchorTargetStickyNavbar_wKjT" id="vllm-cli"><code>vLLM</code> CLI<a href="#vllm-cli" class="hash-link" aria-label="Direct link to vllm-cli" title="Direct link to vllm-cli" translate="no">â€‹</a></h2>
<p>The <code>vllm</code> command-line tool is used to run and manage <code>vLLM</code> models. You can start by viewing the help message with:</p>
<div class="language-sh codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-sh codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">vllm </span><span class="token parameter variable" style="color:hsl(221, 87%, 60%)">--help</span><br></span></code></pre></div></div>
<p>Serve - Starts the vLLM OpenAI Compatible API server.</p>
<div class="language-sh codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-sh codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">vllm serve meta-llama/Llama-2-7b-hf</span><br></span></code></pre></div></div>
<p>Chat - Generate chat completions via the running API server.</p>
<div class="language-sh codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-sh codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token comment" style="color:hsl(230, 4%, 64%)"># Directly connect to localhost API without arguments</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">vllm chat</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)"># Specify API url</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">vllm chat </span><span class="token parameter variable" style="color:hsl(221, 87%, 60%)">--url</span><span class="token plain"> http://</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">{</span><span class="token plain">vllm-serve-host</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">}</span><span class="token plain">:</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">{</span><span class="token plain">vllm-serve-port</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">}</span><span class="token plain">/v1</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)"># Quick chat with a single prompt</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">vllm chat </span><span class="token parameter variable" style="color:hsl(221, 87%, 60%)">--quick</span><span class="token plain"> </span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;hi&quot;</span><br></span></code></pre></div></div>
<p>Complete - Generate text completions based on the given prompt via the running API server.</p>
<div class="language-sh codeBlockContainer_ndbq theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_Zo7j"><pre tabindex="0" class="prism-code language-sh codeBlock_hgHP thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_EpfB"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token comment" style="color:hsl(230, 4%, 64%)"># Directly connect to localhost API without arguments</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">vllm complete</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)"># Specify API url</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">vllm complete </span><span class="token parameter variable" style="color:hsl(221, 87%, 60%)">--url</span><span class="token plain"> http://</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">{</span><span class="token plain">vllm-serve-host</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">}</span><span class="token plain">:</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">{</span><span class="token plain">vllm-serve-port</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">}</span><span class="token plain">/v1</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token comment" style="color:hsl(230, 4%, 64%)"># Quick complete with a single prompt</span><span class="token plain"></span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain">vllm complete </span><span class="token parameter variable" style="color:hsl(221, 87%, 60%)">--quick</span><span class="token plain"> </span><span class="token string" style="color:hsl(119, 34%, 47%)">&quot;The future of AI is&quot;</span><br></span></code></pre></div></div>
<p>For more CLI command references: visit <a href="https://docs.vllm.ai/en/stable/cli/" target="_blank" rel="noopener noreferrer" class="">https://docs.vllm.ai/en/stable/cli/</a>.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_AAw7"><a href="https://github.com/NYU-RTS/rts-docs/blob/main/docs/hpc/08_ml_ai_hpc/08_LLM inference/03_vLLM.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_OZ0G" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_RgrK"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/pr-preview/pr-296/docs/hpc/ml_ai_hpc/LLM inference/run_hf_model/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Basic LLM Inference with Hugging Face transformers</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/pr-preview/pr-296/docs/hpc/ood/ood_intro/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Introduction to Open OnDemand (OOD)</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_Py9d thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#what-is-vllm" class="table-of-contents__link toc-highlight">What is vLLM?</a></li><li><a href="#why-vllm" class="table-of-contents__link toc-highlight">Why <code>vLLM</code>?</a></li><li><a href="#vllm-installation-instructions" class="table-of-contents__link toc-highlight">vLLM Installation Instructions</a><ul><li><a href="#avoid-filling-up-your-home-directory" class="table-of-contents__link toc-highlight">Avoid filling up your <code>$HOME</code> directory</a></li></ul></li><li><a href="#run-vllm" class="table-of-contents__link toc-highlight">Run vLLM</a><ul><li><a href="#online-serving-openai-compatible-api" class="table-of-contents__link toc-highlight">Online Serving (OpenAI-Compatible API)</a></li><li><a href="#offline-inference" class="table-of-contents__link toc-highlight">Offline Inference</a></li><li><a href="#sglang-a-simple-option-for-offline-batch-inference" class="table-of-contents__link toc-highlight">SGLang: A Simple Option for Offline Batch Inference</a></li></ul></li><li><a href="#vllm-cli" class="table-of-contents__link toc-highlight"><code>vLLM</code> CLI</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Contact</div><ul class="footer__items clean-list"><li class="footer__item"><a href="mailto:hpc@nyu.edu" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email HPC support<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_NVDT"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="mailto:hsrn-support@nyu.edu" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email HSRN support<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_NVDT"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="mailto:research-cloud-support@nyu.edu" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email Research Cloud support<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_NVDT"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="mailto:srde-support@nyu.edu" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email Secure Research Data Environment support<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_NVDT"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="mailto:genai-support@nyu.edu" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email general GenAI support<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_NVDT"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="mailto:genai-research-support@nyu.edu" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email GenAI for research support<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_NVDT"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Links</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://guides.nyu.edu/" target="_blank" rel="noopener noreferrer" class="footer__link-item">NYU Libraries Research Guides<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_NVDT"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://sites.google.com/nyu.edu/forc-camp/home" target="_blank" rel="noopener noreferrer" class="footer__link-item">FORC camp<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_NVDT"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://docs.google.com/forms/d/e/1FAIpQLSeHnmkPdR_IvWnT6a7U_V3RpfmQrpS8hjxI11FNnsZMlrBa4g/viewform" target="_blank" rel="noopener noreferrer" class="footer__link-item">Feedback</a></li><li class="footer__item"><a class="footer__link-item" href="/pr-preview/pr-296/blog/">Announcements</a></li><li class="footer__item"><a href="https://github.com/NYU-RTS/rts-docs" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_NVDT"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Made with ðŸ’œ in NYC!</div></div></div></footer></div>
</body>
</html>