"use strict";(self.webpackChunkrts_docs=self.webpackChunkrts_docs||[]).push([["6197"],{89582(e,n,i){i.r(n),i.d(n,{metadata:()=>l,default:()=>h,frontMatter:()=>t,contentTitle:()=>c,toc:()=>a,assets:()=>o});var l=JSON.parse('{"id":"hpc/ml_ai_hpc/LLM inference/vLLM","title":"High-performance LLM inference with vLLM","description":"What is vLLM?","source":"@site/docs/hpc/08_ml_ai_hpc/08_LLM inference/03_vLLM.md","sourceDirName":"hpc/08_ml_ai_hpc/08_LLM inference","slug":"/hpc/ml_ai_hpc/LLM inference/vLLM","permalink":"/pr-preview/pr-296/docs/hpc/ml_ai_hpc/LLM inference/vLLM","draft":false,"unlisted":false,"editUrl":"https://github.com/NYU-RTS/rts-docs/blob/main/docs/hpc/08_ml_ai_hpc/08_LLM inference/03_vLLM.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{},"sidebar":"hpcSidebar","previous":{"title":"Basic LLM Inference with Hugging Face transformers","permalink":"/pr-preview/pr-296/docs/hpc/ml_ai_hpc/LLM inference/run_hf_model"},"next":{"title":"Introduction to Open OnDemand (OOD)","permalink":"/pr-preview/pr-296/docs/hpc/ood/ood_intro"}}'),r=i(62615),s=i(30416);let t={},c="High-performance LLM inference with vLLM",o={},a=[{value:"What is vLLM?",id:"what-is-vllm",level:2},{value:"Why <code>vLLM</code>?",id:"why-vllm",level:2},{value:"vLLM Installation Instructions",id:"vllm-installation-instructions",level:2},{value:"Avoid filling up your <code>$HOME</code> directory",id:"avoid-filling-up-your-home-directory",level:3},{value:"Run vLLM",id:"run-vllm",level:2},{value:"Online Serving (OpenAI-Compatible API)",id:"online-serving-openai-compatible-api",level:3},{value:"Offline Inference",id:"offline-inference",level:3},{value:"SGLang: A Simple Option for Offline Batch Inference",id:"sglang-a-simple-option-for-offline-batch-inference",level:3},{value:"<code>vLLM</code> CLI",id:"vllm-cli",level:2}];function d(e){let n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsxs)(n.h1,{id:"high-performance-llm-inference-with-vllm",children:["High-performance LLM inference with ",(0,r.jsx)(n.code,{children:"vLLM"})]})}),"\n",(0,r.jsx)(n.h2,{id:"what-is-vllm",children:"What is vLLM?"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/",children:(0,r.jsx)(n.code,{children:"vLLM"})})," is an easy-to-use library for LLM inference and serving which support a wide variety of models with optimized kernels ensuring optimal utilization of GPUs."]}),"\n",(0,r.jsxs)(n.h2,{id:"why-vllm",children:["Why ",(0,r.jsx)(n.code,{children:"vLLM"}),"?"]}),"\n",(0,r.jsxs)(n.p,{children:["We tested ",(0,r.jsx)(n.code,{children:"vLLM"})," and ",(0,r.jsx)(n.code,{children:"llama-cpp"})," (the inference framework behind ",(0,r.jsx)(n.code,{children:"ollama"}),") on Torch, and found ",(0,r.jsx)(n.code,{children:"vLLM"})," performs better on Torch for ",(0,r.jsx)(n.code,{children:"Qwen2.5-7B-Instruct"})," with ",(0,r.jsx)(n.code,{children:"512"})," input and ",(0,r.jsx)(n.code,{children:"256"})," output tokens."]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Inference Server"}),(0,r.jsx)(n.th,{children:"Peak Throughput"}),(0,r.jsx)(n.th,{children:"Median Latency(ms)"}),(0,r.jsx)(n.th,{children:"Recommendation"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"vLLM"})}),(0,r.jsx)(n.td,{children:"~4689.6"}),(0,r.jsx)(n.td,{children:"~48.0"}),(0,r.jsx)(n.td,{children:"Best for Batch/Research"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"llama-cpp"})}),(0,r.jsx)(n.td,{children:"~115.0"}),(0,r.jsx)(n.td,{children:"~280.0"}),(0,r.jsx)(n.td,{children:"Best for Single User"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"vllm-installation-instructions",children:"vLLM Installation Instructions"}),"\n",(0,r.jsxs)(n.p,{children:["Create a ",(0,r.jsx)(n.code,{children:"vLLM"})," directory in your /scratch directory, then install the vLLM image:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"apptainer pull docker://vllm/vllm-openai:latest\n"})}),"\n",(0,r.jsxs)(n.h3,{id:"avoid-filling-up-your-home-directory",children:["Avoid filling up your ",(0,r.jsx)(n.code,{children:"$HOME"})," directory"]}),"\n",(0,r.jsxs)(n.p,{children:["To avoid exceeding your ",(0,r.jsx)(n.code,{children:"$HOME"})," quota (50GB) and inode limits (30,000 files), you should redirect ",(0,r.jsx)(n.code,{children:"vLLM"}),"'s cache and Hugging Face's model downloads to your scratch space:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:"export HF_HOME=/scratch/$USER/hf_cache\nexport VLLM_CACHE_ROOT=/scratch/$USER/vllm_cache\n"})}),"\n",(0,r.jsxs)(n.p,{children:["You should run this to configure ",(0,r.jsx)(n.code,{children:"vLLM"})," to always use your ",(0,r.jsx)(n.code,{children:"$SCRATCH"})," storage for consistent use:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:'echo "export HF_HOME=/scratch/\\$USER/hf_cache" >> ~/.bashrc\necho "export VLLM_CACHE_ROOT=/scratch/\\$USER/vllm_cache" >> ~/.bashrc\n'})}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["Files on ",(0,r.jsx)(n.code,{children:"$SCRATCH"})," are not backed up and will be deleted after 60 days of inactivity. Always keep your source code and .slurm scripts in ",(0,r.jsx)(n.code,{children:"$HOME"}),"!"]})}),"\n",(0,r.jsx)(n.h2,{id:"run-vllm",children:"Run vLLM"}),"\n",(0,r.jsx)(n.h3,{id:"online-serving-openai-compatible-api",children:"Online Serving (OpenAI-Compatible API)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"vLLM"})," implements the OpenAI API protocol, allowing it to be a drop-in replacement for applications using OpenAI's services. By default, it starts the server at ",(0,r.jsx)(n.code,{children:"http://localhost:8000"}),". You can specify the address with ",(0,r.jsx)(n.code,{children:"--host"})," and ",(0,r.jsx)(n.code,{children:"--port"})," arguments.\n",(0,r.jsx)(n.strong,{children:"In Terminal 1:"}),"\nStart  vLLM server (In this example we use Qwen model):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:'apptainer exec --nv vllm-openai_latest.sif vllm serve "Qwen/Qwen2.5-0.5B-Instruct"\n'})}),"\n",(0,r.jsx)(n.p,{children:"When you see:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Application startup complete.\n"})}),"\n",(0,r.jsx)(n.p,{children:"Open another terminal and log in to the same computing node as in terminal 1."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"In Terminal 2"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:'curl http://localhost:8000/v1/chat/completions \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n        "model": "Qwen/Qwen2.5-0.5B-Instruct",\n        "messages": [\n            {"role": "user", "content": "Your prompt..."}\n        ]\n    }\'\n'})}),"\n",(0,r.jsx)(n.h3,{id:"offline-inference",children:"Offline Inference"}),"\n",(0,r.jsxs)(n.p,{children:["If you need to process a large dataset at once without setting up a server, you can use ",(0,r.jsx)(n.code,{children:"vLLM"}),"'s LLM class.\nFor example, the following code downloads the ",(0,r.jsx)(n.code,{children:"facebook/opt-125m"})," model from HuggingFace and runs it in ",(0,r.jsx)(n.code,{children:"vLLM"})," using the default configuration."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from vllm import LLM\n\n# Initialize the vLLM engine.\nllm = LLM(model="facebook/opt-125m")\n'})}),"\n",(0,r.jsx)(n.p,{children:"After initializing the LLM instance, use the available APIs to perform model inference."}),"\n",(0,r.jsx)(n.h3,{id:"sglang-a-simple-option-for-offline-batch-inference",children:"SGLang: A Simple Option for Offline Batch Inference"}),"\n",(0,r.jsxs)(n.p,{children:["For cases where users only want to run batch inference and do not need an HTTP endpoint, SGLang provides a much simpler offline engine API compared to running a full vLLM server. It is particularly suitable for dataset processing, evaluation pipelines, and one-off large-scale inference jobs.\nFor more details and examples, see the official SGLang offline engine documentation here: ",(0,r.jsx)(n.a,{href:"https://docs.sglang.io/basic_usage/offline_engine_api.html",children:"https://docs.sglang.io/basic_usage/offline_engine_api.html"})]}),"\n",(0,r.jsxs)(n.h2,{id:"vllm-cli",children:[(0,r.jsx)(n.code,{children:"vLLM"})," CLI"]}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"vllm"})," command-line tool is used to run and manage ",(0,r.jsx)(n.code,{children:"vLLM"})," models. You can start by viewing the help message with:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:"vllm --help\n"})}),"\n",(0,r.jsx)(n.p,{children:"Serve - Starts the vLLM OpenAI Compatible API server."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:"vllm serve meta-llama/Llama-2-7b-hf\n"})}),"\n",(0,r.jsx)(n.p,{children:"Chat - Generate chat completions via the running API server."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:'# Directly connect to localhost API without arguments\nvllm chat\n\n# Specify API url\nvllm chat --url http://{vllm-serve-host}:{vllm-serve-port}/v1\n\n# Quick chat with a single prompt\nvllm chat --quick "hi"\n'})}),"\n",(0,r.jsx)(n.p,{children:"Complete - Generate text completions based on the given prompt via the running API server."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:'# Directly connect to localhost API without arguments\nvllm complete\n\n# Specify API url\nvllm complete --url http://{vllm-serve-host}:{vllm-serve-port}/v1\n\n# Quick complete with a single prompt\nvllm complete --quick "The future of AI is"\n'})}),"\n",(0,r.jsxs)(n.p,{children:["For more CLI command references: visit ",(0,r.jsx)(n.a,{href:"https://docs.vllm.ai/en/stable/cli/",children:"https://docs.vllm.ai/en/stable/cli/"}),"."]})]})}function h(e={}){let{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},30416(e,n,i){i.d(n,{R:()=>t,x:()=>c});var l=i(59471);let r={},s=l.createContext(r);function t(e){let n=l.useContext(s);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),l.createElement(s.Provider,{value:n},e.children)}}}]);