"use strict";(self.webpackChunkrts_docs=self.webpackChunkrts_docs||[]).push([["4474"],{93923(e,t,n){n.r(t),n.d(t,{metadata:()=>i,default:()=>h,frontMatter:()=>s,contentTitle:()=>a,toc:()=>l,assets:()=>c});var i=JSON.parse('{"id":"genai/how_to_guides/batch_inference","title":"Batch Inference","description":"When immediate results are not needed, for instance in transforming large datasets of unstructured data with LLMs, batch inference adds convenience while offering lower costs. Typical completion window are 24/48h as LLM inference providers run your workload when the load on the inference server is low. If you are interested in harnessing this feature, reach out to us at genai-research-support@nyu.edu and we will set up a cloud storage bucket for you.","source":"@site/docs/genai/04_how_to_guides/04_batch_inference.md","sourceDirName":"genai/04_how_to_guides","slug":"/genai/how_to_guides/batch_inference","permalink":"/pr-preview/pr-300/docs/genai/how_to_guides/batch_inference","draft":false,"unlisted":false,"editUrl":"https://github.com/NYU-RTS/rts-docs/blob/main/docs/genai/04_how_to_guides/04_batch_inference.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{},"sidebar":"genaiSidebar","previous":{"title":"Retrieval-augmented generation","permalink":"/pr-preview/pr-300/docs/genai/how_to_guides/retrieval_augmented_generation"},"next":{"title":"Fine tuning","permalink":"/pr-preview/pr-300/docs/genai/how_to_guides/llm_fine_tuning"}}'),o=n(62615),r=n(30416);let s={},a="Batch Inference",c={},l=[{value:"Collect the prompts",id:"collect-the-prompts",level:2},{value:"Upload them to the GCS bucket",id:"upload-them-to-the-gcs-bucket",level:2},{value:"Submit a batch inference job",id:"submit-a-batch-inference-job",level:2},{value:"Query job status",id:"query-job-status",level:2},{value:"Retrieving the output",id:"retrieving-the-output",level:2}];function d(e){let t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"batch-inference",children:"Batch Inference"})}),"\n",(0,o.jsxs)(t.p,{children:["When immediate results are not needed, for instance in transforming large datasets of unstructured data with LLMs, batch inference adds convenience while offering lower costs. Typical completion window are 24/48h as LLM inference providers run your workload when the load on the inference server is low. If you are interested in harnessing this feature, reach out to us at ",(0,o.jsx)(t.a,{href:"mailto:genai-research-support@nyu.edu",children:(0,o.jsx)(t.code,{children:"genai-research-support@nyu.edu"})})," and we will set up a cloud storage bucket for you."]}),"\n",(0,o.jsx)(t.admonition,{type:"info",children:(0,o.jsxs)(t.p,{children:["Batch processing is only supported for LLMs that can be accessed via the ",(0,o.jsx)(t.code,{children:"@vertexai"})," provider."]})}),"\n",(0,o.jsx)(t.h2,{id:"collect-the-prompts",children:"Collect the prompts"}),"\n",(0,o.jsxs)(t.p,{children:["You'll collect the prompts you want to send to the LLM as a newline delimited JSON (",(0,o.jsx)(t.a,{href:"https://jsonlines.org/",children:"JSONLines"}),") where each line contains a single prompt in the OpenAI format. Here's an example we will be using in this example:"]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-json",children:'{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gemini-3-flash-preview", "messages": [{"role": "user", "content": "Where is NYU located?"}], "max_tokens": 2048}}\n{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gemini-3-flash-preview", "messages": [{"role": "user", "content": "What resources are available for genAI research at nyu?"}], "max_tokens": 8196}}\n'})}),"\n",(0,o.jsx)(t.h2,{id:"upload-them-to-the-gcs-bucket",children:"Upload them to the GCS bucket"}),"\n",(0,o.jsxs)(t.p,{children:["We will upload this via the Portkey client to the GCS (",(0,o.jsx)(t.a,{href:"https://cloud.google.com/storage",children:"Google Cloud Storage"}),") bucket via the following script:"]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'from portkey_ai import Portkey\n\n# Initialize the Portkey client\nportkey = Portkey(\n    api_key="",  # Replace with your Portkey API key\n    provider="@vertexai",\n    base_url="https://ai-gateway.apps.cloud.rt.nyu.edu/v1/",\n    vertex_storage_bucket_name="",  # Specify the GCS bucket name\n    provider_file_name="test_dataset.jsonl",  # Specify the file name in GCS\n    provider_model="gemini-3-flash-preview",  # Specify the model to use\n)\n\n# Upload a file for batch inference\nfile = portkey.files.create(file=open("test_dataset.jsonl", mode="rb"), purpose="batch")\n\nprint(file)\n'})}),"\n",(0,o.jsx)(t.p,{children:"This script will print to standard output the location of the uploaded file, like:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-sh",children:'{\n    "id": "...",\n    "bytes": 739,\n    "created_at": null,\n    "filename": "test_dataset.jsonl",\n    "object": "file",\n    "purpose": "batch",\n    "status": "processed",\n    "status_details": null,\n    "create_at": 1771429900875\n}\n\n'})}),"\n",(0,o.jsx)(t.h2,{id:"submit-a-batch-inference-job",children:"Submit a batch inference job"}),"\n",(0,o.jsx)(t.p,{children:"We are now ready to submit the batch inference job. Here's a script to do so:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'from portkey_ai import Portkey\n\n# Initialize the Portkey client\nportkey = Portkey(\n    api_key="",  # Replace with your Portkey API key\n    provider="@vertexai",\n    base_url="https://ai-gateway.apps.cloud.rt.nyu.edu/v1/",\n    vertex_storage_bucket_name="",  # Specify the GCS bucket name\n    provider_model="gemini-3-flash-preview",  # Specify the model to use\n)\n\n# Create a batch inference job\nbatch_job = portkey.batches.create(\n    input_file_id="", # Copy this from the output of the last script\n    endpoint="/v1/chat/completions",  # API endpoint to use\n    completion_window="24h",  # Time window for completion\n    model="gemini-3-flash-preview",\n)\n\nprint(batch_job)\n'})}),"\n",(0,o.jsxs)(t.p,{children:["Upon successful submission, you'll see an ",(0,o.jsx)(t.code,{children:"id"})," field that refers to the job id."]}),"\n",(0,o.jsx)(t.h2,{id:"query-job-status",children:"Query job status"}),"\n",(0,o.jsx)(t.p,{children:"Using the id of the batch inference job, you can query the status by:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-sh",children:'curl -H "x-portkey-api-key: "  -H "x-portkey-provider: @vertexai"  https://ai-gateway.apps.cloud.rt.nyu.edu/v1/batches/your-batch-inference-job-id\n'})}),"\n",(0,o.jsx)(t.p,{children:"The output for a pending job looks like:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-json",children:'{\n  "id": "7284652661620604928",\n  "object": "batch",\n  "endpoint": "/v1/chat/completions",\n  "input_file_id": "...",\n  "completion_window": null,\n  "status": "in_progress",\n  "output_file_id": "...",\n  "error_file_id": "...",\n  "created_at": 1771430154933,\n  "in_progress_at": 1771430241905,\n  "request_counts": {\n    "total": 0,\n    "completed": null,\n    "failed": null\n  },\n  "model": "publishers/google/models/gemini-3-flash-preview"\n}\n'})}),"\n",(0,o.jsxs)(t.p,{children:["Once the job completes, the ",(0,o.jsx)(t.code,{children:"status"})," field will change from ",(0,o.jsx)(t.code,{children:"in_progress"})," to ",(0,o.jsx)(t.code,{children:"completed"}),"."]}),"\n",(0,o.jsx)(t.h2,{id:"retrieving-the-output",children:"Retrieving the output"}),"\n",(0,o.jsx)(t.p,{children:"The output of the batch inference job can be obtained by:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-sh",children:' curl -H "x-portkey-api-key: "  -H "x-portkey-provider: @vertexai"  https://ai-gateway.apps.cloud.rt.nyu.edu/v1/batches/7284652661620604928/output\n'})}),"\n",(0,o.jsx)(t.p,{children:"The output is also a newline delimited json (JSONLines) file that looks like:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-json",children:'  {\n  "id": "batch-7284652661620604928-aOKVaZmJGItT7OsPu-WByQc",\n  "custom_id": "request-1",\n  "response": {\n    "status_code": 200,\n    "request_id": "batch-7284652661620604928-aOKVaZmJGItT7OsPu-WByQc",\n    "body": {\n      "id": "chatcmpl-66109dHQ7PrBtM5Bzi37wXu97lr8m",\n      "object": "chat.completion",\n      "created": 1771430797,\n      "model": "gemini-3-flash-preview",\n      "provider": "vertex-ai",\n      "choices": [\n        {\n          "message": {\n            "role": "assistant",\n            "content": "New York University (NYU) is primarily located in **New York City**\n            ...\n'})})]})}function h(e={}){let{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},30416(e,t,n){n.d(t,{R:()=>s,x:()=>a});var i=n(59471);let o={},r=i.createContext(o);function s(e){let t=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(r.Provider,{value:t},e.children)}}}]);