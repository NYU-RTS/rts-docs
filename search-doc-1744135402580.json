{"searchDocs":[{"title":"MDX Blog Post","type":0,"sectionRef":"#","url":"/rts-docs-dev/blog/mdx-blog-post/","content":"Blog posts support Docusaurus Markdown features, such as MDX. tip Use the power of React to create interactive blog posts. For example, use JSX to create an interactive button: &lt;button onClick={() =&gt; alert('button clicked!')}&gt;Click me!&lt;/button&gt; Click me!","keywords":"","version":null},{"title":"LLM Catalogue","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/genai/external_llms/catalogue/","content":"","keywords":"","version":"Next"},{"title":"OpenAI​","type":1,"pageTitle":"LLM Catalogue","url":"/rts-docs-dev/docs/genai/external_llms/catalogue/#openai","content":" gpt-4o-minio3-minitext-embedding-3-small  ","version":"Next","tagName":"h2"},{"title":"VertexAI​","type":1,"pageTitle":"LLM Catalogue","url":"/rts-docs-dev/docs/genai/external_llms/catalogue/#vertexai","content":" Gemini-2.0 models (flash, flash-lite)Gemini-1.5 models (flash, pro) For a comprehensive list, please refer to the VertexAI documentation. ","version":"Next","tagName":"h2"},{"title":"OpenWebUI","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/genai/external_llms/playground/","content":"OpenWebUI We are working on providing an NYU hosted instance of OpenWebUI. More details about this will be provided soon.","keywords":"","version":"Next"},{"title":"Vector Databases","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/genai/vector_databases/intro/","content":"Vector Databases What is it? How is it different from a regular database?","keywords":"","version":"Next"},{"title":"Pythia","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/genai/getting_started/intro/","content":"Pythia Non-research workflows If you're looking to harness Generative AI for administrative or classroom use, please reach out to genai-support@nyu.edu Welcome to Pythia, the generative AI platform for research workflows. As part of the Pythia platform, the following capabilities are offered: Access to externally hosted LLMsHPC resources for fine tuning LLMsMilvus vector database Personal use If you want to access NYU provided LLMs for personal use, proceed to https://gemini.google.com/app with your NYU credentials.","keywords":"","version":"Next"},{"title":"Portkey","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/genai/external_llms/llm_access/","content":"","keywords":"","version":"Next"},{"title":"Onboarding​","type":1,"pageTitle":"Portkey","url":"/rts-docs-dev/docs/genai/external_llms/llm_access/#onboarding","content":" Send an email to genai-support@nyu.edu to start the onboarding process.  ","version":"Next","tagName":"h2"},{"title":"Getting started with Portkey​","type":1,"pageTitle":"Portkey","url":"/rts-docs-dev/docs/genai/external_llms/llm_access/#getting-started-with-portkey","content":" As part of the onboarding process, you would have received an invite which gives you access to a workspace. We will also add virtual keys for LLMs to your workspace as part of the onboarding process. Once you've accepted it, head over to https://app.portkey.ai/ and select the sign-in with Single Sign-On option and proceed with your NYU email address.  Access to Portkey is only permitted via NYU VPN You need to be connected to the NYU VPN to access the Portkey LLM gateway. If you are not, your requests will timeout and result in connection errors.  You will now be able to create an API key for yourself by access the API Keys item on the left sidebar. With an API key and a virtual key at your disposal, you can now run the following script:  from portkey_ai import Portkey portkey = Portkey( base_url=&quot;https://ai-gateway.apps.cloud.rt.nyu.edu/v1/&quot;, api_key=&quot;&quot;, # Replace with your Portkey API key virtual_key=&quot;&quot;, # Replace with your virtual key ) completion = portkey.chat.completions.create( messages=[ {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are not a helpful assistant&quot;}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Say this is a test&quot;}, ], model=&quot;gemini-2.0-flash-lite&quot;, ) print(completion)   Once the script is executed, you can head back to app.portkey.ai to view the logs for the call! ","version":"Next","tagName":"h2"},{"title":"Fine tuning","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/genai/llm_fine_tuning/intro/","content":"Fine tuning You can use HPC for this. But for most cases, RAG might suffice. Please look into harnessing RAG before attempting to fine-tune a model.","keywords":"","version":"Next"},{"title":"Connecting to the HPC Cluster","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/connecting_to_hpc/","content":"","keywords":"","version":"Next"},{"title":"Remote Connections with the NYU VPN & HPC Gateway Server​","type":1,"pageTitle":"Connecting to the HPC Cluster","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/connecting_to_hpc/#remote-connections-with-the-nyu-vpn--hpc-gateway-server","content":" If you are connecting from a remote location that is not on the NYU network (your home for example), you have two options:  VPN Option: Set up your computer to use the NYU VPN. Once you've created a VPN connection, you can proceed as if you were connected to the NYU net Gateway Option: Go through our gateway servers (example below). Gateways are designed to support only a very minimal set of commands and their only purpose is to let users connect HPC systems without needing to first connect to the VPN  You do not need to use the NYU VPN or gateways if you are connected to the NYU network (wired connection in your office or WiFi) or if you have VPN connection initiated. In this case you can ssh directly to the clusters.  ","version":"Next","tagName":"h2"},{"title":"Command Line Interface (Use Terminal)​","type":1,"pageTitle":"Connecting to the HPC Cluster","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/connecting_to_hpc/#command-line-interface-use-terminal","content":" ","version":"Next","tagName":"h2"},{"title":"Mac & Linux Access​","type":1,"pageTitle":"Connecting to the HPC Cluster","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/connecting_to_hpc/#mac--linux-access","content":" To connect to the gateway servers, simply open a terminal application and enter the following command:  ssh &lt;NetID&gt;@gw.hpc.nyu.edu   After typing in your password you will be logged in to the cluster. Once this connection is established, you can make one more hop and connect to one of the HPC clusters:  # this will connect you to Greene HPC cluster ssh &lt;NetID&gt;@greene.hpc.nyu.edu   ","version":"Next","tagName":"h3"},{"title":"Windows CMD​","type":1,"pageTitle":"Connecting to the HPC Cluster","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/connecting_to_hpc/#windows-cmd","content":" Windows 11 users have several options. First, the CMD program should contain an ssh client, allowing you to log into Greene or Hudson the same way as with a Linux terminal.  ","version":"Next","tagName":"h3"},{"title":"Windows WSL2​","type":1,"pageTitle":"Connecting to the HPC Cluster","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/connecting_to_hpc/#windows-wsl2","content":" If you run Windows 10, you can install WSL, and then install Ubuntu or other Linux distribution (for example, from Microsoft Store). You will have a fully functional Ubuntu with terminal and can connect to cluster using instructions provided above for Linux/Mac users.  Instructions on WSL installation can be found here: https://docs.microsoft.com/en-us/windows/wsl/install-win10  tip One of many options to get terminal that support tabs, etc. is to install 'Windows Terminal' from Microsoft Store.If you are using WSL 2 (Windows subsystem for Linux), you may not be able to access internet when Cisco AnyConnect VPN, installed from exe file, is activated. A potential solution: uninstall Cisco AnyConnect and install AnyConnect using Microsoft Store, and then setup new VPN connection using settings described on IT webpage.  ","version":"Next","tagName":"h3"},{"title":"Setting up SSH Keys​","type":1,"pageTitle":"Connecting to the HPC Cluster","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/connecting_to_hpc/#setting-up-ssh-keys","content":" Instead of typing your password every time you need to log in, you can also specify an ssh key.  Only do that on the computer you trust Generate ssh key pair (terminal in Linux/Mac or cmd/WSL in Windows):https://www.ssh.com/ssh/keygen/ Note the path to ssh key files. Don't share key files with anybody - anybody with this key file can login to your account Log into cluster using regular login/password and then add the content of generated public key file (the one with .pub) to $HOME/.ssh/authorized_keys on cluster Next time you will log into cluster no password will be required  For additional recommendations on how to configure your SSH sessions, see the [ssh configuring and x11 forwarding page].  ","version":"Next","tagName":"h3"},{"title":"PuTTY (Only for Windows)​","type":1,"pageTitle":"Connecting to the HPC Cluster","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/connecting_to_hpc/#putty-only-for-windows","content":" There are many SSH clients for Windows OS, but we recommend using PuTTY SSH if you have not already. Once it is installed, launch PuTTY and configure new session &quot;Session&quot; category as in the screenshot below:    Here we are instructing PuTTY to connect to host gw.hpc.nyu.edu on port 22 using SSH protocol (note, that this interface allows you to save this connection configuration for future). Just like for Linux and Mac users, if you are connecting from the outside of NYU network, you need to go through the gateway servers.  Once you click &quot;Open&quot;, a terminal window with prompt for password will pop up. Enter your NetID password and you should be authorized on the gateway server. Gateways are designed to support only a very minimal set of commands and their only purpose it to let users access HPC systems. Once you are there type in an ssh command that will let you connect to Greene cluster :  # Greene Login ssh greene.hpc.nyu.edu   A new command line interface window will open up that prompts you for your password on the gateway server, from there you can connect to Greene by entering the following:  ssh greene.hpc.nyu.edu  ","version":"Next","tagName":"h2"},{"title":"Open OnDemand (Web-based Graphical User Interface)","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/ood/","content":"","keywords":"","version":"Next"},{"title":"Access the Shell​","type":1,"pageTitle":"Open OnDemand (Web-based Graphical User Interface)","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/ood/#access-the-shell","content":" Under the clusters menu you can select the Greene Shell Access option to access the Linux shell. No local SSH client is required.    Interactive Applications  GUI based applications are accessible without the need for port or X11 forwarding. Select the Interactive Apps menu, select the desired application, and submit the job based on required resources and options.    ","version":"Next","tagName":"h2"},{"title":"Troubleshooting Connections to Open OnDemand​","type":1,"pageTitle":"Open OnDemand (Web-based Graphical User Interface)","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/ood/#troubleshooting-connections-to-open-ondemand","content":" A common issue that can occur is receiving an error that the Open OnDemand page cannot be reached. Sometimes this can indicate that the service is down, but often this is an issue with the the local browser cache. You can test this by opening a private browser window and seeing if https://ood.hpc.nyu.edu will load. If it does, try deleting the cache for https://ood.hpc.nyu.edu in your browser history to resolve this issue.  In Chrome, this can be done by navigating to this page in your settings:  chrome://settings/content/all?searchSubpage=ood.hpc.nyu.edu&amp;search   The link above will automatically search for the Open OnDemand site data and cookies. You can then simply click on the trashcan icon to delete the site cache.    Once done, try navigating again to https://ood.hpc.nyu.edu and the site should load. For other issues please email hpc@nyu.edu. ","version":"Next","tagName":"h2"},{"title":"Apptainer/Singularity","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/containers/intro/","content":"Apptainer/Singularity","keywords":"","version":"Next"},{"title":"Using Containers on HPC","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/containers/containers/","content":"Using Containers on HPC","keywords":"","version":"Next"},{"title":"Squash File System and Singularity","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/containers/squash_file_system_and_singularity/","content":"","keywords":"","version":"Next"},{"title":"Working with Datasets​","type":1,"pageTitle":"Squash File System and Singularity","url":"/rts-docs-dev/docs/hpc/containers/squash_file_system_and_singularity/#working-with-datasets","content":" Writable ext3 overlay images have conda environments installed inside, Singularity can work with squashFS for fixed datasets, such as the coco datasets.  /scratch/work/public/ml-datasets/coco/coco-2014.sqf /scratch/work/public/ml-datasets/coco/coco-2015.sqf /scratch/work/public/ml-datasets/coco/coco-2017.sqf singularity exec \\ --overlay /scratch/wang/zzz/pytorch1.8.0-cuda11.1.ext3:ro \\ --overlay /scratch/work/public/ml-datasets/coco/coco-2014.sqf:ro \\ --overlay /scratch/work/public/ml-datasets/coco/coco-2015.sqf:ro \\ --overlay /scratch/work/public/ml-datasets/coco/coco-2017.sqf:ro \\ /scratch/work/public/singularity/cuda11.1-cudnn8-devel-ubuntu18.04.sif /bin/bash   If you have many tiny files as fixed datasets, please make squashFS files to work with Singularity. Here is an example  Make a temporary folder in /state/partition1, it is a folder in local hard drive on each computer node  mkdir -p /state/partition1/sw77 cd /state/partition1/sw77   Unzip files there, for example  tar -vxzf /scratch/work/public/examples/squashfs/imagenet-example.tar.gz   Change access permissions in case we'll share files with others  find imagenet-example -type d -exec chmod 755 {} \\; find imagenet-example -type f -exec chmod 644 {} \\;   Convert to a single squashFS file on host  mksquashfs imagenet-example imagenet-example.sqf -keep-as-directory   For more details on working with squashFS, please see this page from the SquashFS documentation.  Copy this file to /scratch  cp -rp /state/partition1/sw77/imagenet-example.sqf /scratch/sw77/.   To test, files are in /imagenet-example inside Singularity container  singularity exec --overlay /scratch/sw77/imagenet-example.sqf:ro /scratch/work/public/singularity/ubuntu-20.04.1.sif /bin/bash Singularity&gt; find /imagenet-example | wc -l 1303 Singularity&gt; find /state/partition1/sw77/imagenet-example | wc -l 1303   To delete the tempoary folder on host  rm -rf /state/partition1/sw77  ","version":"Next","tagName":"h2"},{"title":"Working with Datasets","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/datasets/working_with_datasets/","content":"Working with Datasets Please see the Squash File System and Singularity page in the Containers section for details about working with datasets on the clusters.","keywords":"","version":"Next"},{"title":"SSH Tunneling and X11 Forwarding","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/","content":"","keywords":"","version":"Next"},{"title":"Avoiding Man in the Middle Warning.​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#avoiding-man-in-the-middle-warning","content":" If you see this warning:  warning @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed.   Do not be alarmed - this is an issue that occurs because the cluster has multiple login nodes (log-1, log-2, and log-3) that greene.hpc.nyu.edu resolves to.  To avoid this warning, you can add these lines to your SSH configuration file. Open ~/.ssh/config and place the following lines in it:  tip Host greene.hpc.nyu.edu dtn.hpc.nyu.edu gw.hpc.nyu.edu StrictHostKeyChecking no ServerAliveInterval 60 ForwardAgent yes StrictHostKeyChecking no UserKnownHostsFile /dev/null LogLevel ERROR   The above will also fix SSH timeout errors by extending the ServerAliveInterval argument.  ","version":"Next","tagName":"h2"},{"title":"SSH Tunneling (Mac, Linux)​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#ssh-tunneling-mac-linux","content":" Setting up your workstation for SSH tunneling will make logging in and transferring files significantly easier, and installing and running an X server will allow you to use graphical software on the HPC clusters. X server is a software package that draws on your local screen windows created on a remote computer such as on the remote HPC.  Linux users have X set up already. Mac users can download and install XQuartz.  ","version":"Next","tagName":"h2"},{"title":"Set up a reusable tunnel​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#set-up-a-reusable-tunnel","content":" To avoid repeatedly setting up a tunnel, you can write the details of the tunnel into your SSH configuration file. Using your favorite editor, open the file ~/.ssh/config and place the following lines in it:  # first we create the tunnel, with instructions to pass incoming # packets on ports 8027 and 8028 through it and to specific locations Host hpcgwtunnel HostName gw.hpc.nyu.edu ForwardX11 no StrictHostKeyChecking no LocalForward 8027 greene.hpc.nyu.edu:22 UserKnownHostsFile /dev/null User &lt;Your NetID&gt; # next we create an alias for incoming packets on the port # The alias corresponds to where the tunnel forwards these packets Host greene HostName localhost Port 8027 ForwardX11 yes StrictHostKeyChecking no UserKnownHostsFile /dev/null LogLevel ERROR User &lt;Your NetID&gt;   Create this file/directory In case you don't have it. Make sure that &quot;.ssh&quot; directory has correct permissions (it should be &quot;700&quot; or &quot;drwx------&quot;). If needed, set permissions with:  chmod 700 ~/.ssh   You may also need to setup permissions on your local computer:  chmod 700 $HOME chmod 700 $HOME/.ssh ## to be safe, all files inside ~/.ssh should be set 600 chmod 600 ~/.ssh/*   ","version":"Next","tagName":"h3"},{"title":"Start the tunnel​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#start-the-tunnel","content":" To create the tunnel, ssh to it with the following command:  ssh hpcgwtunnel   tip You must leave this window open for the tunnel to remain open. It is best to start a new terminal window for subsequent logins.  ","version":"Next","tagName":"h3"},{"title":"Log in via the tunnel​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#log-in-via-the-tunnel","content":" Open a new terminal window and use ssh to log in to the cluster, as shown below.  ssh greene   Note that you must use the short name defined above in your .ssh/config file, not the fully qualified domain name:  Creating a once-off tunnel.  Alternatively, you can set up a once-off tunnel without editing .ssh/config by running the following command:  ssh -L 8027:greene:22 NetID@gw.hpc.nyu.edu # to set up a tunnel ssh -Y -p 8027 NetID@localhost   This is the equivalent to running &quot;ssh hpcgwtunnel&quot; in the reusable tunnel instructions, but the port forwarding is specified on the command line.  ","version":"Next","tagName":"h3"},{"title":"Tunneling (Windows)​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#tunneling-windows","content":" ","version":"Next","tagName":"h2"},{"title":"Creating the tunnel​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#creating-the-tunnel","content":" First open Putty and prepare to log in to gw.hpc.nyu.edu. If you saved your session during that process, you can load it by selecting from the &quot;Saved Sessions&quot; box and hitting &quot;Load&quot;. Don't hit &quot;Open&quot; yet! Under &quot;Connection&quot; -&gt; &quot;SSH&quot;, just below &quot;X11&quot;, select &quot;Tunnels Write &quot;8026&quot; (the port number) in the &quot;Source port&quot; box, and &quot;greene.hpc.nyu.edu:22&quot; (the machine you wish to tunnel to - 22 is the port that ssh listens on) in the &quot;Destination&quot; box Click &quot;Add&quot;. You can repeat step 3 with a different port number and a different destination. If you do this you will create multiple tunnels, one to each destination Before hitting &quot;Open&quot;, go back to the &quot;Sessions&quot; page, give the session a name (&quot;hpcgw_tunnel&quot;) and hit &quot;Save&quot;. Then next time you need not do all this again, just load the saved session Hit &quot;Open&quot; to login in to gw.hpc.nyu.edu and create the tunnel. A terminal window will appear, asking for your login name (NYU NetID) and password (NYU password). Windows may also ask you to allow certain connections through its firewall - this is so you can ssh to port 8026 on your workstation - the entrance to the tunnel  note You can add other NYU hosts to the tunnel by adding a new source port and destination and clicking &quot;Add&quot;. For example, you could add &quot;Source port = 8025&quot; and &quot;Destination = EXAMPLE.hpc.nyu.edu:22&quot;, then press &quot;Add&quot;. You would then perform Step 2 (below) twice - once for greene on port 8026 and once for an example server on port 8025.  Using your SSH tunnel: To log in via the tunnel, first the tunnel must be open. If you've just completed Step 1, it will be open and you can jump down to &quot;Step 2: Logging in via your SSH tunnel&quot;. If you completed Step 1 yesterday, and now want to re-use the tunnel you created, first start the tunnel:  Starting the tunnel: During a session, you need only do this once - as long as the tunnel is open, new connections will go over it.  Start Putty.exe (again, if necessary), and load the session you saved in settings during procedure above Hit &quot;Open&quot;, and log in to the bastion host with your NYU NetID and password. This will create the tunnel.  ","version":"Next","tagName":"h3"},{"title":"Logging in via your SSH tunnel​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#logging-in-via-your-ssh-tunnel","content":" Start the second Putty.exe. In the &quot;Host Name&quot; box, write &quot;localhost&quot; and in the &quot;Port&quot; box, write &quot;8026&quot; (or whichever port number you specified when you set up the tunnel in the procedure above). We use &quot;localhost&quot; because the entrance of the tunnel is actually on this workstation, at port 8026 Go to &quot;Connections&quot; -&gt; &quot;SSH&quot; -&gt; &quot;X11&quot; and check &quot;Enable X11 forwarding&quot; Optionally, give this session a name (in &quot;Saved Sessions&quot;) and hit &quot;Save&quot; to save it. Then next time instead of steps 1 and 2 you can simply load this saved session Hit &quot;Open&quot;. You will again get a terminal window asking for your login (NYU NetID) and password (NYU password). You are now logged in to the HPC cluster!  ","version":"Next","tagName":"h3"},{"title":"X11 Forwarding​","type":1,"pageTitle":"SSH Tunneling and X11 Forwarding","url":"/rts-docs-dev/docs/hpc/connecting_to_hpc/ssh_tunneling_and_x11_forwarding/#x11-forwarding","content":" In rare cases when you need to interact with GUI applications on HPC clusters, you need to enable X11 forwarding for your SSH connection. Mac and Linux users will need to run the ssh commands described above with an additional flag:  ssh -Y &lt;NYU_NetID&gt;@greene.hpc.nyu.edu   However, Mac users need to install XQuartz, since X-server is no longer shipped with the macOS.  Windows users will also need to install X server software. We recommend two options out there. We recommend installing Xming. Start Xming application and configure PuTTY to support X11 forwarding: ","version":"Next","tagName":"h2"},{"title":"Datasets Available","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/datasets/intro/","content":"","keywords":"","version":"Next"},{"title":"General​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/docs/hpc/datasets/intro/#general","content":" The HPC team makes available a number of public sets that are commonly used in analysis jobs. The data sets are available Read-Only under  /scratch/work/public/ml-datasets//vast/work/public/ml-datasets/  We recommend to use version stored at /vast (when available) to have better read performance  note For some of the datasets users must provide a signed usage agreement before accessing  ","version":"Next","tagName":"h2"},{"title":"Format​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/docs/hpc/datasets/intro/#format","content":" Many datasets are available in the form of '.sqf' file, which can be used with Singularity. For example, in order to use coco dataset, one can run the following commands  $ singularity exec \\ --overlay /&lt;path&gt;/pytorch1.8.0-cuda11.1.ext3:ro \\ --overlay /vast/work/public/ml-datasets/coco/coco-2014.sqf:ro \\ --overlay /vast/work/public/ml-datasets/coco/coco-2015.sqf:ro \\ --overlay /vast/work/public/ml-datasets/coco/coco-2017.sqf:ro \\ /scratch/work/public/singularity/cuda11.1-cudnn8-devel-ubuntu18.04.sif /bin/bash $ singularity exec \\ --overlay /&lt;path&gt;/pytorch1.8.0-cuda11.1.ext3:ro \\ --overlay /vast/work/public/ml-datasets/coco/coco-2014.sqf:ro \\ --overlay /vast/work/public/ml-datasets/coco/coco-2015.sqf:ro \\ --overlay /vast/work/public/ml-datasets/coco/coco-2017.sqf:ro \\ /scratch/work/public/singularity/cuda11.1-cudnn8-devel-ubuntu18.04.sif find /coco | wc -l 532896   ","version":"Next","tagName":"h2"},{"title":"Data Sets​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/docs/hpc/datasets/intro/#data-sets","content":" ","version":"Next","tagName":"h2"},{"title":"COCO Dataset​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/docs/hpc/datasets/intro/#coco-dataset","content":" About data set: https://cocodataset.org/  Common Objects in Context (COCO) is a large-scale object detection, segmentation, and captioning dataset.  Dataset is available under/scratch  /scratch/work/public/ml-datasets/coco/coco-2014.sqf/scratch/work/public/ml-datasets/coco/coco-2015.sqf/scratch/work/public/ml-datasets/coco/coco-2017.sqf  /vast  /vast/work/public/ml-datasets/coco/coco-2014.sqf/vast/work/public/ml-datasets/coco/coco-2015.sqf/vast/work/public/ml-datasets/coco/coco-2017.sqf  ","version":"Next","tagName":"h3"},{"title":"ImageNet and ILSVRC​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/docs/hpc/datasets/intro/#imagenet-and-ilsvrc","content":" About data set: ImageNet (image-net.org)  ImageNet is an image dataset organized according to the WordNet hierarchy (Miller, 1995). Each concept in WordNet, possibly described by multiple words or word phrases, is called a “synonym set” or “synset”. ImageNet populates 21,841 synsets of WordNet with an average of 650 manually verified and full resolution images. As a result, ImageNet contains 14,197,122 annotated images organized by the semantic hierarchy of WordNet (as of August 2014). ImageNet is larger in scale and diversity than the other image classification datasets (https://arxiv.org/abs/1409.0575).  note WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept (https://wordnet.princeton.edu/)  ILSVRC (subset of ImageNet)​  ILSVRC uses a subset of ImageNet images for training the algorithms and some of ImageNet’s image collection protocols for annotating additional images for testing the algorithms (https://arxiv.org/abs/1409.0575). The name comes from 'ImageNet Large Scale Visual Recognition Challenge (ILSVRC)'. Competition was moved to Kaggle (http://image-net.org/challenges/LSVRC/2017/)  What is included (https://arxiv.org/abs/1409.0575).  1000 object classesapproximately 1.2 million training images50 thousand validation images100 thousand test imagesSize of data is about 150 GB (for train and validation)  Dataset is available under  /scratch/work/public/ml-datasets/imagenet/vast/work/public/ml-datasets/imagenet  Get access to Data​  New York University does not own this dataset.  Please open the ImageNet site, find the terms of use (http://image-net.org/download), copy them, replace the needed parts with your name, send us an email including the terms with your name - thereby confirming you agree to the these terms. Once you do this, we can grant you access to the copy of the dataset on the cluster.  ","version":"Next","tagName":"h3"},{"title":"Millions Songs​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/docs/hpc/datasets/intro/#millions-songs","content":" About data set: https://labrosa.ee.columbia.edu/millionsong/  Dataset is available under  /scratch/work/public/MillionSongDataset/vast/work/public/ml-datasets/millionsongdataset/  ","version":"Next","tagName":"h3"},{"title":"Twitter Decahose​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/docs/hpc/datasets/intro/#twitter-decahose","content":" About data set: https://developer.twitter.com/en/docs/twitter-api/enterprise/decahose-api/overview/decahose  NYU has a subscription to Twitter Decahose - 10% random sample of the realtime Twitter Firehose through a streaming connection  Data are stored in GCP cloud (BigQuery) and on HPC clusters Greene and Peel (Parquet format).  Please contact Megan Brown at The Center for Social Media &amp; Politics to get access to data and learn the tools available to work with it.  On cluster dataset is available under (given that you have permissions)  /scratch/work/twitter_decahose/  ","version":"Next","tagName":"h3"},{"title":"ProQuest Congressional Record​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/docs/hpc/datasets/intro/#proquest-congressional-record","content":" About data set: ProQuest Congressional Record  The ProQuest Congressional Record text-as-data collection consists of machine-readable files capturing the full text and a small number of metadata fields for a full run of the Congressional Record between 1789 and 2005. Metadata fields include the date of publication, subjects (for issues for which such information exists in the ProQuest system), and URLs linking the full text to the canonical online record for that issue on the ProQuest Congressional platform. A total of 31,952 issues are available.  Dataset is available under:  /scratch/work/public/proquest/  ","version":"Next","tagName":"h3"},{"title":"C4​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/docs/hpc/datasets/intro/#c4","content":" About data set: c4 | TensorFlow Datasets  A colossal, cleaned version of Common Crawl's web crawl corpus. Based on Common Crawl dataset: https://commoncrawl.org  Dataset is available under  /scratch/work/public/ml-datasets/c4/vast/work/public/ml-datasets/c4  ","version":"Next","tagName":"h3"},{"title":"GQA​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/docs/hpc/datasets/intro/#gqa","content":" About data set: GQA: Visual Reasoning in the Real World (stanford.edu)  Question Answering on Image Scene Graphs  Dataset is available under  /scratch/work/public/ml-datasets/gqa/vast/work/public/ml-datasets/gqa  ","version":"Next","tagName":"h3"},{"title":"MJSynth​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/docs/hpc/datasets/intro/#mjsynth","content":" About data set: Visual Geometry Group - University of Oxford  This is synthetically generated dataset which found to be sufficient for training text recognition on real-world images  This dataset consists of 9 million images covering 90k English words, and includes the training, validation and test splits used in the author's work (archived dataset is about 10 GB)  Dataset is available under  /vast/work/public/ml-datasets/mjsynth  ","version":"Next","tagName":"h3"},{"title":"open-images-dataset​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/docs/hpc/datasets/intro/#open-images-dataset","content":" About data set: Open Images Dataset – opensource.google  A dataset of ~9 million varied images with rich annotations  The images are very diverse and often contain complex scenes with several objects (8.4 per image on average). It contains image-level labels annotations, object bounding boxes, object segmentations, visual relationships, localized narratives, and more  Dataset is available under  /scratch/work/public/ml-datasets/open-images-dataset/vast/work/public/ml-datasets/open-images-dataset  ","version":"Next","tagName":"h3"},{"title":"Pile​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/docs/hpc/datasets/intro/#pile","content":" About data set: The Pile (eleuther.ai)  The Pile is a 825 GiB diverse, open source language modeling data set that consists of 22 smaller, high-quality datasets combined together.  Dataset is available under  /scratch/work/public/ml-datasets/pile/vast/work/public/ml-datasets/pile  ","version":"Next","tagName":"h3"},{"title":"Waymo open dataset​","type":1,"pageTitle":"Datasets Available","url":"/rts-docs-dev/docs/hpc/datasets/intro/#waymo-open-dataset","content":" About data set: Open Dataset – Waymo  The field of machine learning is changing rapidly. Waymo is in a unique position to contribute to the research community with some of the largest and most diverse autonomous driving datasets ever released.  Dataset is available under  /vast/work/public/ml-datasets/waymo_open_dataset_scene_flow/vast/work/public/ml-datasets/waymo_open_dataset_v_1_2_0_individual_files/vast/work/public/ml-datasets/waymo_open_dataset_v_1_3_2_individual_files/vast/work/public/ml-datasets/waymo_open_dataset_v_1_4_1_individual_files ","version":"Next","tagName":"h3"},{"title":"HPC Accounts for Sponsored External Collaborators","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/getting_started/hpc_accounts_external_collaborators/","content":"HPC Accounts for Sponsored External Collaborators External (non-NYU) collaborators can access, with proper sponsorship, the NYU HPC Environment. The first step is to sponsor a collaborator for an NYU netid (if they do not have one already). A department administrator or the faculty sponsor must submit the Affiliate Management Form (the link is only accessible over VPN, or within NYU-Net).Once a netid for the external collaborator is created, the collaborator must submit the Request for an NYU HPC account. tip The collaborator must setup VPN access to be able to access the HPC account request form.The collaborator must enter in the account request form the Netid of the sponsoring NYU Full time facultyThe collaborator should select &quot;External Collaborator&quot; as Affiliation, when filing the HPC account request form. Once the HPC request is submitted, the sponsoring faculty will receive an email with a link to approve (or deny) the HPC account request for the external collaborator. The account approval link can only be accessed over VPN. note Once the sponsoring faculty approves the account request, the HPC account is created within one hour. Once the HPC account is created, the external collaborator can access HPC resources as described here. note As with all sponsored accounts, HPC accounts for external collaborators are valid for a period of 12 months, at which point a renewal process is required to continue access to the NYU HPC environment.","keywords":"","version":"Next"},{"title":"Getting and Renewing an Account","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/getting_started/getting_and_renewing_an_account/","content":"","keywords":"","version":"Next"},{"title":"Who is eligible for an NYU HPC account ?​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/docs/hpc/getting_started/getting_and_renewing_an_account/#who-is-eligible-for-an-nyu-hpc-account-","content":" NYU HPC clusters and related resources are available to full-time NYU faculty and to all NYU staff and, students with sponsorship from a full-time NYU faculty.  ","version":"Next","tagName":"h2"},{"title":"Getting a new account on the NYU HPC clusters​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/docs/hpc/getting_started/getting_and_renewing_an_account/#getting-a-new-account-on-the-nyu-hpc-clusters","content":" To request an NYU HPC account please log in to NYU Identity Management service and follow the link to &quot;Request HPC account&quot;. We have a walkthrough of how to [request an account through IIQ]. If you are a student, alumni or an external collaborator you need an NYU faculty sponsor.  ","version":"Next","tagName":"h2"},{"title":"Renewing HPC account​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/docs/hpc/getting_started/getting_and_renewing_an_account/#renewing-hpc-account","content":" Each year, non-faculty users must renew their HPC account by filling in the account renewal form from the NYU Identity Management service. See Renewing your HPC account with IIQ for a walkthrough of the process.  ","version":"Next","tagName":"h2"},{"title":"Information for faculty who sponsor HPC users​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/docs/hpc/getting_started/getting_and_renewing_an_account/#information-for-faculty-who-sponsor-hpc-users","content":" All full-time NYU faculty members (other than NYU Med School) are eligible to become sponsors and in turn can sponsor:  NYU Degree program students Scholars visiting NYU NYU Research staff NYU School of Medicine faculty, staff and students Other NYU staff/affiliates with a NetID Non-NYU researchers with whom they are actively collaborating  If you need to sponsor an HPC account for an external collaborator (for example, for an NYU alumnus), please, request a &quot;research affiliate&quot; affiliation for your collaborator. You can find the instructions at https://start.nyu.edu/.  You can request a NetID for your student(s) or collaborator(s) at https://start.nyu.edu/pwm/public/. The request form has additional information about affiliates.  HPC faculty sponsors are expected to:  Approve/disapprove sponsored users' association with you Approve/disapprove the purpose for which user is requesting an account on NYU HPC resources Agree to supervise the sponsored individual, to the extent necessary, to ensure proper use of the NYU HPC resource and compliance with all applicable policies Respond promptly to account-related requests from HPC staff  Each year, your sponosred users must renew their account. You will need to approve the renewal by logging into the NYU Identity Management service. We have a walkthrogh of the approval process here  ","version":"Next","tagName":"h2"},{"title":"Bulk HPC Accounts for Courses​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/docs/hpc/getting_started/getting_and_renewing_an_account/#bulk-hpc-accounts-for-courses","content":" HPC bulk accounts request is disabled for HPC sponsors.  If you would like to use JupyterHub for your classes, please don't submit the form below, read [Jupyter Hub page] instead (the link to an intake form is also there) Please fill out this request form for the course, we'll create HPC accounts for the class per request Note that accounts created for courses last until the end of the semester, rather than a full year.  ","version":"Next","tagName":"h2"},{"title":"Getting an account with one of NYU partners​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/docs/hpc/getting_started/getting_and_renewing_an_account/#getting-an-account-with-one-of-nyu-partners","content":" NYU partners ([look for the list here]) with many state and national facilities with a variety of HPC systems and expertise. [Contact us] for assistance setting up a collaboration with any of these.  ","version":"Next","tagName":"h2"},{"title":"Non-NYU Researchers​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/docs/hpc/getting_started/getting_and_renewing_an_account/#non-nyu-researchers","content":" If you are part of collaboration with NYU researcher you need to obtain an affiliate status before applying for an NYU HPC account. A full-time NYU faculty member must sponsor a non-NYU collaborator for an affiliate status.  Please see instructions for affiliate management (NYU NetID login is required to follow the link). Please read instructions about sponsoring external collaborators here.  ","version":"Next","tagName":"h2"},{"title":"Access to cluster after Graduation​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/docs/hpc/getting_started/getting_and_renewing_an_account/#access-to-cluster-after-graduation","content":" If you will still work on a project with an NYU researchers after graduation - refer to the section above for &quot;Non-NYU Researchers&quot;  If you are not part of a collaboration, your access to cluster will end together with NetID becoming non-active. Please copy all your data cluster (if you need any) before that time.  ","version":"Next","tagName":"h2"},{"title":"VPN on a Linux machine​","type":1,"pageTitle":"Getting and Renewing an Account","url":"/rts-docs-dev/docs/hpc/getting_started/getting_and_renewing_an_account/#vpn-on-a-linux-machine","content":" note In order to request a new HPC account or renew an expired one, you need to be connected to the NYU VPN if you are working remotely, Please see instructions on how to install and use the NYU VPN. Linux clients are not officially supported, however we were able to successfully use openVPN client. Here are installation and connection instructions for a debian linux distribution with apt pacakge manager: apt-get install openconnect sudo openconnect -b vpn.nyu.edu When prompted follow the instructions and provide your netID, password, and authenticate with ('push', 'phone1' or 'sms') This method was tested on few Linux distributions and settings however is not guaranteeed to work in future. ","version":"Next","tagName":"h2"},{"title":"Start here!","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/getting_started/intro/","content":"Start here! Welcome to the Frances HPC documentation! If you do not have an HPC account, please proceed to the next section that explains how you may be able to get one. If you are an active user, you can proceed to one of the categories on the left.","keywords":"","version":"Next"},{"title":"Renewing your HPC Account with IIQ","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/getting_started/walkthrough_renew_hpc_account_iiq/","content":"Renewing your HPC Account with IIQ Login to the URL given below, using your netid/password, to create or manage HPC Account Requests: https://identity.it.nyu.edu/ (NYU VPN is required) Upon logging in, an end user’s landing page will look like this If the menu does not appear, select the &quot;burger&quot; menu on the top left hand corner: The burger menu will show an &quot;Update/Renew HPC Account&quot; option - select this. Next complete the form as instructed. Please note that all accounts require the sponsorship of a full-time NYU faculty member. The user’s name will be pre-populated, and the forms required fields must be completed (sponsor, reason for request, consent to terms of use). After clicking “Submit” the chosen sponsor will be notified of the request and provisioning will only occur after approval. NOTE: If your HPC Account is due for renewal you will get an update on your dashboard which will suggest you to fill out a form given in the &quot;Latest form&quot; widget for renewing your account If you are not a full-time NYU faculty member, you will need an NYU faculty member to sponsor your application. This is probably your thesis supervisor, or NYU collaborator. Hit submit, and the request will go to your sponsor to approve (if applicable), and your account will be created, usually within a day of being approved. You will be returned to the dashboard, and now you should see your request in the &quot;Pending Approvals&quot; tables. If after a few days you still do not have an account, check with your sponsor - they may have missed a step in the approval process. If you are still stuck, contact us at hpc@nyu.edu for assistance.","keywords":"","version":"Next"},{"title":"How to approve an HPC Account Request","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/getting_started/walkthrough_approve_hpc_account_request/","content":"How to approve an HPC Account Request When someone nominates you as their HPC sponsor, you should be notified by email. You can also log into IIQ at any time, and if you have a request awaiting your approval, it will appear in your &quot;Actions Items&quot; box, as per the following screenshot: Another way to get to pending approvals is to click on the line item in the “Latest Approvals” section which will lead directly to the approval page. For new HPC Account Requests, the page will look like this: Here, the Approve or Deny button should be clicked, then confirmed, in order to complete the request. If you have any difficulties or questions, please contact us at hpc@nyu.edu.","keywords":"","version":"Next"},{"title":"Machine Learning on HPC","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/ml_ai_hpc/intro/","content":"Machine Learning on HPC","keywords":"","version":"Next"},{"title":"Fine tune LLMs on HPC","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/ml_ai_hpc/llm_fine_tuning/","content":"Fine tune LLMs on HPC LoRA fine-tuning?","keywords":"","version":"Next"},{"title":"LLMs on HPC","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/ml_ai_hpc/llm_on_hpc/","content":"LLMs on HPC How to run LLMs on HPC.","keywords":"","version":"Next"},{"title":"PyTorch on HPC","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/ml_ai_hpc/pytorch_on_hpc/","content":"PyTorch on HPC Distribtued training, inference, etc","keywords":"","version":"Next"},{"title":"Greene Spec Sheet","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/spec_sheet/","content":"","keywords":"","version":"Next"},{"title":"Hardware Specs​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/docs/hpc/spec_sheet/#hardware-specs","content":" Please find Greene's hardware specification in detail at the google sheets here:  tip Hover a mouse over a cell with a black triangle to see more details_    ","version":"Next","tagName":"h2"},{"title":"Mounted Storage Systems​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/docs/hpc/spec_sheet/#mounted-storage-systems","content":" Please find the details on Greene's available storage offerings at the google sheets here:    ","version":"Next","tagName":"h2"},{"title":"General Parallel File System (GPFS)​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/docs/hpc/spec_sheet/#general-parallel-file-system-gpfs","content":" The NYU HPC Clusters are served by a General Parallel File System (GPFS) storage cluster. GPFS is a high-performance clustered file system software developed by IBM that provides concurrent high-speed file access to applications executing on multiple nodes of clusters.  The cluster storage runs on Lenovo Distributed Storage Solution DSS-G hardware:  2x DSS-G 202 116 Solid State Drives (SSDs)464 TB raw storage 2x DSS-G 240 668 Hard Disk Drives (HDDs)9.1 PB raw storage  ","version":"Next","tagName":"h2"},{"title":"GPFS Performance​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/docs/hpc/spec_sheet/#gpfs-performance","content":" \tRead Bandwidth\t78 GB per second reads Write Bandwidth\t42 GB per second writes I/O Performance\t~650k Input/Output operations per second (IOPS)  ","version":"Next","tagName":"h3"},{"title":"Flash Tier Storage (VAST)​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/docs/hpc/spec_sheet/#flash-tier-storage-vast","content":" An all flash file system, using VAST Flash storage is now available on Greene. Flash storage is optimal for computational workloads with high I/O rates. For example, if you have jobs to run with huge number of tiny files, VAST may be a good candidate.  Please contact the team hpc@nyu.edu for more information.  NVMe Interface778 TB Total StorageAvailable to all users as read onlyWrite access available to approved users only  ","version":"Next","tagName":"h2"},{"title":"Research Project Space (RPS)​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/docs/hpc/spec_sheet/#research-project-space-rps","content":" Research Project Space (RPS) volumes provide working spaces for sharing data and work amongst project or lab members for long term research needs.RPS directories are available on the Greene HPC cluster.RPS is backed up. There is no file purging policy on RPS.There is a cost per TB per year, and inodes per year for RPS volumes.  Please find more inforamtion at [Research Project Space page].  ","version":"Next","tagName":"h2"},{"title":"Data Transfer Nodes (gDTN)​","type":1,"pageTitle":"Greene Spec Sheet","url":"/rts-docs-dev/docs/hpc/spec_sheet/#data-transfer-nodes-gdtn","content":" \tNode Type\tLenovo SR630 Number of Nodes\t2 CPUs\t2x Intel Xeon Gold 6244 8C 150W 3.6 GHz Processor. Memory\t192 GB (total) - 12x 16GB DDR4, 2933 MHz Local Disk\t1x 1.92 TB SSD Infiniband Interconnect\t1x Mellanox ConnectX-6 HDR100/100GbE VPI 1-Port x16 PCIe 3.0 HCA Ethernet Connectivity to the NYU High-Speed Research Network (HSRN)\t200 Gbit - 1x Mellanox ConnectX-5 EDR IB/100GbE VPI Dual-Port x16 PCIe 3.0 HCA ","version":"Next","tagName":"h2"},{"title":"Available storage systems","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/storage/available_storage_systems/","content":"","keywords":"","version":"Next"},{"title":"GPFS​","type":1,"pageTitle":"Available storage systems","url":"/rts-docs-dev/docs/hpc/storage/available_storage_systems/#gpfs","content":" ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Available storage systems","url":"/rts-docs-dev/docs/hpc/storage/available_storage_systems/#configuration","content":" The NYU HPC cluster storage runs on Lenovo Distributed Storage Solution DSS-G hardware:  2x DSS-G 202 116 Solid State Drives (SSDs)464TB raw storage 2x DSS-G 240 668 Hard Disk Drives (HDDs)9.1PB raw storage  ","version":"Next","tagName":"h3"},{"title":"Performance​","type":1,"pageTitle":"Available storage systems","url":"/rts-docs-dev/docs/hpc/storage/available_storage_systems/#performance","content":" Read Speed: 78 GB per second read speedsWrite Speed: 42 GB per second write speedsI/O Performance: up to 650k input/output operations per second (IOPS)  ","version":"Next","tagName":"h3"},{"title":"Flash Tier Storage (VAST)​","type":1,"pageTitle":"Available storage systems","url":"/rts-docs-dev/docs/hpc/storage/available_storage_systems/#flash-tier-storage-vast","content":" An all flash file system, using VAST Flash storage, is now available on Greene. Flash storage is optimal for computational workloads with high I/O rates. For example, If you have jobs to run with huge amount of tiny files, VAST may be a good candidate. If you and your lab members are interested, please reach out to hpc@nyu.edu for more information.NVMe interfaceTotal size: 778 TBAccess: /vast is available for all users to read and available to approved users to write data.  ","version":"Next","tagName":"h2"},{"title":"Research Project Space (RPS)​","type":1,"pageTitle":"Available storage systems","url":"/rts-docs-dev/docs/hpc/storage/available_storage_systems/#research-project-space-rps","content":" Research Project Space (RPS) volumes provide working spaces for sharing data and code amongst project or lab members.RPS directories are available on the Greene HPC cluster.There is no old-file purging policy on RPS.RPS is backed up.There is a cost per TB per year and inodes per year for RPS volumes.  More information on the Research Project Space is available page.  ","version":"Next","tagName":"h2"},{"title":"Data Transfer Nodes Specs (gDTN)​","type":1,"pageTitle":"Available storage systems","url":"/rts-docs-dev/docs/hpc/storage/available_storage_systems/#data-transfer-nodes-specs-gdtn","content":" Node type: Lenovo SR630Number of nodes: 2CPU: 2x Intel Xeon Gold 6244 8C 150W 3.6GHz ProcessorMemory: 192GB (total) - 12x 16GB DDR4, 2933MHzLocal disk: 1x 1.92TB SSDInfiniband interconnect: 1x Mellanox ConnectX-6 HDR100 /100GbE VPI 1-Port x16 PCIe 3.0 HCAEthernet connectivity to the NYU High-Speed Research Network ( HSRN ): 200Gbit - 1x Mellanox ConnectX-5 EDR IB/100GbE VPI Dual-Port x16 PCIe 3.0 HCA ","version":"Next","tagName":"h2"},{"title":"Open OnDemand (OOD) with Conda/Singularity","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/ood/open_on_demand/","content":"","keywords":"","version":"Next"},{"title":"OOD + Singularity + conda​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/docs/hpc/ood/open_on_demand/#ood--singularity--conda","content":" This page describes how to use your Singularity with conda environment in Open OnDemand (OOD) GUI at Greene.  ","version":"Next","tagName":"h2"},{"title":"Log Into Greene via the Terminal​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/docs/hpc/ood/open_on_demand/#log-into-greene-via-the-terminal","content":" The following commands must be run from the terminal. Information on accessing via the terminal can be found at the Connecting to the HPC page.  ","version":"Next","tagName":"h3"},{"title":"Preinstallation Warning​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/docs/hpc/ood/open_on_demand/#preinstallation-warning","content":" warning If you have initialized Conda in your base environment, your prompt on Greene may show something like: (base) [NETID@log-1 ~]$ then you must first comment out or remove this portion of your ~/.bashrc file: # &gt;&gt;&gt; conda initialize &gt;&gt;&gt; # !! Contents within this block are managed by 'conda init' !! __conda_setup=&quot;$('/share/apps/anaconda3/2020.07/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)&quot; if [ $? -eq 0 ]; then eval &quot;$__conda_setup&quot; else if [ -f &quot;/share/apps/anaconda3/2020.07/etc/profile.d/conda.sh&quot; ]; then . &quot;/share/apps/anaconda3/2020.07/etc/profile.d/conda.sh&quot; else export PATH=&quot;/share/apps/anaconda3/2020.07/bin:$PATH&quot; fi fi unset __conda_setup # &lt;&lt;&lt; conda initialize &lt;&lt;&lt; The above code automatically makes your environment look for the default shared installation of Conda on the cluster and will sabotage any attempts to install packages to a Singularity environment. Once removed or commented out, log out and back into the cluster for a fresh environment.  ","version":"Next","tagName":"h3"},{"title":"Prepare Overlay File​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/docs/hpc/ood/open_on_demand/#prepare-overlay-file","content":" mkdir /scratch/$USER/my_env cd /scratch/$USER/my_env cp -rp /scratch/work/public/overlay-fs-ext3/overlay-15GB-500K.ext3.gz . gunzip overlay-15GB-500K.ext3.gz   Above we used the overlay file &quot;overlay-15GB-500K.ext3.gz&quot; which will contain all of the installed packages. There are more optional overlay files. You can find instructions on the following pages: Singularity with Conda, Squash File System and Singularity.  ","version":"Next","tagName":"h3"},{"title":"Launch Singularity Environment for Installation​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/docs/hpc/ood/open_on_demand/#launch-singularity-environment-for-installation","content":" singularity exec --overlay /scratch/$USER/my_env/overlay-15GB-500K.ext3:rw /scratch/work/public/singularity/cuda12.3.2-cudnn9.0.0-ubuntu-22.04.4.sif /bin/bash   Above we used the Singularity OS image &quot;cuda12.3.2-cudnn9.0.0-ubuntu-22.04.4.sif &quot; which provides the base operating system environment for the conda environment. There are other Singularity OS images available at /scratch/work/public/singularity  Launching Singularity with the --overlay flag mounts the overlay file to a new directory: /ext3 - you will notice that when not using Singularity /ext3 is not available. Be sure that you have the Singularity prompt (Singularity&gt;) and that /ext3 is available before the next step:  Singularity&gt; ls -lah /ext3 total 8.5K drwxrwxr-x. 2 root root 4.0K Oct 19 10:01 . drwx------. 29 root root 8.0K Oct 19 10:01 ..   ","version":"Next","tagName":"h3"},{"title":"Install Miniforge to Overlay File​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/docs/hpc/ood/open_on_demand/#install-miniforge-to-overlay-file","content":" wget --no-check-certificate https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh sh Miniforge3-Linux-x86_64.sh -b -p /ext3/miniforge3   Next, create a wrapper script at /ext3/env.sh  touch /ext3/env.sh echo '#!/bin/bash' &gt;&gt; /ext3/env.sh echo 'unset -f which' &gt;&gt; /ext3/env.sh echo 'source /ext3/miniforge3/etc/profile.d/conda.sh' &gt;&gt; /ext3/env.sh echo 'export PATH=/ext3/miniforge3/bin:$PATH' &gt;&gt; /ext3/env.sh echo 'export PYTHONPATH=/ext3/miniforge3/bin:$PATH' &gt;&gt; /ext3/env.sh   Your /ext3/env.sh file should now contain the following:  #!/bin/bash unset -f which source /ext3/miniforge3/etc/profile.d/conda.sh export PATH=/ext3/miniforge3/bin:$PATH export PYTHONPATH=/ext3/miniforge3/bin:$PATH   The wrapper script will activate your conda environment, to which you will be installing your packages and dependencies.  Next, activate your conda environment with the following:  source /ext3/env.sh   ","version":"Next","tagName":"h3"},{"title":"Install Packages to Miniforge Environment​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/docs/hpc/ood/open_on_demand/#install-packages-to-miniforge-environment","content":" Now that your environment is activated, you can update and install packages  conda config --remove channels defaults conda update -n base conda -y conda clean --all --yes conda install pip --yes conda install ipykernel --yes # Note: ipykernel is required to run as a kernel in the Open OnDemand Jupyter Notebooks   To confirm that your environment is appropriately referencing your Miniforge installation, try out the following:  unset which which conda # output: /ext3/miniforge3/bin/conda which python # output: /ext3/miniforge3/bin/python python --version # output: Python 3.8.5 which pip # output: /ext3/miniforge3/bin/pip   Now use either conda install or pip to install your required python packages to the Miniforge environment.  To install larger packages, like Tensorflow, you must first start an interactive job with adequate compute and memory resources to install packages. The login nodes restrict memory to 2GB per user, which may cause some large packages to crash.  srun --cpus-per-task=2 --mem=10GB --time=04:00:00 --pty /bin/bash # wait to be assigned a node singularity exec --overlay /scratch/$USER/my_env/overlay-15GB-500K.ext3:rw /scratch/work/public/singularity/cuda12.3.2-cudnn9.0.0-ubuntu-22.04.4.sif /bin/bash source /ext3/env.sh # activate the environment   After it is running, you’ll be redirected to a compute node. From there, run singularity to setup on conda environment, same as you were doing on login node.  ","version":"Next","tagName":"h3"},{"title":"Configure iPython Kernels​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/docs/hpc/ood/open_on_demand/#configure-ipython-kernels","content":" To create a kernel named my_env copy the template files to your home directory.  mkdir -p ~/.local/share/jupyter/kernels cd ~/.local/share/jupyter/kernels cp -R /share/apps/mypy/src/kernel_template ./my_env # this should be the name of your Singularity env cd ./my_env ls #kernel.json logo-32x32.png logo-64x64.png python # files in the ~/.local/share/jupyter/kernels directory   To set the conda environment, edit the file named 'python' in /.local/share/jupyter/kernels/my_env/.  The python file is a wrapper script that the Jupyter notebook will use to launch your Singularity container and attach it to the notebook.  At the bottom of the file we have the template singularity command.  singularity exec $nv \\ --overlay /scratch/$USER/my_env/overlay-15GB-500K.ext3:ro \\ /scratch/work/public/singularity/cuda12.3.2-cudnn9.0.0-ubuntu-22.04.4.sif \\ /bin/bash -c &quot;source /ext3/env.sh; $cmd $args&quot;   warning If you used a different overlay (/scratch/$USER/my_env/overlay-15GB-500K.ext3 shown above) or .sif file (/scratch/work/public/singularity/cuda12.3.2-cudnn9.0.0-ubuntu-22.04.4.sif shown above), you MUST change those lines in the command above to the files you used.  Edit the default kernel.json file by setting PYTHON_LOCATION and KERNEL_DISPLAY_NAME using a text editor like nano/vim.  { &quot;argv&quot;: [ &quot;PYTHON_LOCATION&quot;, &quot;-m&quot;, &quot;ipykernel_launcher&quot;, &quot;-f&quot;, &quot;{connection_file}&quot; ], &quot;display_name&quot;: &quot;KERNEL_DISPLAY_NAME&quot;, &quot;language&quot;: &quot;python&quot; }   to  { &quot;argv&quot;: [ &quot;/home/&lt;Your NetID&gt;/.local/share/jupyter/kernels/my_env/python&quot;, &quot;-m&quot;, &quot;ipykernel_launcher&quot;, &quot;-f&quot;, &quot;{connection_file}&quot; ], &quot;display_name&quot;: &quot;my_env&quot;, &quot;language&quot;: &quot;python&quot; }   Update the &quot;&lt;Your NetID&gt;&quot; to your own NetID without the &quot;&lt;&gt;&quot; symbols.  ","version":"Next","tagName":"h3"},{"title":"Launch an Open OnDemand Jupyter Notebook​","type":1,"pageTitle":"Open OnDemand (OOD) with Conda/Singularity","url":"/rts-docs-dev/docs/hpc/ood/open_on_demand/#launch-an-open-ondemand-jupyter-notebook","content":" https://ood.hpc.nyu.edu   ","version":"Next","tagName":"h3"},{"title":"Best Practices on HPC Storage","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/storage/best_practices/","content":"","keywords":"","version":"Next"},{"title":"User Quota Limits and the myquota command​","type":1,"pageTitle":"Best Practices on HPC Storage","url":"/rts-docs-dev/docs/hpc/storage/best_practices/#user-quota-limits-and-the-myquota-command","content":" All users have quote limits set on HPC fie systems. There are several types of quota limits, such as limits on the amount of disk space (disk quota), number of files (inode quota) etc. The default user quota limits on HPC file systems are listed on our Data Management page.  Running out of quota causes a variety of issues such as running user jobs being interrupted or users being unable to finish the installation of packages under their home directory.  One of the common issues users report is running out of inodes in their home directory. This usually occurs during software installation, for example installing conda environment under their home directory. Users can check their current utilization of quota using the myquota command. The myquota command provides a report of the current quota limits on mounted file systems, the user's quota utilization, as well as the percentage of quota utilization.  In the following example the user who executes the myquota command is out of inodes in their home directory. The user inode quota  limit on the /home file system 30.0K inodes and the user has 33000 inodes, thus 110% of the inode quota limit.  $ myquota Hostname: log-1 at Sun Mar 21 21:59:08 EDT 2021 Filesystem Environment Backed up? Allocation Current Usage Space Variable /Flushed? Space / Files Space(%) / Files(%) /home $HOME Yes/No 50.0GB/30.0K 8.96GB(17.91%)/33000(110.00%) /scratch $SCRATCH No/Yes 5.0TB/1.0M 811.09GB(15.84%)/2437(0.24%) /archive $ARCHIVE Yes/No 2.0TB/20.0K 0.00GB(0.00%)/1(0.00%) /vast $VAST No/Yes 2.0TB/5.0M 0.00GB(0.00%)/1(0.00%)   Users can find out the number of inodes (files) used per subdirectory under their home directory ($HOME), by running the following commands:  $cd $HOME $ for d in $(find $(pwd) -maxdepth 1 -mindepth 1 -type d | sort -u); do n_files=$(find $d | wc -l); echo $d $n_files; done /home/netid/.cache 1507 /home/netid/.conda 2 /home/netid/.config 2 /home/netid/.ipython 11 /home/netid/.jupyter 2 /home/netid/.keras 2 /home/netid/.local 24185 /home/netid/.nv 2 /home/netid/.sacrebleu 46 /home/netid/.singularity 1 /home/netid/.ssh 5 /home/netid/.vscode-server 7216   ","version":"Next","tagName":"h2"},{"title":"Large number of small files​","type":1,"pageTitle":"Best Practices on HPC Storage","url":"/rts-docs-dev/docs/hpc/storage/best_practices/#large-number-of-small-files","content":" In case your dataset or workflow requires to use large number of small files, this can create a bottleneck due to read/write rates.  Please refer to our page on working with a large number of files to learn about some of the options we recommend to consider.  ","version":"Next","tagName":"h2"},{"title":"Installing Python packages​","type":1,"pageTitle":"Best Practices on HPC Storage","url":"/rts-docs-dev/docs/hpc/storage/best_practices/#installing-python-packages","content":" Your home directory has relatively small number of inodes. In case you would create conda or python environment in you home directory, this can eat up all the inodes.  Please review best practices for managing packages under the Package Management section of the Greene Software Page. ","version":"Next","tagName":"h2"},{"title":"Globus","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/storage/globus/","content":"","keywords":"","version":"Next"},{"title":"Transferring data between endpoints​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/docs/hpc/storage/globus/#transferring-data-between-endpoints","content":" ","version":"Next","tagName":"h2"},{"title":"Endpoint​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/docs/hpc/storage/globus/#endpoint","content":" A globus Endpoint is a data transfer location, a location where data can be moved to or from using Globus transfer, sync and sharing service. An endpoint can either be a personal endpoint (on a user’s personal computer) or a server endpoint (located on a server, for use by multiple users). Please read for details.  ","version":"Next","tagName":"h3"},{"title":"Collection​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/docs/hpc/storage/globus/#collection","content":" A collection is a named set of files (or blobs), hierarchically organized in folders.  ","version":"Next","tagName":"h3"},{"title":"Data Sharing​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/docs/hpc/storage/globus/#data-sharing","content":" How to share data using Globus is described: https://docs.globus.org/how-to/share-files/  The first step in transferring data is to get a Globus account at https://www.globus.org/. Click on &quot;Log in&quot; at upper right corner. Select &quot;New York University&quot; from the pull-down menu and click on &quot;Continue&quot;.    Enter your NYU NetID and password in the familiar screen, and hit &quot;LOGIN&quot; then go through the Multi-Factor Authentication.    The &quot;File Manager&quot; panel should come up as the following image. In order to be able to transfer files, you will need to specify two Collections. A collection is defined on top of an endpoint. We can search for a collection using an endpoint name. The Server Endpoint on the NYU HPC storage is nyu#greene .        ","version":"Next","tagName":"h3"},{"title":"Server and Personal Endpoints​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/docs/hpc/storage/globus/#server-and-personal-endpoints","content":" note The NYU HPC Server Endpoint: nyu#greene  Globus Connect Server is already installed on the NYU HPC cluster creating a Server Endpoint named nyu#greene, that is available to authorized users (users with a valid HPC account) using Globus. If you want to move data to or from your computer and the NYU HPC cluster, you need to install Globus Connect Personal on your computer, thus creating a Personal Endpoint on your computer.  ","version":"Next","tagName":"h2"},{"title":"Moving data between Server Endpoints​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/docs/hpc/storage/globus/#moving-data-between-server-endpoints","content":" If you plan to transfer data between Server Endpoints, such as between the NYU server endpoint nyu#greene and a server endpoint at another institution, you do not need to install Globus Connect Personal on your computer.  Creating a Personal Endpoint on your computer​  This needs to be done only once on your personal computer.  After clicking &quot;Transfer or Sync to...&quot;, click &quot;Search&quot; on the upper right side. Then follow the link &quot;Install Globus Connect Personal&quot;.  More information about Globus Connect Personal and download links for Linux, Mac and Windows can be found at: https://www.globus.org/globus-connect-personal    ","version":"Next","tagName":"h3"},{"title":"Transfer files between your Personal Endpoint and NYU nyu#greene​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/docs/hpc/storage/globus/#transfer-files-between-your-personal-endpoint-and-nyu-nyugreene","content":" To transfer files you need to specify two collections (endpoints). Specify one of them as Greene scratch directory, or Greene archive directory or Greene home directory. The other endpoint is the one created for your personal computer (e.g. My Mac Laptop) if it is involved in the transfer. When you first use the Greene directory collection, authentication/consent is required for the Globus web app to manage collections on this endpoint on your behalf.    When writing to your Greene archive directory, please pay attention that there is a default inode limit of 20K per user.  When the second Endpoint is chosen to be your personal computer, your computer home directory content will show up. Now select directory and files (you may select multiple files when clicking on file names while pressing down &quot;shift&quot; key), click one of the two blue Start buttons to indicate the transfer direction. After clicking the blue Start button, you should see a message indicating a transfer request has been submitted successfully, and a transfer ID is generated. Globus file transfer service takes care of the actual copying.  When the transfer is done, you should receive an email notification. Click &quot;ACTIVITY&quot; on the Globus portal, select the transfer you want to check, a finished transfer should look like the following:    ","version":"Next","tagName":"h3"},{"title":"Small file download from web browsers​","type":1,"pageTitle":"Globus","url":"/rts-docs-dev/docs/hpc/storage/globus/#small-file-download-from-web-browsers","content":" Globus support HTTPS access to data. To download a small file from your web browser, select a file and right-click your mouse, then click 'Download' at the popup menu.    Additional info can be found at this page https://docs.globus.org/how-to/get-started/. Feel free to send any question. Good luck! ","version":"Next","tagName":"h3"},{"title":"Data Transfers","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/storage/data_transfers/","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/docs/hpc/storage/data_transfers/#introduction","content":" The main tools to transfer data to/from HPC systems  Linux tools like scp and rsyncPlease use Data transfer nodes  note Note: while one can transfer data while on login nodes, it is considered a bad practice  Globusrclone to/from cloud storage like NYU (Google) DriveOpenOnDemandOther tools  ","version":"Next","tagName":"h2"},{"title":"Data-Transfer nodes​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/docs/hpc/storage/data_transfers/#data-transfer-nodes","content":" Attached to the NYU HPC cluster Greene, the Greene Data Transfer Node (gDTN) are nodes optimized for transferring data between cluster file systems (e.g. scratch) and other endpoints outside the NYU HPC clusters, including user laptops and desktops. The gDTNs have 100-Gb/s Ethernet connections to the High Speed Research Network (HSRN) and are connected to the HDR Infiniband fabric of the HPC clusters.  The HPC cluster filesystems include /home, /scratch, /archive and the HPC Research Project Space are available on the gDTN.  The Data-Transfer Node (DTN) can be access in a variety of ways  From NYU-net and the High Speed Research Network: use SSH to the DTN hostname gdtn.hpc.nyu.eduFrom the Greene cluster (e.g., the login nodes): the hostname can be shortened to gdtnFor example, to log in to a DTN from the Greene cluster, to carry out some copy operation, and to log back out, you can use a command sequence like:  ssh gdtn rsync ... logout   Via specific tools like Globus (see below)  ","version":"Next","tagName":"h2"},{"title":"Linux & Mac Tools​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/docs/hpc/storage/data_transfers/#linux--mac-tools","content":" ","version":"Next","tagName":"h2"},{"title":"scp and rsync​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/docs/hpc/storage/data_transfers/#scp-and-rsync","content":" Please transfer data using Data-Transfer nodes  Sometimes these two tools are convenient for transferring small files. Using the DTNs does not require to set up an SSH tunnel; use the hostname dtn.hpc.nyu.edu for one-step copying. See below for examples of commands invoked on the command line on a laptop running a Unix-like operating system:  scp HMLHWBGX7_n01_HK16.fastq.gz jdoe55@dtn.hpc.nyu.edu:/scratch/jdoe55/ rsync -av HMLHWBGX7_n01_HK16.fastq.gz jdoe55@dtn.hpc.nyu.edu:/scratch/jdoe55/   In particular, rsync can also be used on the DTNs to copy directories recursively between filesystems, e.g. (assuming that you are logged in to a DTN),  rsync -av /scratch/username/project1 /rw/sharename/   where username would be your user name, project1 a directory to be copied to the Research Workspace, and sharename the name of a share on the Research Workspace (either your NetID or the name of a project you're a member of).  ","version":"Next","tagName":"h3"},{"title":"Windows Tools​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/docs/hpc/storage/data_transfers/#windows-tools","content":" ","version":"Next","tagName":"h2"},{"title":"File Transfer Clients​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/docs/hpc/storage/data_transfers/#file-transfer-clients","content":" Windows 10 machines may have the Linux Subsystem installed, which will allow for the use of Linux tools, as listed above, but generally it is recommended to use a client such as WinSCP or FileZilla to transfer data. Additionally, Windows users may also take advantage of Globus to transfer files.  ","version":"Next","tagName":"h3"},{"title":"Tunneling​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/docs/hpc/storage/data_transfers/#tunneling","content":" Read the detailed instructions for setting up tunnels.  ","version":"Next","tagName":"h3"},{"title":"Globus​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/docs/hpc/storage/data_transfers/#globus","content":" Globus is the recommended tool to use for large-volume data transfers. It features automatic performance tuning and automatic retries in cases of file-transfer failures. Data-transfer tasks can be submitted via a web portal. The Globus service will take care of the rest, to make sure files are copied efficiently, reliably, and securely. Globus is also a tool for you to share data with collaborators, for whom you only need to provide the email addresses.  The Globus endpoint for Greene is available at nyu#greene. The endpoint nyu#prince has been retired.  Please see detailed instructions  ","version":"Next","tagName":"h2"},{"title":"rclone​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/docs/hpc/storage/data_transfers/#rclone","content":" rclone - rsync for cloud storage, is a command line program to sync files and directories to and from cloud storage systems such as Google Drive, Amazon Drive, S3, B2 etc. rclone is available on DTNs.  Please see the documentation for how to use it.  ","version":"Next","tagName":"h2"},{"title":"Open OnDemand​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/docs/hpc/storage/data_transfers/#open-ondemand","content":" One can use Open OnDemand interface to upload data. However, please use it only for small data!  tip Please use Data-Transfer nodes while moving large data  ","version":"Next","tagName":"h2"},{"title":"FDT​","type":1,"pageTitle":"Data Transfers","url":"/rts-docs-dev/docs/hpc/storage/data_transfers/#fdt","content":" FDT stands for &quot;Fast Data Transfer&quot;. It is a command line application written in Java. With the plugin mechanism, FDT allows users to load user-defined classes for Pre- and Post-Processing of file transfers. Users can start their own server processes. If you have use cases for FDT, visit the download page to get fdt.jar to start. Please contact hpc@nyu.edu for any questions. ","version":"Next","tagName":"h3"},{"title":"HPC Storage","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/storage/intro_and_data_management/","content":"","keywords":"","version":"Next"},{"title":"Highlights​","type":1,"pageTitle":"HPC Storage","url":"/rts-docs-dev/docs/hpc/storage/intro_and_data_management/#highlights","content":" 9.5 PB Total GPFS Storage Up to 78 GB per second read speedsUp to 650k input/output operations per second (IOPS) Research Project Space (RPS): RPS volumes provide working spaces for sharing data and code amongst project or lab members  ","version":"Next","tagName":"h2"},{"title":"Introduction to HPC Data Management​","type":1,"pageTitle":"HPC Storage","url":"/rts-docs-dev/docs/hpc/storage/intro_and_data_management/#introduction-to-hpc-data-management","content":" The NYU HPC Environment provides access to a number of file systems to better serve the needs of researchers managing data during the various stages of the research data lifecycle (data capture, analysis, archiving, etc.). Each HPC file system comes with different features, policies, and availability.  In addition, a number of data management tools are available that enable data transfers and data sharing, recommended best practices, and various scenarios and use cases of managing data in the HPC Environment.  Multiple public data sets are available to all users of the HPC environment, such as a subset of The Cancer Genome Atlas (TCGA), the Million Song Database, ImageNet, and Reference Genomes.  Below is a list of file systems with their characteristics and a summary table. Reviewing the list of available file systems and the various Scenarios/Use cases that are presented below, can help select the right file systems for a research project. As always, if you have any questions about data storage in the HPC environment, you can request a consultation with the HPC team by sending email to hpc@nyu.edu.  ","version":"Next","tagName":"h2"},{"title":"Data Security Warning​","type":1,"pageTitle":"HPC Storage","url":"/rts-docs-dev/docs/hpc/storage/intro_and_data_management/#data-security-warning","content":" warning Moderate Risk Data - HPC Approved​ The HPC Environment has been approved for storing and analyzing Moderate Risk research data, as defined in the NYU Electronic Data and System Risk Classification Policy.High Risk research data, such as those that include Personal Identifiable Information (PII) or electronic Protected Health Information (ePHI) or Controlled Unclassified Information (CUI) should NOT be stored in the HPC Environment. note only the Office of Sponsored Projects (OSP) and Global Office of Information Security (GOIS) are empowered to classify the risk categories of data. tip High Risk Data - Secure Research Data Environments (SRDE) Approved​ Because the HPC system is not approved for High Risk data, we recommend using an approved system like the Secure Research Data Environments (SRDE).  ","version":"Next","tagName":"h3"},{"title":"Data Storage options in the HPC Environment​","type":1,"pageTitle":"HPC Storage","url":"/rts-docs-dev/docs/hpc/storage/intro_and_data_management/#data-storage-options-in-the-hpc-environment","content":" User Home Directories​  Every individual user has a home directory (under /home/$USER, environment variable $HOME) for permanently storing code and important configuration files. Home Directories provide limited storage space (50 GB) and inodes (files) 30,000 per user. Users can check their quota utilization using the myquota command.  User home directories are backed up daily and old files under $HOME are not purged.  The User home directories are available on all HPC clusters (Greene) and on every cluster node (login nodes, compute nodes) as well as and Data Transfer Node (gDTN).  warning Avoid changing file and directory permissions in your home directory to allow other users to access files.  User Home Directories are not ideal for sharing files and folders with other users. HPC Scratch of Research Project Space (RPS) are better file systems for sharing data.  warning One of the common issues that users report regarding their home directories is running out of inodes, i.e. the number of files stored under their home exceeds the inode limit, which by default is set to 30,000 files. This typically occurs when users install software under their home directories, for example, when working with Conda and Julia environments, that involve many small files.  tip To find out the current space and inode quota utilization and the distribution of files under your home directory, please see: Understanding user quota limits and the myquota command.Working with Conda environments: To avoid running out of inode limits in home directories, the HPC team recommends setting up conda environments with Singularity overlay images  HPC Scratch​  The HPC scratch file system is the HPC file system where most of the users store research data needed during the analysis phase of their research projects. The scratch file system provides temporary storage for datasets needed for running jobs.  Files stored in the HPC scratch file system are subject to the HPC Scratch old file purging policy: Files on the /scratch file system that have not been accessed for 60 or more days will be purged.  Every user has a dedicated scratch directory (/scratch/$USER) with 5 TB disk quota and 1,000,000 inodes (files) limit per user.  The scratch file system is available on all nodes (compute, login, etc.) on Greene as well as Data Transfer Node (gDTN).  warning There are No Back ups of the scratch file system. Files that were deleted accidentally or removed due to storage system failures CAN NOT be recovered.  tip Since there are no back ups of HPC Scratch file system, users should not put important source code, scripts, libraries, executables in /scratch. These important files should be stored in file systems that are backed up, such as /home or Research Project Space (RPS). Code can also be stored in a git repository.Old file purging policy on HPC Scratch: All files on the HPC Scratch file system that have not been accessed for more than 60 days will be removed. It is a policy violation to use scripts to change the file access time. Any user found to be violating this policy will have their HPC account locked. A second violation may result in your HPC account being turned off.To find out the user's current disk space and inode quota utilization and the distribution of files under your scratch directory, please see: Understanding user quota Limits and the myquota command.Once a research project completes, users should archive their important files in the HPC Archive file system.  HPC Vast​  The HPC Vast all-flash file system is the HPC file system where users store research data needed during the analysis phase of their research projects, particuarly for high I/O data that can bottleneck on the scratch file system. The Vast file system provides temporary storage for datasets needed for running jobs.  Files stored in the HPC vast file system are subject to the HPC Vast old file purging policy: Files on the /vast file system that have not been accessed for 60 or more days will be purged.  Every user has a dedicated vast directory (/vast/$USER) with 2 TB disk quota and 5,000,000 inodes (files) limit per user.  The vast file system is available on all nodes (compute, login, etc.) on Greene as well as Data Transfer Node (gDTN).  warning There are No Back ups of the vastsc file system. Files that were deleted accidentally or removed due to storage system failures CAN NOT be recovered.  tip Since there are no back ups of HPC Vast file system, users should not put important source code, scripts, libraries, executables in /vast. These important files should be stored in file systems that are backed up, such as /home or Research Project Space (RPS). Code can also be stored in a git repository.Old file purging policy on HPC Vast: All files on the HPC Vast file system that have not been accessed for more than 60 days will be removed. It is a policy violation to use scripts to change the file access time. Any user found to be violating this policy will have their HPC account locked. A second violation may result in your HPC account being turned off.To find out the user's current disk space and inode quota utilization and the distribution of files under your vast directory, please see: Understanding user quota Limits and the myquota command.Once a research project completes, users should archive their important files in the HPC Archive file system.  HPC Research Project Space​  The HPC Research Project Space (RPS) provides data storage space for research projects that is easily shared amongst collaborators, backed up, and not subject to the old file purging policy. HPC RPS was introduced to ease data management in the HPC environment and eliminate the need of having to frequently copying files between Scratch and Archive file systems by having all projects files under one area. These benefits of the HPC RPS come at a cost. The cost is determined by the allocated disk space and the number of files (inodes).  For detailed information about RPS see: HPC Research Project Space  HPC Work​  The HPC team makes available a number of public sets that are commonly used in analysis jobs. The data sets are available Read-Only under /scratch/work/public.  For some of the datasets users must provide a signed usage agreement before accessing.  Public datasets available on the HPC clusters can be viewed on the Datasets page.  HPC Archive​  Once the Analysis stage of the research data lifecycle has completed, HPC users should tar their data and code into a single tar.gz file and then copy the file to their archive directory (/archive/$USER). The HPC Archive file system is not accessible by running jobs; it is suitable for long-term data storage. Each user has access to a default disk quota of 2TB and 20,000 inode (files) limit. The rather low limit on the number of inodes per user is intentional. The archive file system is available only on login nodes of Greene. The archive file system is backed up daily.  Here is an example tar command that combines the data in a directory named my_run_dir under $SCRATCH and outputs the tar file in the user's $ARCHIVE:  # to archive `$SCRATCH/my_run_dir` tar cvf $ARCHIVE/simulation_01.tar -C $SCRATCH my_run_dir   NYU (Google) Drive​  Google Drive (NYU Drive) is accessible from the NYU HPC environment and provides an option to users who wish to archive data or share data with external collaborators who do not have access to the NYU HPC environment.  Currently (March 2021) there is no limit on the amount of data a user can store on Google Drive and there is no cost associated with storing data on Google Drive (although we hear rumors that free storage on Google Drive may be ending soon).  However, there are limits to the data transfer rate in moving to/from Google Drive. Thus, moving many small files to Google Drive is not going to be efficient.  Please read the Instructions on how to use cloud storage within the NYU HPC Environment.  HPC Storage Mounts Comparison Table​    Please see the next page for best practices for data management on NYU HPC systems. ","version":"Next","tagName":"h3"},{"title":"Large Number of Small Files","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/storage/large_number_of_small_files/","content":"","keywords":"","version":"Next"},{"title":"Motivation​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/docs/hpc/storage/large_number_of_small_files/#motivation","content":" Many datasets contain a large number of files (for example ImageNet contains 14 million images, with ~150 GB size). How to deal with this data? How to store it? How to use for computations? Long-term storage of data is not an issue - an archive like tar.gz can handle this pretty well. However, when you want to use data in computations, the performance may depend on how you handle the data on disk.  Here are some ideas you can try and evaluate performance for your own project  ","version":"Next","tagName":"h2"},{"title":"Squash file system to be used with Singularity​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/docs/hpc/storage/large_number_of_small_files/#squash-file-system-to-be-used-with-singularity","content":" Please read here  ","version":"Next","tagName":"h2"},{"title":"Use jpg/png files on disk​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/docs/hpc/storage/large_number_of_small_files/#use-jpgpng-files-on-disk","content":" One option is to store image files (like png or jpg) on the disk and read from disk directly.  An issue with this approach, is that many linux file system can hold only a limited number of files.  # One can open greene cluster and run the following command $ df -ih /scratch/ Filesystem Inodes IUsed IFree IUse% Mounted on 10.0.0.40@o2ib:10.0.0.41@o2ib:/scratch1 1.6G 209M 1.4G 14% /scratch   This shows us that the total number of files '/scratch' can hold (currently) is about 1.6 G. This looks like a large number. But let us translate this into number of datasets like ImageNet (14 mil images) -&gt; 100 datasets like that would almost fully occupy Total possible slots for files! This is a problem!  And even if you can ignore this on your own PC, on HPC, there is a limit of files each user can put on /scratch to prevent such problems.  This is the reason why you just can't extract all those files in /scratch  ","version":"Next","tagName":"h2"},{"title":"SLURM_TMPDIR​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/docs/hpc/storage/large_number_of_small_files/#slurm_tmpdir","content":" Another option would be to start SLURM job and extract everything into $SLURM_TMPDIR. This can work, but would require to do un-tar every time you run SLURM command.  ","version":"Next","tagName":"h2"},{"title":"SLURM_RAM_TMPDIR​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/docs/hpc/storage/large_number_of_small_files/#slurm_ram_tmpdir","content":" You can also use the custom-made RAM mapped disk using #SLURM_RAM_TMPDIR while submitting the job. In this case when you start a job you first un-tar your files to $SLURM_RAM_TMPDIR and then read from there. This basically requires you to use 2*(size of the data) size of RAM just to hold the data.  ","version":"Next","tagName":"h2"},{"title":"Binary files (pickle, etc)​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/docs/hpc/storage/large_number_of_small_files/#binary-files-pickle-etc","content":" Store data in some binary file (say pickle in Python) which you load fully when you start SLURM job.  This option may require a lot of RAM - thus you may have to wait a long time for scheduler to find resources for your job. Also this approach would not work on regular PC without so much RAM, and thus your scripts are not transferable.  ","version":"Next","tagName":"h2"},{"title":"Container files, one-file databases​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/docs/hpc/storage/large_number_of_small_files/#container-files-one-file-databases","content":" Special containers, which allow to either load data fast fully or access chosen elements without loading the whole dataset into RAM.  ","version":"Next","tagName":"h2"},{"title":"SQLite​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/docs/hpc/storage/large_number_of_small_files/#sqlite","content":" If you have structured data, a good option would be to use SQLite. Please refer to this page for more information  ","version":"Next","tagName":"h3"},{"title":"HDF5​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/docs/hpc/storage/large_number_of_small_files/#hdf5","content":" One can think about HDF5 file as a &quot;container file&quot; (database of a sort), which holds a lot of objects inside.  HDF5 files do not have a file size limitation, and can hold huge number of objects inside, providing fast read/write access to those objects.  It is easy to learn how to subset data and load to RAM only to those data objects that you need.  More info:  Developers website book (free with NYU email) hdf5 in Pythonhdf5 in R  hdf5 supports reading and writing in parallel, so you can use several nodes reading from the same file.  More info: Documentation, Tutorial, Help Desk  ","version":"Next","tagName":"h3"},{"title":"LMDB​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/docs/hpc/storage/large_number_of_small_files/#lmdb","content":" LMDB (Lightning Memory-Mapped Database) is a light-weight, high-speed embedded database for key-value data.  Essentially, this is a large file sitting on the disk that contains a lot of smaller objects inside.  This is a memory-mapped database meaning, file can be larger than RAM. OS is responsible for managing the pages (like caching frequently uses pages).  For practical use it means: say you have 10 GB of RAM, and LMDB file of 100 GB. When you connect to this file, OS may deside to load 5GB to RAM, and the rest 95GB will be attached as virtual memory. PRINCE does not have limit for virtual memory. Of course, if your RAM is larger than LMDB file, this database will perform the best, as OS will have enough resources to keep what is needed directly in RAM.  note when you write key-value pairs to LMDB they have to be byte-encoded. For example, if you use Python you can use: for string st.encode(), for np.array use ar.tobytes(), or in general pickle.dumps()  Problem with large number of files: LMDB uses B Tree, which has O(long n) complexity for search.  Thus, when number of elements in LMDB becomes really big, search of specific element slows down considerably  More info:  Developer websitePython package for lmdR package for lmdbDeep Learning Tensorflow with LMDB examplePytorch with LMDB example  LMDB supports reading by many readers and many parallel thread from the same file  Formats inside HDF5/LMDB: binary, numpy, other..​  One can store data in different way inside LMDB or HDF5. For example we can store binary representation of jpeg, or we can store python numpy array. In the first case file can be read from any language, in the second - only from Python. We can also store objects from other languages - for example tibble in R  Other formats​  There are other formats like Bcolz, Zarr, and others. Some examples can be found here.  ","version":"Next","tagName":"h3"},{"title":"Example Code​","type":1,"pageTitle":"Large Number of Small Files","url":"/rts-docs-dev/docs/hpc/storage/large_number_of_small_files/#example-code","content":" A benchmarking of various ways of reading data was performed on now retired Prince HPC cluster. You can find the code used to perform that benchmarking and the results at this repository.For those of you interested in using multiple cores for data reading, this code example below may be useful. Multiple cores on the same node are used. Parallelization is based on joblib Python module ","version":"Next","tagName":"h2"},{"title":"Research Project Space (RPS)","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/storage/research_project_space/","content":"","keywords":"","version":"Next"},{"title":"Description​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/docs/hpc/storage/research_project_space/#description","content":" Research Project Space (RPS) volumes provide working space for sharing data and code amongst project or lab members. RPS directories are built on the same parallel file system (GPFS) like HPC Scratch. They are mounted on the cluster Compute Nodes, and thus they can be accessed by running jobs. RPS directories are backed up and there is no old file purging policy. These features of RPS simplify the management of data in the HPC environment as users of the HPC Cluster can store their data and code on RPS directories and they do not need to move data between the HPC Scratch and the HPC Archive file systems.  Due to limitations of the underlying parallel file system, the total number of RPS volumes that can be created is limited. There is an annual cost associated with RPS. The disk space and inode usage in RPS directories do not count towards quota limits in other HPC file systems (Home, Scratch, and Archive).  ","version":"Next","tagName":"h2"},{"title":"Calculating RPS Costs​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/docs/hpc/storage/research_project_space/#calculating-rps-costs","content":" The PI should estimate the cost of the RPS volume by taking into account storage size and number of inodes (files). The cost is calculated annually. Costs are divided into the total space, in terabytes, and the number of inodes, in blocks of 200,000.  1 TB of Storage Cost: $100200,000 inodes Cost: $100  An initial RPS volume request must include both storage space and inodes. Modifications of existing RPS volumes can include just Storage or just inode adjustments.  An initial request includes 1TB and 200,000 inodes for an annual cost of $200.  ","version":"Next","tagName":"h2"},{"title":"Example RPS Requests​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/docs/hpc/storage/research_project_space/#example-rps-requests","content":" Requests can include more storage or files, as needed, such as 1TB and 400,000 inodes or 2TB and 200,000 inodes. Both of the previous examples would cost $300, since they are requesting an incremental increase of storage or inodes, respectively.  This would be the breakdown of the examples listed above:  1 TB ($100) + 400,000 inodes ($200) = $3002 TB ($200) + 200,000 inodes ($100) = $300  ","version":"Next","tagName":"h3"},{"title":"Submitting an RPS volume Request or Modification​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/docs/hpc/storage/research_project_space/#submitting-an-rps-volume-request-or-modification","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Decide the size (in TB) and number of inodes (files) that is needed for one year​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/docs/hpc/storage/research_project_space/#step-1-decide-the-size-in-tb-and-number-of-inodes-files-that-is-needed-for-one-year","content":" The minimum size of an RPS request (to create a new RPS volume or extend an existing one) is 1TB of space.  If this is a new/first request, you must purchase both storage and inodes. A typical request includes 200,000 inodes per TB of storage.  Before submitting an RPS request (request for a new RPS volume or extending the size of an existing volume) PIs should estimate the growth of their data (in terms of storage space and number of files) during the entire year, rather than submitting a request based on their data storage needs at the time of the request.  ","version":"Next","tagName":"h3"},{"title":"Step 2: Determine the cost of the request​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/docs/hpc/storage/research_project_space/#step-2-determine-the-cost-of-the-request","content":" Determine the total annual cost of the request and the contact info of the School/Department/Center finance person.  ","version":"Next","tagName":"h3"},{"title":"Step 3: Verify that the project PI has a valid HPC user account​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/docs/hpc/storage/research_project_space/#step-3-verify-that-the-project-pi-has-a-valid-hpc-user-account","content":" The PI administers the top level RTS directory and grants access to other users. Thus the PI must have a valid HPC user account at the time of request. Please note that the HPC user account of NYU faculty never expires and thus does not need to be renewed every year. If the PI does not have an HPC account, please request one here.  ","version":"Next","tagName":"h3"},{"title":"Step 4: The PI submits the request to the HPC team via email​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/docs/hpc/storage/research_project_space/#step-4-the-pi-submits-the-request-to-the-hpc-team-via-email","content":" Only PIs can submit RPS requests by contacting the HPC team via email (hpc@nyu.edu). Please include in the request the size (TB and number of inodes), and the contact information of the School/Department/Center finance person. The HPC team will review the request and will contact the PI with any questions. If the request is approved, the HPC team will create (or adjust) the RPS volume with the PI's HPC user account as the owner of the RPS directory. An invoice will be generated by the IT finance team.  ","version":"Next","tagName":"h3"},{"title":"Current HPC RPS Stakeholders​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/docs/hpc/storage/research_project_space/#current-hpc-rps-stakeholders","content":" HPC RPS Stakeholders  ","version":"Next","tagName":"h2"},{"title":"FAQs​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/docs/hpc/storage/research_project_space/#faqs","content":" ","version":"Next","tagName":"h2"},{"title":"Data Retention and Backups​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/docs/hpc/storage/research_project_space/#data-retention-and-backups","content":" How long can I keep the lab data in RPS? For as long as the lab pays for the RPS resources. Even if the current HPC cluster retires, the RPS volumes will be transferred to the next cluster How can I find out how much of the storage and inodes have I used in my lab RPS volume Please contact HPC support What kind of backups are provided? Backups are done once a day (daily incremental). Backups are kept for 30 days. This means that if something was deleted more than 30 days ago, it won't be in the back ups and thus it won't be recoverable. Where are backups stored? RPS backups are stored on public cloud (AWS S3 Storage buckets).  ","version":"Next","tagName":"h3"},{"title":"Billing and Payments​","type":1,"pageTitle":"Research Project Space (RPS)","url":"/rts-docs-dev/docs/hpc/storage/research_project_space/#billing-and-payments","content":" What happens if I do not pay my bill? If the invoice is not paid for more than 60 days, the lab RPS directory will be 'tar'-ed and copied to an archival area. If 60 more days pass and the invoice is still not paid the tar files will be deleted. Can I pay for RPS using a credit card? Unfortunately we're unable to process credit card payments Can I pay for multiple years instead of paying every year? Yes, we can arrange for multiyear agreement ","version":"Next","tagName":"h3"},{"title":"Singularity with Conda","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/containers/singularity_with_conda/","content":"","keywords":"","version":"Next"},{"title":"What is Singularity?​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/docs/hpc/containers/singularity_with_conda/#what-is-singularity","content":" Singularity is a free, cross-platform and open-source program that creates and executes containers on the HPC clusters. Containers are streamlined, virtualized environments for specific programs or packages. Singularity is an industry standard tool to utilize containers in HPC environments. Containers allow for the support of highly specific environments and further increase scientific reproducibility and portability. Using Singularity containers, researchers can work in the reproducible containerized environments of their choice can easily tailor them to their needs.  ","version":"Next","tagName":"h2"},{"title":"Using Singularity Overlays for Miniforge (Python & Julia)​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/docs/hpc/containers/singularity_with_conda/#using-singularity-overlays-for-miniforge-python--julia","content":" ","version":"Next","tagName":"h2"},{"title":"Preinstallation Warning​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/docs/hpc/containers/singularity_with_conda/#preinstallation-warning","content":" warning If you have initialized Conda in your base environment, your prompt on Greene may show something like: (base) [NETID@log-1 ~]$ then you must first comment out or remove this portion of your ~/.bashrc file: # &gt;&gt;&gt; conda initialize &gt;&gt;&gt; # !! Contents within this block are managed by 'conda init' !! __conda_setup=&quot;$('/share/apps/anaconda3/2020.07/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)&quot; if [ $? -eq 0 ]; then eval &quot;$__conda_setup&quot; else if [ -f &quot;/share/apps/anaconda3/2020.07/etc/profile.d/conda.sh&quot; ]; then . &quot;/share/apps/anaconda3/2020.07/etc/profile.d/conda.sh&quot; else export PATH=&quot;/share/apps/anaconda3/2020.07/bin:$PATH&quot; fi fi unset __conda_setup # &lt;&lt;&lt; conda initialize &lt;&lt;&lt; The above code automatically makes your environment look for the default shared installation of Conda on the cluster and will sabotage any attempts to install packages to a Singularity environment. Once removed or commented out, log out and back into the cluster for a fresh environment.  ","version":"Next","tagName":"h3"},{"title":"Miniforge Environment PyTorch Example​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/docs/hpc/containers/singularity_with_conda/#miniforge-environment-pytorch-example","content":" Conda environments allow users to create customizable, portable work environments and dependencies to support specific packages or versions of software for research. Common conda distributions include Anaconda, Miniconda and Miniforge. Packages are available via &quot;channels&quot;. Popular channels include &quot;conda-forge&quot; and &quot;bioconda&quot;. In this tutorial we shall use Miniforge which sets &quot;conda-forge&quot; as the package channel. Traditional conda environments, however, also create a large number of files that can cut into quotas. To help reduce this issue, we suggest using Singularity, a container technology that is popular on HPC systems. Below is an example of how to create a pytorch environment using Singularity and Miniforge.  Create a directory for the environment  mkdir /scratch/&lt;NetID&gt;/pytorch-example cd /scratch/&lt;NetID&gt;/pytorch-example   Copy an appropriate gzipped overlay images from the overlay directory. You can browse available images to see available options  ls /scratch/work/public/overlay-fs-ext3   In this example we use overlay-15GB-500K.ext3.gz as it has enough available storage for most conda environments. It has 15GB free space inside and is able to hold 500K files You can use another size as needed.  cp -rp /scratch/work/public/overlay-fs-ext3/overlay-15GB-500K.ext3.gz . gunzip overlay-15GB-500K.ext3.gz   Choose a corresponding Singularity image. For this example we will use the following image  /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif   For Singularity image available on nyu HPC greene, please check the singularity images folder  /scratch/work/public/singularity/   For the most recent supported versions of PyTorch, please check the PyTorch website.  Launch the appropriate Singularity container in read/write mode (with the :rw flag)  singularity exec --overlay overlay-15GB-500K.ext3:rw /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif /bin/bash   The above starts a bash shell inside the referenced Singularity Container overlayed with the 15GB 500K you set up earlier. This creates the functional illusion of having a writable filesystem inside the typically read-only Singularity container.  Now, inside the container, download and install miniforge to /ext3/miniforge3  wget --no-check-certificate https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh bash Miniforge3-Linux-x86_64.sh -b -p /ext3/miniforge3 # rm Miniforge3-Linux-x86_64.sh # if you don't need this file any longer   Next, create a wrapper script /ext3/env.sh using a text editor, like nano.  touch /ext3/env.sh nano /ext3/env.sh   The wrapper script will activate your conda environment, to which you will be installing your packages and dependencies. The script should contain the following:  #!/bin/bash unset -f which source /ext3/miniforge3/etc/profile.d/conda.sh export PATH=/ext3/miniforge3/bin:$PATH export PYTHONPATH=/ext3/miniforge3/bin:$PATH   Activate your conda environment with the following:  source /ext3/env.sh   If you have the &quot;defaults&quot; channel enabled, please disable it with  conda config --remove channels defaults   Now that your environment is activated, you can update and install packages:  conda update -n base conda -y conda clean --all --yes conda install pip -y conda install ipykernel -y # Note: ipykernel is required to run as a kernel in the Open OnDemand Jupyter Notebooks   To confirm that your environment is appropriately referencing your Miniforge installation, try out the following:  unset -f which which conda # output: /ext3/miniforge3/bin/conda which python # output: /ext3/miniforge3/bin/python python --version # output: Python 3.8.5 which pip # output: /ext3/miniforge3/bin/pip exit # exit Singularity   Install packages​  You may now install packages into the environment with either the pip install or conda install commands.  First, start an interactive job with adequate compute and memory resources to install packages. The login nodes restrict memory to 2GB per user, which may cause some large packages to crash.  srun --cpus-per-task=2 --mem=10GB --time=04:00:00 --pty /bin/bash # wait to be assigned a node singularity exec --overlay overlay-15GB-500K.ext3:rw /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif /bin/bash source /ext3/env.sh # activate the environment   After it is running, you’ll be redirected to a compute node. From there, run singularity to setup on conda environment, same as you were doing on login node.  We will install PyTorch as an example:  pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116 pip3 install jupyter jupyterhub pandas matplotlib scipy scikit-learn scikit-image Pillow   For the latest versions of PyTorch please check the PyTorch website.  You can see the available space left on your image with the following commands:  find /ext3 | wc -l # output: should be something like 45445 du -sh /ext3 # output should be something like 4.9G /ext3   Now, exit the Singularity container and then rename the overlay image. Typing 'exit' and hitting enter will exit the Singularity container if you are currently inside it. You can tell if you're in a Singularity container because your prompt will be different, such as showing the prompt 'Singularity&gt;'  exit mv overlay-15GB-500K.ext3 my_pytorch.ext3   Test your PyTorch Singularity Image​  singularity exec --overlay /scratch/&lt;NetID&gt;/pytorch-example/my_pytorch.ext3:ro /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif /bin/bash -c 'source /ext3/env.sh; python -c &quot;import torch; print(torch.__file__); print(torch.__version__)&quot;' #output: /ext3/miniforge3/lib/python3.8/site-packages/torch/__init__.py #output: 1.8.0+cu111   note the end ':ro' addition at the end of the pytorch ext3 image starts the image in read-only mode. To add packages you will need to use ':rw' to launch it in read-write mode.  ","version":"Next","tagName":"h3"},{"title":"Using your Singularity Container in a SLURM Batch Job​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/docs/hpc/containers/singularity_with_conda/#using-your-singularity-container-in-a-slurm-batch-job","content":" Below is an example script of how to call a python script, in this case torch-test.py, from a SLURM batch job using your new Singularity image  torch-test.py:  #!/bin/env python import torch print(torch.__file__) print(torch.__version__) # How many GPUs are there? print(torch.cuda.device_count()) # Get the name of the current GPU print(torch.cuda.get_device_name(torch.cuda.current_device())) # Is PyTorch using a GPU? print(torch.cuda.is_available())   Now we will write the SLURM job script, run-test.SBATCH, that will start our Singularity Image and call the torch-test.py script.  run-test.SBATCH:  #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=1:00:00 #SBATCH --mem=2GB #SBATCH --gres=gpu #SBATCH --job-name=torch module purge singularity exec --nv \\ --overlay /scratch/&lt;NetID&gt;/pytorch-example/my_pytorch.ext3:ro \\ /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif\\ /bin/bash -c &quot;source /ext3/env.sh; python torch-test.py&quot;   You will notice that the singularity exec command features the '--nv flag' - this flag is reguired to pass the CUDA drivers from a GPU to the Singularity container.  Run the run-test.SBATCH script  sbatch run-test.SBATCH   Check your SLURM output for results, an example is shown below  cat slurm-3752662.out # example output: # /ext3/miniforge3/lib/python3.8/site-packages/torch/__init__.py # 1.8.0+cu111 # 1 # Quadro RTX 8000 # True   ","version":"Next","tagName":"h3"},{"title":"Optional: Convert ext3 to a compressed, read-only squashfs filesystem​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/docs/hpc/containers/singularity_with_conda/#optional-convert-ext3-to-a-compressed-read-only-squashfs-filesystem","content":" Singularity images can be compressed into read-only squashfs filesystems to conserve space in your environment. Use the following steps to convert your ext3 Singularity image into a smaller squashfs filesystem.  srun -N1 -c4 singularity exec --overlay my_pytorch.ext3:ro /scratch/work/public/singularity/centos-8.2.2004.sif mksquashfs /ext3 /scratch/&lt;NetID&gt;/pytorch-example/my_pytorch.sqf -keep-as-directory -processors 4   Here is an example of the amount of compression that can be realized by converting:  ls -ltrsh my_pytorch.* 5.5G -rw-r--r-- 1 wang wang 5.5G Mar 14 20:45 my_pytorch.ext3 2.2G -rw-r--r-- 1 wang wang 2.2G Mar 14 20:54 my_pytorch.sqf   Notice that it saves over 3GB of storage in this case, though your results may vary.  Use a squashFS Image for Running Jobs​  You can use squashFS images similarly to the ext3 images.  singularity exec --overlay /scratch/&lt;NetID&gt;/pytorch-example/my_pytorch.sqf:ro /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif /bin/bash -c 'source /ext3/env.sh; python -c &quot;import torch; print(torch.__file__); print(torch.__version__)&quot;' #example output: /ext3/miniforge3/lib/python3.8/site-packages/torch/__init__.py #example output: 1.8.0+cu111   Adding Packages to a Full ext3 or squashFS Image​  If the first ext3 overlay image runs out of space or you are using a squashFS conda enviorment, but need to install a new package inside, please copy another writable ext3 overlay image to work together.  Open the first image in read only mode  cp -rp /scratch/work/public/overlay-fs-ext3/overlay-2GB-100K.ext3.gz . gunzip overlay-2GB-100K.ext3.gz singularity exec --overlay overlay-2GB-100K.ext3 --overlay /scratch/&lt;NetID&gt;/pytorch-example/my_pytorch.ext3:ro /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu-22.04.2.sif /bin/bash source /ext3/env.sh pip install tensorboard   note Click here for information on how to configure your conda environment.  Please also keep in mind that once the overlay image is opened in default read-write mode, the file will be locked. You will not be able to open it from a new process. Once the overlay is opened either in read-write or read-only mode, it cannot be opened in RW mode from other processes either. For production jobs to run, the overlay image should be open in read-only mode. You can run many jobs at the same time as long as they are run in read-only mode. In this ways, it will protect the computation software environment, software packages are not allowed to change when there are jobs running.  ","version":"Next","tagName":"h3"},{"title":"Julia Singularity Image​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/docs/hpc/containers/singularity_with_conda/#julia-singularity-image","content":" Singularity can be used to set up a Julia environment.  Create a directory for your julia work, such as /scratch/&lt;NetID&gt;/julia, and then change to your home directory. An example is shown below.  mkdir /home/&lt;NetID&gt;/julia cd /home/&lt;NetID&gt;/julia   Copy an overlay image, such as the 2GB 100K overlay, which generally has enough storage for Julia packages. Once copied, unzip to the same folder, rename to julia-pkgs.ext3  cp -rp /scratch/work/public/overlay-fs-ext3/overlay-2GB-100K.ext3.gz . gunzip overlay-2GB-100K.ext3.gz mv overlay-2GB-100K.ext3 julia-pkgs.ext3   Copy the following wrapper script in the directory  cp -rp /share/apps/utils/julia-setup/* .   Now launch writable Singularity overlay to install packages  module purge module load knitro/12.3.0 module load julia/1.5.3 ~/julia/my-julia-writable using Pkg Pkg.add(&quot;KNITRO&quot;) Pkg.add(&quot;JuMP&quot;)   Now exit from the container to launch a read only version to test (example below)  ~/julia/my-julia _ _ _ _(_)_ | Documentation: https://docs.julialang.org (_) | (_) (_) | _ _ _| |_ __ _ | Type &quot;?&quot; for help, &quot;]?&quot; for Pkg help. | | | | | | |/ _` | | | | |_| | | | (_| | | Version 1.5.3 (2020-11-09) _/ |\\__'_|_|_|\\__'_| | Official https://julialang.org/ release |__/ | julia&gt; using Pkg julia&gt; using JuMP, KNITRO julia&gt; m = Model(with_optimizer(KNITRO.Optimizer)) A JuMP Model Feasibility problem with: Variables: 0 Model mode: AUTOMATIC CachingOptimizer state: EMPTY_OPTIMIZER Solver name: Knitro julia&gt; @variable(m, x1 &gt;= 0) x1 julia&gt; @variable(m, x2 &gt;= 0) x2 julia&gt; @NLconstraint(m, x1*x2 == 0) x1 * x2 - 0.0 = 0 julia&gt; @NLobjective(m, Min, x1*(1-x2^2)) julia&gt; optimize!(m)   You can make the above code into a julia script to test batch jobs. Save the following as test-knitro.jl  using Pkg using JuMP, KNITRO m = Model(with_optimizer(KNITRO.Optimizer)) @variable(m, x1 &gt;= 0) @variable(m, x2 &gt;= 0) @NLconstraint(m, x1*x2 == 0) @NLobjective(m, Min, x1*(1-x2^2)) optimize!(m)   You can add additional packages with commands like the one below.  note Please do not install new packages when you have Julia jobs running, this may create issues with your Julia installation)  ~/julia/my-julia-writable -e 'using Pkg; Pkg.add([&quot;Calculus&quot;, &quot;LinearAlgebra&quot;])'   Run a SLURM job to test with the following sbatch command (e.g. julia-test.SBATCH)  #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=1:00:00 #SBATCH --mem=2GB #SBATCH --job-name=julia-test module purge module load julia/1.5.3 module load knitro/12.3.0 ~/julia/my-julia test-knitro.jl   Then run the command with the following:  sbatch julia-test.SBATCH   Once the job completes, check the SLURM output (example below)  cat slurm-1022969.out ======================================= Academic License (NOT FOR COMMERCIAL USE) Artelys Knitro 12.3.0 ======================================= Knitro presolve eliminated 0 variables and 0 constraints. datacheck: 0 hessian_no_f: 1 par_numthreads: 1 Problem Characteristics ( Presolved) ----------------------- Objective goal: Minimize Objective type: general Number of variables: 2 ( 2) bounded below only: 2 ( 2) bounded above only: 0 ( 0) bounded below and above: 0 ( 0) fixed: 0 ( 0) free: 0 ( 0) Number of constraints: 1 ( 1) linear equalities: 0 ( 0) quadratic equalities: 0 ( 0) gen. nonlinear equalities: 1 ( 1) linear one-sided inequalities: 0 ( 0) quadratic one-sided inequalities: 0 ( 0) gen. nonlinear one-sided inequalities: 0 ( 0) linear two-sided inequalities: 0 ( 0) quadratic two-sided inequalities: 0 ( 0) gen. nonlinear two-sided inequalities: 0 ( 0) Number of nonzeros in Jacobian: 2 ( 2) Number of nonzeros in Hessian: 3 ( 3) Knitro using the Interior-Point/Barrier Direct algorithm. Iter Objective FeasError OptError ||Step|| CGits -------- -------------- ---------- ---------- ---------- ------- 0 0.000000e+00 0.000e+00 WARNING: The initial point is a stationary point and only the first order optimality conditions have been verified. EXIT: Locally optimal solution found. Final Statistics ---------------- Final objective value = 0.00000000000000e+00 Final feasibility error (abs / rel) = 0.00e+00 / 0.00e+00 Final optimality error (abs / rel) = 0.00e+00 / 0.00e+00 # of iterations = 0 # of CG iterations = 0 # of function evaluations = 1 # of gradient evaluations = 1 # of Hessian evaluations = 0 Total program time (secs) = 1.03278 ( 1.014 CPU time) Time spent in evaluations (secs) = 0.00000 ===============================================================================   ","version":"Next","tagName":"h3"},{"title":"Using CentOS 8 for Julia (for Module Compatibility)​","type":1,"pageTitle":"Singularity with Conda","url":"/rts-docs-dev/docs/hpc/containers/singularity_with_conda/#using-centos-8-for-julia-for-module-compatibility","content":" Building on the previous Julia example, this will demonstrate how to set up a similar environment using the Singularity CentOS 8 image for additional customization. Using the CentOS 8 overlay allows for the loading of modules installed on Greene, such as Knitro 12.3.0  Copy overlay image  cp -rp /scratch/work/public/overlay-fs-ext3/overlay-2GB-100K.ext3.gz . gunzip overlay-2GB-100K.ext3.gz mv overlay-2GB-100K.ext3 julia-pkgs.ext3   The path in this example is /scratch/&lt;NetID&gt;/julia/julia-pkgs.ext3  To use modules installed into /share/apps you can make two directories  mkdir julia-compiled julia-logs   Now, in this example, the absoulte paths are as follows  /scratch/&lt;NetID&gt;/julia/julia-compiled /scratch/&lt;NetID&gt;/julia/julia-logs   To launch Singularity with overlay images in writable mode to install packages  singularity exec \\ --overlay /scratch/&lt;NetID&gt;/julia/julia-pkgs.ext3 \\ --bind /share/apps \\ --bind /scratch/&lt;NetID&gt;/julia/julia-compiled:/ext3/pkgs/compiled \\ --bind /scratch/&lt;NetID&gt;/julia/julia-logs:/ext3/pkgs/logs \\ /scratch/work/public/apps/greene/centos-8.2.2004.sif \\ /bin/bash   Implement a wrapper script /ext3/env.sh  #/bin/bash export JULIA_DEPOT_PATH=/ext3/pkgs # this changes the default installation path to the environment source /opt/apps/lmod/lmod/init/bash module use /share/apps/modulefiles module purge module load knitro/12.3.0 module load julia/1.5.3   Load julia via the wrapper script and check that it loads properly  source /ext3/env.sh which julia # example output: /share/apps/julia/1.5.3/bin/julia julia --version # example output: julia version 1.5.3   Run julia to install packages  julia &gt; using Pkg &gt; Pkg.add(&quot;KNITRO&quot;) &gt; Pkg.add(&quot;JuMP&quot;)   Set up a similar test script like the test-knitro.jl script above. Name it test.jl:  using Pkg using JuMP, KNITRO m = Model(with_optimizer(KNITRO.Optimizer)) @variable(m, x1 &gt;= 0) @variable(m, x2 &gt;= 0) @NLconstraint(m, x1*x2 == 0) @NLobjective(m, Min, x1*(1-x2^2)) optimize!(m)   Now implement a wrapper script named julia into ~/bin, the overlay image is in readonly mode  #!/bin/bash args='' for i in &quot;$@&quot;; do i=&quot;${i//\\\\/\\\\\\\\}&quot; args=&quot;$args \\&quot;${i//\\&quot;/\\\\\\&quot;}\\&quot;&quot; done module purge singularity exec \\ --overlay /scratch/&lt;NetID&gt;/julia/julia-pkgs.ext3:ro \\ --bind /share/apps \\ --bind /scratch/&lt;NetID&gt;/julia/julia-compiled:/ext3/pkgs/compiled \\ --bind /scratch/&lt;NetID&gt;/julia/julia-logs:/ext3/pkgs/logs \\ /scratch/work/public/apps/greene/centos-8.2.2004.sif \\ /bin/bash -c &quot; source /ext3/env.sh julia $args &quot;   Make the wrapper executable  chmod 755 ~/bin/julia   Test your installation with a SLURM job example. The following code has been put into a file called test-julia-centos.SBATCH  #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=1:00:00 #SBATCH --mem=2GB #SBATCH --job-name=julia-test module purge julia test.jl   Run the above with the following:  sbatch test-julia-centos.SBATCH   Read the output (example below)  cat slurm-764085.out   Installing New Julia Packages Later​  Implement another writable julia-writable with overlay image writable in order to install new Julia packages later  cd /home/&lt;NetID&gt;/bin cp -rp julia julia-writable #!/bin/bash args='' for i in &quot;$@&quot;; do i=&quot;${i//\\\\/\\\\\\\\}&quot; args=&quot;$args \\&quot;${i//\\&quot;/\\\\\\&quot;}\\&quot;&quot; done module purge singularity exec \\ --overlay /scratch/&lt;NetID&gt;/julia/julia-pkgs.ext3 \\ --bind /share/apps \\ --bind /scratch/&lt;NetID&gt;/julia/julia-compiled:/ext3/pkgs/compiled \\ --bind /scratch/&lt;NetID&gt;/julia/julia-logs:/ext3/pkgs/logs \\ /scratch/work/public/apps/greene/centos-8.2.2004.sif \\ /bin/bash -c &quot; source /ext3/env.sh julia $args &quot;   Check the writable image  which julia-writable #example output: ~/bin/julia-writable   Install packages to the writable image  julia-writable -e 'using Pkg; Pkg.add([&quot;Calculus&quot;, &quot;LinearAlgebra&quot;])'   If you do not need host packages installed in /share/apps, you can work with Singularity OS image  /scratch/work/public/singularity/ubuntu-20.04.1.sif   download Julia installation package from https://julialang-s3.julialang.org/bin/linux/x64/1.5/julia-1.5.3-linux-x86_64.tar.gz  install Julia to /ext3, setup PATH properly. It will be easy to move to other servers in future. ","version":"Next","tagName":"h3"},{"title":"Sharing Data on HPC","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/storage/sharing_data_on_hpc/","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/docs/hpc/storage/sharing_data_on_hpc/#introduction","content":" To share files on the cluster with other users, we recommend using file access control lists (FACL) for a user to share access to their data with others. FACL mechanism allows a fine-grained control access to any files by any users or groups of users. We discourage users from setting '777' permissions with chmod, because this can lead to data loss (by a malicious user or unintentionally, by accident). Linux commands getfacl and setfacl are used to view and set access.  ACL mechanism, just like regular Linux POSIX, allows three different levels of access control:  Read (r) - the permission to see the contents of a fileWrite (w) - the permission to edit a fileeXecute (X) - the permission to call a file or run it (in this case we use X instead of x because the X permission uses inherited executable permissions and not all files need execution)  This level of access can be granted to  user (owner of the file)group (owner group)other (everyone else)  ACL allows to grant the same type access without modifying file ownership and without changing POSIX permissions.  ","version":"Next","tagName":"h2"},{"title":"Viewing ACL​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/docs/hpc/storage/sharing_data_on_hpc/#viewing-acl","content":" Use getfacl to retrieve access permissions for a file.  $ getfacl myfile.txt # file: myfile.txt # owner: ab123 # group: users user::rw- group::--- other::--- The example above illustrates that in most cases ACL looks just like the chmod-based permissions: owner of the file has read and write permission, members of the group and everyone else have no permissions at all. Setting ACL Modify access permissions Use setfacl: # general syntax: $ setfacl [option] [action/specification] file # most important options are # -m to modify ACL # -x to remove ACL # -R to apply the action recursively (apply to everything inside the directory) # To set permissions for a user (user is either the user name or ID): $ setfacl -m &quot;u:user:permissions&quot; &lt;file/dir&gt; ## To set permissions for a group (group is either the group name or ID): $ setfacl -m &quot;g:group:permissions&quot; &lt;file/dir&gt; # To set permissions for others: $ setfacl -m &quot;other:permissions&quot; &lt;file/dir&gt; # To allow all newly created files or directories to inherit entries from the parent directory (this will not affect files which will be copied into the directory afterwards): $ setfacl -dm &quot;entry&quot; &lt;dir&gt; # To remove a specific entry: $ setfacl -x &quot;entry&quot; &lt;file/dir&gt; # To remove the default entries: $ setfacl -k &lt;file/dir&gt; # To remove all entries (entries of the owner, group and others are retained): $ setfacl -b &lt;file/dir&gt;   ","version":"Next","tagName":"h2"},{"title":"Important: Give Access to Parent Directories in the Path​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/docs/hpc/storage/sharing_data_on_hpc/#important-give-access-to-parent-directories-in-the-path","content":" When you would like to set ACL to say /a/b/c/example.out, you also need to set appropriate ACLs to all the parent directories in the path. If you want to give read/write/execute permissions for the file /a/b/c/example.out, you would also need to give at least r-x permissions to the directories: /a, /a/b, and /a/b/c.  ","version":"Next","tagName":"h3"},{"title":"Remove All ACL Entries​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/docs/hpc/storage/sharing_data_on_hpc/#remove-all-acl-entries","content":" # setfacl -b abc   ","version":"Next","tagName":"h3"},{"title":"Check ACLs​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/docs/hpc/storage/sharing_data_on_hpc/#check-acls","content":" # getfacl abc # file: abc # owner: someone # group: someone user::rw- group::r-- other::r--   You can see with ls -l if a file has extended permissions set with setfacl: the + in the last column of the permissions field indicates that this file has detailed access permissions via ACLs:  $ ls -la total 304 drwxr-x---+ 18 ab123 users 4096 Apr 3 14:32 . drwxr-xr-x 1361 root root 0 Apr 3 09:35 .. -rw------- 1 ab123 users 4502 Mar 28 22:27 my_private_file -rw-r-xr--+ 1 ab123 users 29 Feb 11 23:18 dummy.txt   ","version":"Next","tagName":"h3"},{"title":"Flags​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/docs/hpc/storage/sharing_data_on_hpc/#flags","content":" Please read 'man setfacl' for possible flags. For example:  '-m' - modify'-x' - remove'-R' - recursive (apply ACL to all content inside a directory)'-d' - default (set given settings as default - useful for a directory - all the new content inside in the future will have given ACL)  ","version":"Next","tagName":"h3"},{"title":"Examples​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/docs/hpc/storage/sharing_data_on_hpc/#examples","content":" ","version":"Next","tagName":"h2"},{"title":"File ACL Example​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/docs/hpc/storage/sharing_data_on_hpc/#file-acl-example","content":" Set read, write, and execute (rwX) permissions for user johnny to file named abc:  # setfacl -m &quot;u:johnny:rwX&quot; abc   note We recommend for the permissions using a capital 'X' as using a lowercase 'x' will make all files executable, so we reommcned this: Check permissions: # getfacl abc # file: abc # owner: someone # group: someone user::rw- user:johnny:rwX group::r-- mask::rwX other::r-- Change permissions for user johnny: # setfacl -m &quot;u:johnny:r-X&quot; abc Check permissions: # getfacl abc # file: abc # owner: someone # group: someone user::rw- user:johnny:r-X group::r-- mask::r-X other::r--   ","version":"Next","tagName":"h3"},{"title":"Directory ACL Example​","type":1,"pageTitle":"Sharing Data on HPC","url":"/rts-docs-dev/docs/hpc/storage/sharing_data_on_hpc/#directory-acl-example","content":" Let's say alice123 wants to share directory /scratch/alice123/shared/researchGroup/group1 with user bob123  ## Read/execute access to /scratch/alice123 setfacl -m u:bob123:r-X /scratch/alice123 ## Read/execute access to /scratch/alice123/shared setfacl -m u:bob123:r-X /scratch/alice123/shared ## Read/execute access to /scratch/alice123/shared/researchGroup setfacl -m u:bob123:r-X /scratch/alice123/shared/researchGroup ## Now I can finally can give access to directory /scratch/alice123/shared/researchGroup/group1 setfacl -Rm u:bob123:rwX /scratch/alice123/shared/researchGroup/group1   note user bob123 will be able to see content of the following directories /scratch/alise123//scratch/alise123/shared/scratch/alise123/shared/researchGroup//scratch/alise123/shared/researchGroup/group1 ","version":"Next","tagName":"h3"},{"title":"HPC Storage System Status","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/storage/system_status/","content":"","keywords":"","version":"Next"},{"title":"Allocation and Utilization data​","type":1,"pageTitle":"HPC Storage System Status","url":"/rts-docs-dev/docs/hpc/storage/system_status/#allocation-and-utilization-data","content":" Below you can find data for the following file system mounts  GPFS file system: /home, /scratch, /archiveVAST file system: /vastHDFS file system of Hadoop cluster Peel  note To be able to see the panels below you need to be within NYU network. Use VPN if you are not on campus. You can find details about connecting to the VPN on the Connecting to the HPC Cluster page. ","version":"Next","tagName":"h2"},{"title":"Slurm: Main Commands","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_main_commands/","content":"","keywords":"","version":"Next"},{"title":"srun​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_main_commands/#srun","content":" Run a parallel job on cluster managed by Slurm, can be used:  Individual job submission where resources are allocated.In sbatch batch scripts as job steps making use of the allocated resource pool.within salloc instance making use of the resource pool.  man srun # for more information   Option\tDescription--help\tDisplay help information and exit --account\tCharge resource used by this job to a specified account --ntasks or --nodes\tRequest the number of tasks for the job Or Request the number of nodes to be allocated for this job --ntasks-per-node\tRequest that ntasks be invoked on each node. Meant to be used with --nodes --cpus-per-task\tRequest that ncpus be allocated per process. This may be useful if the job is multithreaded and requires more than one CPU per task for optimal performance. --mem or --mem-per-cpu\tSpecify the real memory required per node. Default units are megabytes. Different units can be specified using the suffix [ K | M | G | T ] Or Minimum memory required per allocated CPU --output\tRedirect stdout to a file --error\tRedirect stderr to a file --label\tPrepend task numbers to lines of stdout/err --partition\tRequest a specific partition for the resource allocation. If not specified, the default behavior is to allow the slurm controller to select the default partition as designated by the system administrator. --pty\tExecute task zero with pseudo terminal mode or using pseudo terminal specified by &lt;File Descriptor&gt;. --gres\tSpecifies a comma-delimited list of generic consumable resources, examples: --gres=gpu:1, --gres=gpu:v100:2, --gres=help or --gres=none --chdir\tSet the working directory of srun before it is executed  ","version":"Next","tagName":"h2"},{"title":"sbatch​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_main_commands/#sbatch","content":" man sbatch # for more information   Some of the popularly used directives are:  Option\tDescription#SBATCH --account\tCharge resource used by this jab to a specified account #SBATCH --nodes or #SBATCH --ntasks\tRequest allocation of minimum or maximum nodes for this job #SBATCH --ntasks-per-node\tRequest that ntasks be invoked on each node, used with --nodes #SBATCH --cpus-per-task\tAdvise the Slurm controller that ensuing job steps will require ncpus number of processors per task. Without this option, the controller will just try to allocate one processor per task #SBATCH --mem\tSpecify the real memory required per node. Default units are megabytes. Different units can be specified using the suffix [ K | M | G | T ] #SBATCH --gres\tSpecifies a comma-delimited list of generic consumable resources. #SBATCH --output\tInstruct Slurm to connect the batch script's standard output directly to a specified filename #SBATCH --error\tInstruct Slurm to connect the batch script's standard error directly to a specified filename #SBATCH --mail-user\tUser to receive email notifications of state changes as defined by --mail-type #SBATCH --mail-type\tNotify user by email when certain event types occur. Valid type values are NONE, BEGIN, END, FAIL, REQUEUE, ALL etc. Multiple type values may be specified in a comma separated list. The user to be notified is indicated with --mail-user. #SBATCH --job-name\tSpecify a name for the job allocation, the default is the name of the batch script or just sbatch #SBATCH --constraint\tEnable constraints such as --constraint=&quot;nvidia&quot; to select any kind of nvidia GPUs or --constraint=&quot;amd&quot; to select any kind of amd GPUs or --constraint=&quot;a100|h100&quot; to select either any one of two GPUs #SBATCH --chdir\tSet the working directory of sbatch script before it is executed  ","version":"Next","tagName":"h2"},{"title":"salloc​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_main_commands/#salloc","content":" The options for salloc are similar to the ones used by srun or sbatch, consult the salloc manual pages for more information on additional options and their environment variables:  man salloc # for detailed information   ","version":"Next","tagName":"h2"},{"title":"sinfo​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_main_commands/#sinfo","content":" View information about slurm nodes and partitions.  man sinfo # for more information   sinfo --Format=Partition,GRES,CPUs,Features:26,NodeList   Format\tDescriptionAvailable\tState/availability of a partition CPUs\tNumber of CPUs per node CPUsState\tNumber of CPUs by state in the format &quot;allocated/idle/other/total&quot; Features:26\tFeatures available on the node, use : followed by a number which specifies the max number of characters printed for this column. sinfo prints max 20 characters by default per column Gres\tGeneric resource associated with the nodes GresUsed\tGeneric resource currently in use on the nodes MaxCPUsPerNode\tThe Max number of CPUs per node available to jobs in this partition Memory\tSize of memory per node in Megabytes NodeAI\tNumber of nodes by state in the format &quot;allocated/idle&quot; Nodes\tNumber of nodes NodeList\tList of node names Partition or PartitionName\tPartition name  ","version":"Next","tagName":"h2"},{"title":"squeue​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_main_commands/#squeue","content":" View information about jobs located on slurm scheduling queue.  man squeue # for more information   Options\tDescription--me\tPrints queued jobs for the current user --user\tPrints queued jobs under a specific user, or a comma list of users --job\tSpecify a comma seperated list of job IDs to display --help\tPrint a help message describing all options squeue  ","version":"Next","tagName":"h2"},{"title":"sacct​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_main_commands/#sacct","content":" Displays accounting data for all jobs and job steps in the Slurm job accounting log or Slurm database.  man sacct # for more information   Most popularly used format options are:  Options\tDescription--format\tComma separated list of fields. (use &quot;--helpformat&quot; for a list of available fields). NOTE: When using the format option for listing various fields you can put a %NUMBER afterwards to specify how many characters should be printed. e.g. format=name%30 will print 30 characters of field name right justified. A %-30 will print 30 characters left justified. --helpformat\tPrint a list of fields that can be specified with --format option  Some popular options for --format are:  Format\tDescriptionJobID\tThe identification number of the job or job step JobName\tThe name of the job or job step State\tDisplays the job status or state, such as COMPLETED, TIMEOUT, FAILED etc AllocCPUS\tNumber of CPUs allocated to the job Elapsed\tElapsed time for the job Start\tInitiation time for the job End\tTermination time for the job  ","version":"Next","tagName":"h2"},{"title":"scancel​","type":1,"pageTitle":"Slurm: Main Commands","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_main_commands/#scancel","content":" Used to signal jobs or job steps that are under the control of slurm. A signal in the sense, send a termination signal to cancel a job.  Options\tDescription--interactive\tInteractive mode. Confirm each job_id.step_id before performing the cancel operation --jobname\tRestrict the scancel operations to a provided job name --me\tCancel all your jobs scancel &lt;a_job_id&gt;\tCancel a job and all it's steps scancel &lt;a_job_id&gt;.&lt;step_id_a&gt; &lt;a_job_id&gt;.&lt;step_id_b&gt;\tOnly cancel steps a and b for a given job, but not the rest of the steps scancel &lt;JobID_ArrayID&gt;\tOnly cancel a array id of an job array ","version":"Next","tagName":"h2"},{"title":"Greene System Status","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/system_status/","content":"","keywords":"","version":"Next"},{"title":"Resources Allocation and Queue (AMD nodes not included)​","type":1,"pageTitle":"Greene System Status","url":"/rts-docs-dev/docs/hpc/system_status/#resources-allocation-and-queue-amd-nodes-not-included","content":"   ","version":"Next","tagName":"h2"},{"title":"Resource Allocation and Queue by partitions (AMD nodes not included)​","type":1,"pageTitle":"Greene System Status","url":"/rts-docs-dev/docs/hpc/system_status/#resource-allocation-and-queue-by-partitions-amd-nodes-not-included","content":"   ","version":"Next","tagName":"h2"},{"title":"AMD Nodes System Status​","type":1,"pageTitle":"Greene System Status","url":"/rts-docs-dev/docs/hpc/system_status/#amd-nodes-system-status","content":"   ","version":"Next","tagName":"h2"},{"title":"Storage System Status​","type":1,"pageTitle":"Greene System Status","url":"/rts-docs-dev/docs/hpc/system_status/#storage-system-status","content":" Below you may find data for the following file system mounts  GPFS file system: /home, /scratch, /archiveVAST file system: /vast   ","version":"Next","tagName":"h2"},{"title":"Transferring Cloud Storage Data with rclone","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/","content":"","keywords":"","version":"Next"},{"title":"Transferring files to and from Google Drive with RCLONE​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#transferring-files-to-and-from-google-drive-with-rclone","content":" Having access to Google Drive from the HPC environment provides an option to archive data and even share data with collaborators who have no access to the NYU HPC environment. Other options to archiving data include the HPC Archive file system and using Globus to share data with collaborators.  Access to Google Drive is provided by rclone - rsync for cloud storage - a command line program to sync files and directories to and from cloud storage systems such as Google Drive, Amazon Drive, S3, B2 etc. rclone is available on Greene cluster as a module, the module currently (November 2022) is rclone/1.60.1  For more details on how to use rclone to sync files to Google Drive, please see: https://rclone.org/drive/  rclone can be invoked in one of the three modes:  Copy mode to just copy new/changed filesSync (one way) mode to make a directory identicalCheck mode to check for file hash equality  Please try with these options:  rclone --transfers=32 --checkers=16 --drive-chunk-size=16384k --drive-upload-cutoff=16384k copy source:sourcepath dest:destpath   This option works great for file sizes 1Gb+ to 250GB. Keep in mind that there is a rate limiting of 2 files/sec for upload into Google Drive. Small file transfers don’t work that well. If you have many small jobs, please tar the parent directory of such folders and splits the tar file into 100GB chunks and uploads then into Google Drive.  ","version":"Next","tagName":"h2"},{"title":"rclone Configuration​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#rclone-configuration","content":" You need to configure rclone before you will be able to move files between the HPC Environment and Google Drive  There are specific instruction on the rclone web site: https://rclone.org/drive/  ","version":"Next","tagName":"h2"},{"title":"Step 1: Login to Greene:​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#step-1-login-to-greene","content":" Follow instructions to log into the Greene HPC cluster.  ","version":"Next","tagName":"h3"},{"title":"Step 2: Load the rclone module​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#step-2-load-the-rclone-module","content":" $ module load rclone/1.60.1   ","version":"Next","tagName":"h3"},{"title":"Step 3: Configure rclone​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#step-3-configure-rclone","content":" Configuring rclone and setting up remote access to your Google Drive, using the command:  $ rclone config   This will try to open the config files and you will see the below content:  You can select one of the options (here we show how to set up a new remote)  2021/03/23 18:10:29 NOTICE: Config file &quot;/home/netid/.config/rclone/rclone.conf&quot; not found - using defaults No remotes found - make a new one n) New remote s) Set configuration password q) Quit config n/s/q&gt; n name&gt; remote1 Type of storage to configure. Enter a string value. Press Enter for the default (&quot;&quot;). Choose a number from below, or type in your own value 1 / 1Fichier \\ &quot;fichier&quot; 2 / Alias for an existing remote \\ &quot;alias&quot; 3 / Amazon Drive \\ &quot;amazon cloud drive&quot; 4 / Amazon S3 Compliant Storage Provider (AWS, Alibaba, Ceph, Digital Ocean, Dreamhost, IBM COS, Minio, Tencent COS, etc) \\ &quot;s3&quot; 5 / Backblaze B2 \\ &quot;b2&quot; 6 / Box \\ &quot;box&quot; 7 / Cache a remote \\ &quot;cache&quot; 8 / Citrix Sharefile \\ &quot;sharefile&quot; 9 / Dropbox \\ &quot;dropbox&quot; 10 / Encrypt/Decrypt a remote \\ &quot;crypt&quot; 11 / FTP Connection \\ &quot;ftp&quot; 12 / Google Cloud Storage (this is not Google Drive) \\ &quot;google cloud storage&quot; 13 / Google Drive \\ &quot;drive&quot; 14 / Google Photos \\ &quot;google photos&quot; .... .... .... 37 / premiumize.me \\ &quot;premiumizeme&quot; 38 / seafile \\ &quot;seafile&quot; Storage&gt; 13 ** See help for drive backend at: https://rclone.org/drive/ ** Google Application Client Id Setting your own is recommended. See https://rclone.org/drive/#making-your-own-client-id for how to create your own. If you leave this blank, it will use an internal key which is low performance. Enter a string value. Press Enter for the default (&quot;&quot;). client_id&gt; Just Hit Enter OAuth Client Secret Leave blank normally. Enter a string value. Press Enter for the default (&quot;&quot;). client_secret&gt; Just Hit Enter Scope that rclone should use when requesting access from drive. Enter a string value. Press Enter for the default (&quot;&quot;). Choose a number from below, or type in your own value 1 / Full access all files, excluding Application Data Folder. \\ &quot;drive&quot; 2 / Read-only access to file metadata and file contents. \\ &quot;drive.readonly&quot; / Access to files created by rclone only. 3 | These are visible in the drive website. | File authorization is revoked when the user deauthorizes the app. \\ &quot;drive.file&quot; / Allows read and write access to the Application Data folder. 4 | This is not visible in the drive website. \\ &quot;drive.appfolder&quot; / Allows read-only access to file metadata but 5 | does not allow any access to read or download file content. \\ &quot;drive.metadata.readonly&quot; scope&gt; 1 ID of the root folder Leave blank normally. Fill in to access &quot;Computers&quot; folders (see docs), or for rclone to use a non root folder as its starting point. Enter a string value. Press Enter for the default (&quot;&quot;). root_folder_id&gt; Just Hit Enter Service Account Credentials JSON file path Leave blank normally. Needed only if you want use SA instead of interactive login. Leading `~` will be expanded in the file name as will environment variables such as `${RCLONE_CONFIG_DIR}`. Enter a string value. Press Enter for the default (&quot;&quot;). service_account_file&gt; Just Hit Enter Edit advanced config? (y/n) y) Yes n) No (default) y/n&gt; n Remote config Use auto config? * Say Y if not sure * Say N if you are working on a remote or headless machine y) Yes (default) n) No y/n&gt; n Please go to the following link: https://accounts.google.com/o/oauth2/auth?access_type=offline&amp;client_id= CUT AND PASTE The URL ABOVE INTO A BROWSER ON YOUR LAPTOP/DESKTOP Log in and authorize rclone for access Enter verification code&gt; ENTER VERIFICATION CODE HERE Configure this as a team drive? y) Yes n) No (default) y/n&gt; n -------------------- [remote1] type = drive scope = drive token = {&quot;access_token&quot;:&quot;, removed &quot;} -------------------- y) Yes this is OK (default) e) Edit this remote d) Delete this remote y/e/d&gt; y Current remotes: Name Type ==== ==== remote1 drive e) Edit existing remote n) New remote d) Delete remote r) Rename remote c) Copy remote s) Set configuration password q) Quit config e/n/d/r/c/s/q&gt; q   ","version":"Next","tagName":"h3"},{"title":"Step 4:​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#step-4","content":" Sample commands:  $ rclone lsd remote1:   Transferring files to Google Drive, using the command below:  $ rclone copy &lt;source_folder&gt; &lt;remote_name&gt;:&lt;name_of_folder_on_gdrive&gt;   It looks something like below:  $ rclone copy /home/user1 remote1:backup_home_user1   ","version":"Next","tagName":"h3"},{"title":"Step 5:​","type":1,"pageTitle":"Transferring Cloud Storage Data with rclone","url":"/rts-docs-dev/docs/hpc/storage/transferring_cloud_storage_data_with_rclone/#step-5","content":" The files are transferred and you can find the files on your Google Drive.  note Rclone only copies new files or files different from the already existing files on Google Drive. ","version":"Next","tagName":"h3"},{"title":"Tools and Software","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/tools_and_software/intro/","content":"Tools and Software","keywords":"","version":"Next"},{"title":"Slurm Tutorial","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_tutorial/","content":"","keywords":"","version":"Next"},{"title":"Introduction to High Performance Computing Clusters​","type":1,"pageTitle":"Slurm Tutorial","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_tutorial/#introduction-to-high-performance-computing-clusters","content":" In a High Performance Computing Cluster, such as the NYU-IT HPC Greene cluster, there are hundreds of computing nodes interconnected by high-speed networks.  Linux operating system ( in our case Red Hat Enterprise Linux) runs on each of the nodes individually. The resources are shared among many users for their technical or scientific computing purposes.  Slurm is a cluster software layer built on top of the interconnected nodes, aiming at orchestrating the nodes' computing activites, so that the cluster could be viewed as a unified, enhanced and scalable computing system by its users.  In NYU HPC clusters the users coming from many departments with various disciplines and subjects, with their own computing projects, impose on us very diverse requirements regarding hardware, software resources, and processing parallelism. Users submit jobs, which compete for computing resources.  The Slurm software system is a resource manager and a job scheduler, which is designed to allocate resources and schedule jobs. Slurm is an open-source software, with a large user community, and has been installed on many top 500 supercomputers.  This tutorial assumes you have a NYU HPC account. If not, you may find the steps to apply for an account on the Getting and renewing an account page. It also assumes you are comfortable with Linux command-line environment. To learn about linux please read [Tutorial 1]. Please review the [Hardware Specs page] for more information on Greene's hardware specifications.  ","version":"Next","tagName":"h2"},{"title":"Slurm Commands​","type":1,"pageTitle":"Slurm Tutorial","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_tutorial/#slurm-commands","content":" For an overview of useful Slurm commands, please read (Slurm Main Commands) page before continuing the tutorial.  ","version":"Next","tagName":"h2"},{"title":"Software and Environment Modules​","type":1,"pageTitle":"Slurm Tutorial","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_tutorial/#software-and-environment-modules","content":" Lmod, an Environment Module system, is a tool for managing multiple versions and configurations of software packages and is used by many HPC centers around the world. With Environment Modules, software packages are installed away from the base system directories, and for each pacakge, an associated modulefile describes what must be altered in a user's shell environment - such as the $PATH environment variable - in order to use the software package. The modulefile also describes dependencies and conflicts between this software package and other package and versions.  To use a given software package, you load the corresponding module. Unloading the module afterwards cleanly undoes the changes that loading the modules made to your environment, thus freeing you to use other software packages that might have conflicted with the first one.  Below is a list of modules and their associated functions:  module load &lt;module-name&gt; : loads a module For example : module load python3 module unload &lt;module-name&gt; : unloads a module For example : module unload python3 module show &lt;module-name&gt; : see exactly what effect loading a module will have with module purge : remove all loaded modules from your environment module whatis &lt;module-name&gt; : Find out more about a software package module list : check which modules are currently loaded in your environment module avail : check what software packages are available module help &lt;module-name&gt; : A module file may include more detailed help for software package  ","version":"Next","tagName":"h2"},{"title":"Batch Job Example​","type":1,"pageTitle":"Slurm Tutorial","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_tutorial/#batch-job-example","content":" Batch jobs require a script file for the SLURM scheduler to interpret and execute. The SBATCH file contains both commands specific for SLURM to interpret as well as programs for it execute. Below is a simple example of a batch job to run a Stata do file, the file is named myscript.sbatch :  #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=5:00:00 #SBATCH --mem=2GB #SBATCH --job-name=myTest #SBATCH --mail-type=END #SBATCH --mail-user=bob.smith@nyu.edu #SBATCH --output=slurm_%j.out module purge module load stata/14.2 RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*} mkdir -p $RUNDIR DATADIR=$SCRATCH/my_project/data cd $RUNDIR stata -b do $DATADIR/data_0706.do   Below we will break down each line of the SBATCH script. More options can be found on the (SchedMD website).  ## This tells the shell how to execute the script #!/bin/bash ## The #SBATCH lines are read by SLURM for options. ## In the lines below we ask for a single node, ## one task for that node, and one cpu for each task. #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 ## Time is the estimated time to complete, in this case 5 hours. #SBATCH --time=5:00:00 ## We expect no more than 2GB of memory to be needed #SBATCH --mem=2GB ## To make them easier to track, ## it's best to name jobs something recognizable. ## You can then use the name to look up reports with tools like squeue. #SBATCH --job-name=myTest ## These lines manage mail alerts for when the job ends, ## and who the email should be sent to. #SBATCH --mail-type=END #SBATCH --mail-user=bob.smith@nyu.edu ## This places the standard output and standard error into the same file, ## in this case slurm_&lt;job_id&gt;.out #SBATCH --output=slurm_%j.out ## First we ensure a clean environment by purging the current one module purge ## Load the desired software, in this case stata 14.2 module load stata/14.2 ## Create a unique directory to run the job in. RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*} mkdir -p $RUNDIR ## Set an environment variable for where the data is stored. DATADIR=$SCRATCH/my_project/data ## Change directories to the unique run directory cd $RUNDIR ## Execute the desired Stata do file script stata -b do $DATADIR/data_0706.do   You can submit the job with the following command:  sbatch myscript.sbatch   The command will result in the job queuing as it awaits resources to become available (which varies on the number of other jobs being run on the cluster). You can see the status of yor jobs with the following command:  squeue --me   NOTE: Calling just squeue without passing the --me option will display all users' job queue status by default  Lastly, you can read the output of your job in the slurm-&lt;job_ID&gt;.out file produced by running your job. This is where logs regarding the execution of your job can be found, including errors or system messages. You can print the contents to the screen from the directory containing the output file with the following command:  cat slurm-&lt;job_ID&gt;.out   ","version":"Next","tagName":"h2"},{"title":"Interactive Job Example​","type":1,"pageTitle":"Slurm Tutorial","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_tutorial/#interactive-job-example","content":" While the majority of the jobs on the cluster are submitted with the sbatch command, and executed in the background, there are also methods to run applications interactively throughthe srun command. Interactive jobs allow the users to enter commands and data on the command line (or in a graphical interface), providing an experience similar to working on a desktop or laptop. Examples of common interactive tasks are:  Editing files Compiling and debugging code Exploring data, to obtain a rough idea of characteristics on the topic Getting graphical windows to run visualization Running software tools in interactive sessions  Interactive jobs also help avoid issues with the login nodes. If you are working on a login node and your job is too IO intensive, it may be removed without notice. Running interactive jobs on compute nodes does not impact many users and in addition provides access to resources that are not available on the login nodes, such as interactive access to GPUs, high memory, exclusive access to all the resources of a compute node, etc.  In the srun example below, through --pty /bin/bash we request allocation of a pseudo terminal (pty) and start a bash shell session. By default the resource allocated is a single CPU core and 2GB memory for 1 hour time limit.  srun --pty /bin/bash   To request resources such as 4 CPU cores, 4 GB memory for 2 hours of maximum duration, you can add the following arguments:  srun --cpus-per-task=4 --time=2:00:00 --mem=4GB --pty /bin/bash   Similarly, to request one GPU card, 3 GB memory for a duration of 1.5 hours you can pass the following arguments to srun:  srun --time=1:30:00 --mem=3GB --gres=gpu:1 --pty /bin/bash   Once the job begins you will notice your prompt change, for example:  [mdw303@log-3 ~]$ srun --pty /bin/bash srun: job 7864254 queued and waiting for resources srun: job 7864254 has been allocated resources [mdw303@cs080 ~]$   You can see above that the prompt changed from log-3 ( one of the login nodes ) to cs080 ( one of the compute nodes ), meaning we have created a pseudo terminal and logged in with a bash shell on a compute node from our login node.  You can now load modules, software and run them interactively on the compute node having the resources ( CPUs, memory, GPUs etc ) that we asked for.  Below outlines the steps to start an interactive session and launch R:  [sk6404@log-1 ~]$ srun --cpus-per-task=1 --pty /bin/bash [sk6404@cs022 ~]$ module purge [sk6404@cs022 ~]$ module load r/intel/4.0.3 [sk6404@cs022 ~]$ module list Currently Loaded Modules: 1) intel/19.1.2 2) r/intel/4.0.3 [sk6404@cs022 ~]$ R R version 4.0.3 (2020-10-10) -- &quot;Bunny-Wunnies Freak Out&quot; Copyright (C) 2020 The R Foundation for Statistical Computing Platform: x86_64-centos-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. &gt; 5 + 10 [1] 15 &gt; q() Save workspace image? [y/n/c]: n [sk6404@cs022 ~]$ exit exit [sk6404@log-1 ~]$   ","version":"Next","tagName":"h2"},{"title":"MPI Job Example​","type":1,"pageTitle":"Slurm Tutorial","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_tutorial/#mpi-job-example","content":" MPI stands for &quot;Message Passing Interface&quot; and is managed by a program, such as OpenMPI, to coordinate code and resources across the HPC cluster for your job to run workloads in parallel. You may have heard of HPC sometimes referred to as &quot;parallel computing&quot; because the ability to run many processes simultaneously - aka in parallel - is how the best efficiencies can be realized on the cluster. Users interested in MPI generally must compile the program they want to run using an MPI compiler.  Greene supports two common OpenMPI versions, Intel and GCC. These can be loaded as modules:  ","version":"Next","tagName":"h2"},{"title":"Intel's OpenMPI​","type":1,"pageTitle":"Slurm Tutorial","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_tutorial/#intels-openmpi","content":" module load openmpi/intel/4.1.1   ","version":"Next","tagName":"h3"},{"title":"GCC's OpenMPI​","type":1,"pageTitle":"Slurm Tutorial","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_tutorial/#gccs-openmpi","content":" module load openmpi/gcc/4.1.1   Below we will illustrate an example of how to compile a C script for MPI. Copy this into your working directory as hellompi.c :  #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;mpi.h&gt; int main(int argc, char *argv[], char *envp[]) { int numprocs, rank, namelen; char processor_name[MPI_MAX_PROCESSOR_NAME]; MPI_Init(&amp;argc, &amp;argv); MPI_Comm_size(MPI_COMM_WORLD, &amp;numprocs); MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); MPI_Get_processor_name(processor_name, &amp;namelen); printf(&quot;Process %d on %s out of %d\\n&quot;, rank, processor_name, numprocs); MPI_Finalize(); }   Once copied into your directory, load OpenMPI and compile it with the following:  module load openmpi/intel/4.1.1 mpicc hellompi.c -o hellompi   Next, create a hellompi.sbatch script:  #!/bin/bash #SBATCH --nodes=4 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=1:00:00 #SBATCH --mem=2GB #SBATCH --job-name=hellompi #SBATCH --output=hellompi.out # Load the default OpenMPI module. module purge module load openmpi/intel/4.1.1 # Run the hellompi program with mpirun. The -n flag is not required; # mpirun will automatically figure out the best configuration from the # Slurm environment variables. mpirun ./hellompi   Run the job with the following command:  sbatch hellompi.sbatch   After the job runs, cat the hellompi.out output file to see that your processes ran on multiple nodes. There may be some errors, but your output should contain something like the following, indicating the process was run in parallel on multiple nodes:  Process 0 on cs265.nyu.cluster out of 4 Process 1 on cs266.nyu.cluster out of 4 Process 2 on cs267.nyu.cluster out of 4 Process 3 on cs268.nyu.cluster out of 4   ","version":"Next","tagName":"h3"},{"title":"GPU Job Example​","type":1,"pageTitle":"Slurm Tutorial","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_tutorial/#gpu-job-example","content":" To request one GPU card, use SBATCH directives in job script:  #SBATCH --gres=gpu:1   To request a specific card type, use e.g. --gres=gpu:v100:1. The card types currently available are v100 and RTX 8000. As an example, let's submit an Amber job. Amber is a molecular dynamics software package. The recipe is:  mkdir -p /scratch/$USER/myambertest cd /scratch/$USER/myambertest cp /share/apps/Tutorials/slurm/example/amberGPU/* . sbatch run-amber.s   From the tutorial example directory we copy over Amber input data files &quot;inpcrd&quot;, &quot;prmtop&quot; and &quot;mdin&quot;, and the job script file &quot;run-amber.s&quot;.  NOTE: At the time of writing this you may need to update the run-amber.s script to load amber version 20.06, rather than the default 16.06.  The content of the job script &quot;run-amber.s&quot; should be as follows:  #!/bin/bash #SBATCH --job-name=myAmberJobGPU #SBATCH --nodes=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:30:00 #SBATCH --mem=3GB #SBATCH --gres=gpu:1 module purge module load amber/openmpi/intel/20.06 cd /scratch/$USER/myambertest pmemd.cuda -O   The demo Amber job should take ~2 minutes to finish once it starts running. When the job is done, several output files are generated. Check the one named mdout, which has a section most relevant here:  |--------------------- INFORMATION ---------------------- | GPU (CUDA) Version of PMEMD in use: NVIDIA GPU IN USE. | Version 16.0.0 | | 02/25/2016 [......] |------------------- GPU DEVICE INFO -------------------- | | CUDA_VISIBLE_DEVICES: 0 | CUDA Capable Devices Detected: 1 | CUDA Device ID in use: 0 | CUDA Device Name: Tesla V100 | CUDA Device Global Mem Size: 11439 MB | CUDA Device Num Multiprocessors: 13 | CUDA Device Core Freq: 0.82 GHz | |--------------------------------------------------------   ","version":"Next","tagName":"h2"},{"title":"Array Job Example​","type":1,"pageTitle":"Slurm Tutorial","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_tutorial/#array-job-example","content":" Using job array you may submit many similar jobs with almost identical job requirement. This reduces loads on both users and the scheduler system. Job arrays can only be used in batch jobs. Usually the only requirement difference among jobs in a job array is the input file or files. Please follow the recipe below to try the example. There are 5 input files named sample-1.txt, sample-2.txt to sample-5.txt in sequential order. Running one command sbatch run-jobarray.s, you submit 5 jobs to process each of these input files individually. Run the following commands to create the directory and submit the array job:  mkdir -p /scratch/$USER/myjarraytest cd /scratch/$USER/myjarraytest cp /share/apps/Tutorials/slurm/example/jobarray/* . ls   OUTPUT: run-jobarray.s sample-1.txt sample-2.txt sample-3.txt sample-4.txt sample-5.txt wordcount.py  sbatch --array=1-5 run-jobarray.s   The content of the job script run-jobarray.s is copied below:  #!/bin/bash #SBATCH --job-name=myJobarrayTest #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --array=1-5 # this creates an array! #SBATCH --time=5:00 #SBATCH --mem=1GB #SBATCH --output=wordcounts_%A_%a.out #SBATCH --error=wordcounts_%A_%a.err module purge module load python/intel/3.8.6 cd /scratch/$USER/myjarraytest python2 wordcount.py sample-$SLURM_ARRAY_TASK_ID.txt   Job array submissions create an environment variable called SLURM_ARRAY_TASK_ID, which is unique for each job in the array job. It is usually embedded somewhere so that at a job running time it's unique value is incorporated into producing a proper file name. Also as shown above: two additional options %A and %a, denoting the job ID and the task ID (i.e. job array index) respectively, are available for specifying a job's stdout, and stderr file names. ","version":"Next","tagName":"h2"},{"title":"Conda Environments (Python, R)","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/tools_and_software/conda_environments/","content":"","keywords":"","version":"Next"},{"title":"What is Conda?​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/docs/hpc/tools_and_software/conda_environments/#what-is-conda","content":" Package, dependency and environment management for any language—Python, R, Ruby, Lua, Scala, Java, JavaScript, C/ C++, FORTRAN, and more.  Please find more information at this link: https://docs.conda.io/en/latest/  Conda provides a great way to install packages that are already compiled, so you don't need to go over the long compilation process. If a package you need is not available, you can install it (and compile it when needed) using pip (Python) or install.packages (R).  note Reproducibility: One of the ways to ensure the reproducibility of your results is to have an independent conda environment in the directory of each project (one of the options shown below). This will also keep conda environment files away from your /home/$USER directory.  ","version":"Next","tagName":"h2"},{"title":"Advantages/disadvantages of using Conda​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/docs/hpc/tools_and_software/conda_environments/#advantagesdisadvantages-of-using-conda","content":" ","version":"Next","tagName":"h2"},{"title":"Advantages​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/docs/hpc/tools_and_software/conda_environments/#advantages","content":" A lot of pre-compiled packages (fast and easy to install)Note for Python: pip also offers pre-compiled packages (wheels). List can be found here https://pythonwheels.com. However, Conda has a significantly larger number of pre-compiled packages.Compiled packages use highly efficient Intel Math Kernel Library (MKL) library  ","version":"Next","tagName":"h3"},{"title":"Disadvantages​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/docs/hpc/tools_and_software/conda_environments/#disadvantages","content":" Conda does not take advantage of packages already installed in the system (while virtualenv and venv do)As you will see below, you may need to do additional steps to keep track of all installed packages (including those installed by pip and/or install.packages)  ","version":"Next","tagName":"h3"},{"title":"Initializing Conda​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/docs/hpc/tools_and_software/conda_environments/#initializing-conda","content":" Load anaconda module  module purge module load anaconda3/2020.07   Conda init can create problems with package installation, so we suggest using source activate instead of conda activate, even though conda activate is considered a best practice by the Anaconda developers.  ","version":"Next","tagName":"h2"},{"title":"Automatic deletion of your files​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/docs/hpc/tools_and_software/conda_environments/#automatic-deletion-of-your-files","content":" This page describes the installation of packages on /scratch. One has to remember, though, that files stored in the HPC scratch file system are subject to the HPC Scratch old file purging policy: Files on the /scratch file system that have not been accessed for 60 or more days will be purged (read more about Data Management.  Thus you can consider the following options  Reinstall your packages if some of the files get deleted You can do this manuallyYou can do this automatically. For example, within a workflow of a pipeline software like Nextflow Pay for &quot;Research Project Space&quot; - read more hereUse Singularity and install packages within a corresponding overlay file - read more here  ","version":"Next","tagName":"h3"},{"title":"Python​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/docs/hpc/tools_and_software/conda_environments/#python","content":" Load anaconda module  module purge module load anaconda3/2020.07   tip Keep your program/project in /scratch and create conda environment using '-p' parameter. This will keep all the files inside the project's directory, instead of putting in in your /home/$USER  conda create -p ./penv python=3 ## environment will be created in project directory conda activate ./penv   Also, you need to create a symbolic link, so conda will download files for packages to be installed into scratch, not your home directory.  mkdir /home/&lt;NetID&gt;/.conda mkdir /scratch/&lt;NetID&gt;/conda_pkgs ln -s /scratch/&lt;NetID&gt;/conda_pkgs /home/&lt;NetID&gt;/.conda/pkgs   Install pre-compiled packages available in conda  conda install -c anaconda pandas   Other packages may be installed (and compiled when needed) using pip  pip install &lt;package_name&gt;   note Conda and packages install by default to ~/.local/lib/python&lt;version&gt;  If you did use 'pip install --user' to install some packages (without conda or other virtual environment), they will be available in ~/.local/lib/python&lt;version&gt;  warning The primary takeaway: Let say you have tornado v.6 installed in ~/.local/lib/python&lt;version&gt;, and tornado v.5 installed by conda install. When you will do conda activate you will have tornado v.6 available!! Not v.5!! (this behaviour is the same for packages installed by to ~/.local/lib/python&lt;version&gt; before or after you create your conda environment) pip freeze will give v.6 conda list will give v.5 Solution To overcome this, do export PYTHONNOUSERSITE=True after conda activate  ","version":"Next","tagName":"h2"},{"title":"R​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/docs/hpc/tools_and_software/conda_environments/#r","content":" Load anaconda module  module load anaconda3/2020.07   tip Keep your program/project in /scratch and create conda environment using '-p' parameter. This will keep all the files inside the project's directory, instead of putting them in your /home/$USER  conda create -p ./renv r=3.5 ## environment will be created in project directory ## OR conda create -c conda-forge -p ./penv r-base=3.6.3 ## environment will be created in project directory conda activate ./renv   Install pre-compiled packages available in conda:  https://docs.anaconda.com/anaconda/packages/r-language-pkg-docs/  conda install -c r r-dplyr   Other packages may be installed (and compiled) using install.packages()  install.packages(&quot;&lt;package_name&gt;&quot;)   ","version":"Next","tagName":"h2"},{"title":"Reproducibility​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/docs/hpc/tools_and_software/conda_environments/#reproducibility","content":" Packages installed only using conda  Save a list of packages (so you are able to report environment in publication, and to restore/reproduce env on another machine at any time)  # save conda list --export &gt; requirements.txt # restore conda create -p ./penv --file requirements.txt   note This will not list packages installed by pip or install.packages()  If you installed extra packages using pip (Python)  In this you can use  export PYTHONNOUSERSITE=True ## to ingnore packages in ~/.local/lib/python&lt;version&gt; # save conda list --export &gt; conda_requirements.txt pip freeze &gt; pip_requirements.txt # restore conda create -p ./penv --file conda_requirements.txt pip install -r pip_requirements.txt   note Alternatively, you can use conda env export &gt; all_requirements.txt, which will save both: packages installed by conda and by pip.  However, this may fail if your conda environment is created as a sub-directory of your project's directory (which we recommend)  Installed extra packages using install.packages? (R)  Usecase: You need packages not availalbe in conda channels, and want to use install.packages.  Command conda list --export will not include packages installed by &quot;install.packages&quot;. So, do not use conda install at all. To have reproducibility in this case you need to use Conda and renv together, as described below  Conda + pakcrat: specific version of R and install.packages (R)  use conda to install version of R you needdo not use 'conda install' at alluse renvinstall all the packages using install.packagesuse renv as described here to keep track of the environment  In order for conda + renv to work, you need to add following steps:  After you activate conda AND before loading R export R_RENV_DEFAULT_LIBPATHS=&lt;path_to_project_directory&gt;/renv/lib/x86_64-conda_cos6-linux-gnu/&lt;version&gt;/ Start R and execute .libPaths(c(.libPaths(), Sys.getenv(&quot;R_RENV_SYSTEM_LIBRARY&quot;)))   ","version":"Next","tagName":"h2"},{"title":"Use conda env in a batch script​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/docs/hpc/tools_and_software/conda_environments/#use-conda-env-in-a-batch-script","content":" The part of the batch script which will call the command shall look like (replace &lt;path_to_env&gt; to an appropriate value)  ","version":"Next","tagName":"h2"},{"title":"Python​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/docs/hpc/tools_and_software/conda_environments/#python-1","content":" Single node​  #!/bin/bash #SBATCH --job-name=test #SBATCH --nodes=1 #SBATCH --cpus-per-task=1 #SBATCH --ntasks-per-node=4 #SBATCH --mem=8GB #SBATCH --time=1:00:00 module purge; module load anaconda3/2020.07; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; source /share/apps/anaconda3/2020.07/etc/profile.d/conda.sh; conda activate ./penv; export PATH=./penv/bin:$PATH; python python_script.py   Multiple nodes, using MPI​  mpiexec --mca bash -c &quot;module purge; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; module load anaconda3/2020.07; source /share/apps/anaconda3/2020.07/etc/profile.d/conda.sh; conda activate ./penv; export PATH=./penv/bin:$PATH; python python_script.py&quot;   ","version":"Next","tagName":"h3"},{"title":"R (conda packages only)​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/docs/hpc/tools_and_software/conda_environments/#r-conda-packages-only","content":" #!/bin/bash #SBATCH --job-name=test #SBATCH --nodes=1 #SBATCH --cpus-per-task=1 #SBATCH --ntasks-per-node=4 #SBATCH --mem=8GB #SBATCH --time=1:00:00 module purge; module load anaconda3/2020.07; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; source /share/apps/anaconda3/2020.07/etc/profile.d/conda.sh; conda activate ./renv; export PATH=./renv/bin:$PATH; Rscript r_script.R   Multiple nodes, using MPI​  mpiexec --mca bash -c &quot;module purge; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; module load anaconda3/2020.07; source /share/apps/anaconda3/2020.07/etc/profile.d/conda.sh; conda activate ./renv; export PATH=./renv/bin:$PATH; Rscript r_script.R&quot;   ","version":"Next","tagName":"h3"},{"title":"R (conda with renv combination)​","type":1,"pageTitle":"Conda Environments (Python, R)","url":"/rts-docs-dev/docs/hpc/tools_and_software/conda_environments/#r-conda-with-renv-combination","content":" In this case, when you use sbatch you would activate conda in sbatch script, and R script will pickup packages installed in renv  module purge module load anaconda3/2020.07 source /share/apps/anaconda3/2020.07/etc/profile.d/conda.sh conda activate ./renv Rscript test.R  ","version":"Next","tagName":"h3"},{"title":"Python Packages with Virtual Environments","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/tools_and_software/python_packages_with_virtual_environments/","content":"","keywords":"","version":"Next"},{"title":"Create project directory and load Python module​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#create-project-directory-and-load-python-module","content":" ## Find python version you need module avail python ## created directory for your project and cd there mkdir /scratch/$USER/my_project cd /scratch/$USER/my_project ## load python module (different versions available) module load python/intel/3.8.6   ","version":"Next","tagName":"h2"},{"title":"Automatic deletion of your files​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#automatic-deletion-of-your-files","content":" This page describes the installation of packages on /scratch. One has to remember, though, that files stored in the HPC scratch file system are subject to the HPC Scratch old file purging policy: Files on the /scratch file system that have not been accessed for 60 or more days will be purged (read more).  Thus you can consider the following options  Reinstall your packages if some of the files get deleted You can do this manuallyYou can do this automatically. For example, within a workflow of a pipeline software like Nextflow Pay for &quot;Research Project Space&quot; - read more hereUse Singularity and install packages within a corresponding overlay file - read more here  ","version":"Next","tagName":"h2"},{"title":"Create virtual environment​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#create-virtual-environment","content":" It is advisable to create private environment inside the project directory. This boosts reproducibility and does not use space in /home/$USER  ","version":"Next","tagName":"h2"},{"title":"virtualenv​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#virtualenv","content":" virtualenv is a tool to create isolated Python environments  Since Python 3.3, a subset of it has been integrated into the standard library under the venv module.  Note: you may need to install virtualenv first, if it is not yet installed (instructions)  Now create new virtual environment in current directory  EmptyORinherit all packages from those installed on HPC already (and available in PATH after you load python module)  ## created directory for your project and cd there mkdir /scratch/$USER/my_project cd /scratch/$USER/my_project ## Create an EMPTY virtual environment virtualenv venv ## Create an virtual environment that inherits system packages virtualenv venv --system-site-packages   ","version":"Next","tagName":"h3"},{"title":"venv​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#venv","content":" venv is package shipped with Python3. It provides subset of options available in virtualenv tool (link).  python3 -m venv venv   Create new virtual environment in current directory  EmptyORinherit all packages from those installed on HPC already (and available in PATH after you load python module)  ## created directory for your project and cd there mkdir /scratch/$USER/my_project cd /scratch/$USER/my_project ##EMPTY ## (use venv command to create environment called &quot;venv&quot;) python3 -m venv venv ## Inhering all packages python3 -m venv venv --system-site-packages   ","version":"Next","tagName":"h3"},{"title":"Install packages. Keep things reproducible​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#install-packages-keep-things-reproducible","content":" ## activate source venv/bin/activate ## install packages pip install &lt;package you need&gt; ## If package was inherited, but you want to install it in your own env anyway pip install &lt;package you need&gt; --ignore-installed ## export list of packages (to report together with paper and/or to reproduce environment on another computer) pip freeze &gt; requirements.txt ## restore pip install -r requirements.txt   ","version":"Next","tagName":"h2"},{"title":"Close an Activated Virtual Environment​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#close-an-activated-virtual-environment","content":" If you have activated a virtual environment, you can exit it with the following command:  deactivate   ","version":"Next","tagName":"h2"},{"title":"Use with sbatch​","type":1,"pageTitle":"Python Packages with Virtual Environments","url":"/rts-docs-dev/docs/hpc/tools_and_software/python_packages_with_virtual_environments/#use-with-sbatch","content":" When you use this env in sbatch script, please use  module purge; source venv/bin/activate; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; python python_script.py   If you use mpi  mpiexec bash -c &quot;module purge; source venv/bin/activate; export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK; python python_script.py&quot;  ","version":"Next","tagName":"h2"},{"title":"Slurm - Submitting Jobs","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/","content":"","keywords":"","version":"Next"},{"title":"Batch vs Interactive Jobs​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#batch-vs-interactive-jobs","content":" HPC workloads are usually better suited to batch processing than interactive working.A batch job is sent to the system when submitted with an sbatch command.The working pattern we are all familiar with is interactive - where we type ( or click ) something interactively, and the computer performs the associated action. Then we type ( or click ) the next thing.Comments at the start of the script, which match a special pattern ( #SBATCH ) are read as Slurm options.  ","version":"Next","tagName":"h2"},{"title":"The trouble with interactive environments​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#the-trouble-with-interactive-environments","content":" There is a reason why GUIs are less common in HPC environments: point-and-click is necessarily interactive. In HPC environments (as we'll see in session 3) work is scheduled in order to allow exclusive use of the shared resources. On a busy system there may be several hours wait between when you submit a job and when the resources become available, so a reliance on user interaction is not viable. In Unix, commands need not be run interactively at the prompt, you can write a sequence of commands into a file to be run as a script, either manually (for sequences you find yourself repeating frequently) or by another program such as the batch system.  The job might not start immediately, and might take hours or days, so we prefer a batch approach:  Plan the sequence of commands which will perform the actions we need Write them into a script.  I can now run the script interactively, which is a great way to save effort if i frequently use the same workflow, or ...  Submit the script to a batch system, to run on dedicated resources when they become available.  ","version":"Next","tagName":"h3"},{"title":"Where does the output go ?​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#where-does-the-output-go-","content":" The batch system writes stdout and stderr from a job to a file named for example &quot;slurm-12345.out&quot; You can change either stdout or stderr using sbatch options. While a job is running, it caches the stdout an stderr in the job working directory.You can use redirection to send output of a specific command into a file.  ","version":"Next","tagName":"h3"},{"title":"Writing and Submitting a Job​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#writing-and-submitting-a-job","content":" There are two aspects to a batch job script:  A set of SBATCH directives describing the resources required and other information about the job.The script itself, comprised of commands to set up and perform the computations without additional user interaction.  ","version":"Next","tagName":"h3"},{"title":"A simple example​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#a-simple-example","content":" A typical batch script on an NYU HPC cluster looks something like these:  #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=5:00:00 #SBATCH --mem=2GB #SBATCH --job-name=myTest #SBATCH --mail-type=END #SBATCH --mail-user=bob.smith@nyu.edu #SBATCH --output=slurm_%j.out #SBATCH --error=slurm_%j.err module purge module load stata/17.0 RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*} mkdir -p $RUNDIR DATADIR=$SCRATCH/my_project/data cd $RUNDIR stata -b do $DATADIR/data_0706.do #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --time=5:00:00 #SBATCH --mem=2GB #SBATCH --job-name=myTest #SBATCH --mail-type=END #SBATCH --mail-user=bob.smith@nyu.edu #SBATCH --output=slurm_%j.out #SBATCH --error=slurm_%j.err module purge SRCDIR=$HOME/my_project/code RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*} mkdir -p $RUNDIR cd $SLURM_SUBMIT_DIR cp my_input_params.inp $RUNDIR cd $RUNDIR module load fftw/intel/3.3.9 $SRCDIR/my_exec.exe &lt; my_input_params.inp   We'll work through them more closely in a moment. You submit the job with sbatch:  sbatch myscript.sh   And monitor it's progress with:  squeue -u $USER   What just happened ? Here's an annotated version of the first script:  #!/bin/bash # This line tells the shell how to execute this script, and is unrelated # to SLURM. # at the beginning of the script, lines beginning with &quot;#SBATCH&quot; are read by # SLURM and used to set queueing options. You can comment out a SBATCH # directive with a second leading #, eg: ##SBATCH --nodes=1 # we need 1 node, will launch a maximum of one task and use one cpu for the task: #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=1 # we expect the job to finish within 5 hours. If it takes longer than 5 # hours, SLURM can kill it: #SBATCH --time=5:00:00 # we expect the job to use no more than 2GB of memory: #SBATCH --mem=2GB # we want the job to be named &quot;myTest&quot; rather than something generated # from the script name. This will affect the name of the job as reported # by squeue: #SBATCH --job-name=myTest # when the job ends, send me an email at this email address. #SBATCH --mail-type=END #SBATCH --mail-user=bob.smith@nyu.edu # both standard output and standard error are directed to the same file. # It will be placed in the directory I submitted the job from and will # have a name like slurm_12345.out #SBATCH --output=slurm_%j.out # once the first non-comment, non-SBATCH-directive line is encountered, SLURM # stops looking for SBATCH directives. The remainder of the script is executed # as a normal Unix shell script # first we ensure a clean running environment: module purge # and load the module for the software we are using: module load stata/17.0 # next we create a unique directory to run this job in. We will record its # name in the shell variable &quot;RUNDIR&quot;, for better readability. # SLURM sets SLURM_JOB_ID to the job id, ${SLURM_JOB_ID/.*} expands to the job # id up to the first '.' We make the run directory in our area under $SCRATCH, because at NYU HPC # $SCRATCH is configured for the disk space and speed required by HPC jobs. RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*} mkdir $RUNDIR # we will be reading data in from somewhere, so define that too: DATADIR=$SCRATCH/my_project/data # the script will have started running in $HOME, so we need to move into the # unique directory we just created cd $RUNDIR # now start the Stata job: stata -b do $DATADIR/data_0706.do The second script has the same SBATCH directives, but this time we are using code we compiled ourselves. Starting after the SBATCH directives: # first we ensure a clean running environment: module purge # and ensure we can find the executable: SRCDIR=$HOME/my_project/code # create a unique directory to run this job in, as per the script above RUNDIR=$SCRATCH/my_project/run-${SLURM_JOB_ID/.*} mkdir $RUNDIR # By default the script will have started running in the directory we ran sbatch from. # Let's assume our input file is in the same directory in this example. SLURM # sets some environment variables with information about the job, including # SLURM_SUBMIT_DIR which is the directory the job was submitted from. So lets # go there and copy the input file to the run directory on /scratch: cd $SLURM_SUBMIT_DIR cp my_input_params.inp $RUNDIR # go to the run directory to begin the run: cd $RUNDIR # load whatever environment modules the executable needs: module load fftw/intel/3.3.9 # run the executable (sending the contents of my_input_params.inp to stdin) $SRCDIR/my_exec.exe &lt; my_input_params.inp   ","version":"Next","tagName":"h3"},{"title":"Batch Jobs​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#batch-jobs","content":" Jobs are submitted with the sbatch command:  sbatch options job-script   The options tell SLURM information about the job, such as what resources will be needed. These can be specified in the job-script as SBATCH directives, or on the command line as options, or both ( in which case the command line options take precedence should the two contradict each other ). For each option there is a corresponding SBATCH directive with the syntax:  #SBATCH option   For example, you can specify that a job needs 2 nodes and 4 cores on each node ( by default one CPU core per task ) on each node by adding to the script the directive:  #!/bin/bash #SBATCH --nodes=2 #SBATCH --ntasks-per-node=4   or as a command-line option to sbatch when you submit the job:  sbatch --nodes=2 --ntasks-per-node=4 my_script.sh   ","version":"Next","tagName":"h2"},{"title":"Options to manage job output​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#options-to-manage-job-output","content":" -J jobname Give the job a name. The default is the filename of the job script. Within the job, $SLURM_JOB_NAME expands to the job name. -o path/for/stdout Send stdout to path/for/stdout. The default filename is slurm-${SLURM_JOB_ID}.out, e.g. slurm-12345.out, in the directory from which the job was submitted. -e path/for/stderr Send stderr to path/for/stderr. --mail-user=my_email_address@nyu.edu Send mail to my_email_address@nyu.edu when certain events occur. --mail-type=type Valid type values are NONE, BEGIN, END, FAIL, REQUIRE, ALL.  ","version":"Next","tagName":"h3"},{"title":"Options to set the job environment:​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#options-to-set-the-job-environment","content":" --export=VAR1,VAR2=&quot;some value&quot;,VAR3 Pass variables to the job, either with a specific value (the VAR= form) or from the submitting environment ( without &quot;=&quot; ) --get-user-env[=timeout][mode] Run something like &quot;su - &lt;username&gt; -c /usr/bin/env&quot; and parse the output. Default timeout is 8 seconds. The mode value can be &quot;S&quot;, or &quot;L&quot; in which case &quot;su&quot; is executed with &quot;-&quot; option.  ","version":"Next","tagName":"h3"},{"title":"Options to request compute resources​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#options-to-request-compute-resources","content":" -t, --time=time Set a limit on the total run time. Acceptable formats include &quot;minutes&quot;, &quot;minutes:seconds&quot;, &quot;hours:minutes:seconds&quot;, &quot;days-hours&quot;, &quot;days-hours:minutes&quot; and &quot;days-hours:minutes:seconds&quot;. --mem=MB Maximum memory per node the job will need in MegaBytes --mem-per-cpu=MB Memory required per allocated CPU in MegaBytes -N, --node=num Number of nodes are required. Default is 1 node.-n, --ntasks=numMaximum number tasks will be launched. Default is one task per node.--ntasks-per-node=ntasksRequest that ntasks be invoked on each node.-c, --cpus-per-task=ncpusRequire ncpus number of CPU cores per task. Without this option, allocate one core per task. Requesting the resources you need, as accurately as possible, allows your job to be started at the earliest opportunity as well as helping the system to schedule work efficiently to everyone's benefit.  ","version":"Next","tagName":"h3"},{"title":"Options for running interactively on the compute nodes with srun​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#options-for-running-interactively-on-the-compute-nodes-with-srun","content":" -nnum Specify the number of tasks to run, eg. -n4. Default is one CPU core per task. Don't just submit the job, but also wait for it to start and connect stdout, stderrand stdin to the current terminal. -ttime Request job running duration, eg. -t1:30:00 --mem=MB Specify the real memory required per node in MegaBytes, eg. --mem=4000--ptyExecute the first task in pseudo terminal mode, eg. --pty /bin/bash, to start a bash command shell --x11 Enable X forwarding, so programs using a GUI can be used during the session (provided you have X forwarding to your workstation set up)To leave an interactive batch session, type exit at the command prompt  ","version":"Next","tagName":"h3"},{"title":"Options for delaying starting a job​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#options-for-delaying-starting-a-job","content":" --begin=time Delay starting this job until after the specified date and time, eg. --begin=9:42:00, to start the job at 9:42:00 am -d, --dependency=dependency_list (More info here https://slurm.schedmd.com/sbatch.html)Example 1 --dependency=afterok:12345, to delay starting this job until the job 12345 has completed successfully Example 2 Let us say job 1 uses sbatch file job1.sh, and job 2 uses job2.shInside the batch file of the second job (job2.sh) add#SBATCH --dependency=afterok:$job1Start the first job and get id of the jobjob1=$(echo $(sbatch job1.sh) | grep -Eo &quot;[0-9]+&quot;)Schedule second jobs to start when the first one endssbatch job2.sh  ","version":"Next","tagName":"h3"},{"title":"Options for running many similar jobs​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#options-for-running-many-similar-jobs","content":" -a, --array=indexes Submit an array of jobs with array ids as specified. Array ids can be specified as a numerical range, a comma-seperated list of numbers, or as some combination of the two. Each job instance will have an environment variable SLURM_ARRAY_JOB_ID and SLURM_ARRAY_TASK_ID. For example:--array=1-11, to start an array job with index from 1 to 11--array=1-7:2, to submit an array job with index step size 2--array=1-9%4, to submit an array job with simultaneously running job elements set to 4The srun command is similar to pbsdsh. It launches tasks on allocated resources  ","version":"Next","tagName":"h3"},{"title":"R Job Example​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#r-job-example","content":" Create a directory and an example R script  mkdir /scratch/$USER/examples cd /scratch/$USER/examples   Create example.R inside the examples directory:  df &lt;- data.frame(x=c(1,2,3,1), y=c(7,19,2,2)) df indices &lt;- order(df$x) order(df$x) df[indices,] df[rev(order(df$y)),]   Create the following SBATCH script named run-R.sbatch :  #!/bin/bash # #SBATCH --job-name=RTest #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --mem=2GB #SBATCH --time=01:00:00 module purge module load r/intel/4.0.4 cd /scratch/$USER/examples R --no-save -q -f example.R &gt; example.out 2&gt;&amp;1   Run the job using sbatch.  sbatch run-R.sbatch   ","version":"Next","tagName":"h2"},{"title":"Array Jobs​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#array-jobs","content":" Using job array you may submit many similar jobs with almost identical job requirement. This reduces loads on both shoulders of users and the scheduler system. Job array can only be used in batch jobs. Usually the only requirement difference among jobs in a job array is the input file or files.  Please follow the recipe below to try the example. There are 5 input files named sample-1.txt, sample-2.txt to sample-5.txt in sequential order. Running one command sbatch --array=1-5 run-jobarray.s, you submit 5 jobs to process each of these input files individually.  Prepare the data before submitting an array job:  mkdir -p /scratch/$USER/myjarraytest cd /scratch/$USER/myjarraytest cp /share/apps/Tutorials/slurm/example/jobarray/* . ls   Submit the array job:  sbatch --array=1-5 run-jobarray.s   The content of the job script run-jobarray.s is copied below:  #!/bin/bash #SBATCH --job-name=myJobarrayTest #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --time=5:00 #SBATCH --mem=1GB #SBATCH --output=wordcounts_%A_%a.out #SBATCH --error=wordcounts_%A_%a.err module purge module load python/intel/3.8.6 cd /scratch/$USER/myjarraytest python wordcount.py sample-$SLURM_ARRAY_TASK_ID.txt   Job array submission introduces an environment variable.SLURM_ARRAY_TASK_ID, which is unique for each job array job. It is usually embedded somewhere so that at a job running time it's unique value is incorporated into producing a proper file name.  Also as shown above: two additional options %A and %a, denoting the job ID and the task ID ( i.e. job array index ) respectively, are available for specifying a job's stdout, and stderr file names.  ","version":"Next","tagName":"h2"},{"title":"More examples​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#more-examples","content":" You can find more examples here:  /scratch/work/public/examples/slurm/jobarry/   ","version":"Next","tagName":"h2"},{"title":"GPU Jobs​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#gpu-jobs","content":" To request one GPU card, use SBATCH directive in job script:  #SBATCH --gres=gpu:1   To request a specific card type, use eg. --gres=gpu:v100:1. As an example, let's submit an Amber job. Amber is a molecular dynamics software package. The recipe is:  mkdir -p /scratch/$USER/myambertest cd /scratch/$USER/myambertest cp /share/apps/Tutorials/slurm/example/amberGPU/* . sbatch run-amber.s   There are three NVIDIA GPU types and one AMD GPU type that can be used.  CAUTION: AMD GPUs require code to be compatible with ROCM drivers, not CUDA  To request NVIDIA GPUs  RTX8000  #SBATCH --gres=gpu:rtx8000:1   V100  #SBATCH --gres=gpu:v100:1   A100  #SBATCH --gres=gpu:a100:1   To request AMD GPUs  MI50  #SBATCH --gres=gpu:mi50:1   From the tutorial example directory we copy over Amber input data files &quot;inpcrd&quot;, &quot;prmtop&quot; and &quot;mdin&quot;, and the job script file &quot;run-amber.s&quot;. The content of the job script &quot;run-amber.s&quot; is:  #!/bin/bash # #SBATCH --job-name=myAmberJobGPU #SBATCH --nodes=1 #SBATCH --cpus-per-task=1 #SBATCH --time=00:30:00 #SBATCH --mem=3GB #SBATCH --gres=gpu:1 module purge module load amber/openmpi/intel/20.06 cd /scratch/$USER/myambertest pmemd.cuda -O   The demo Amber job should take ~2 minutes to finish once it starts runnning. When the job is done, several output files are generated. Check the one named &quot;mdout&quot;, which has a section most relevant here:  |--------------------- INFORMATION ---------------------- | GPU (CUDA) Version of PMEMD in use: NVIDIA GPU IN USE. | Version 16.0.0 | | 02/25/2016 [......] |------------------- GPU DEVICE INFO -------------------- | | CUDA_VISIBLE_DEVICES: 0 | CUDA Capable Devices Detected: 1 | CUDA Device ID in use: 0 | CUDA Device Name: Tesla K80 | CUDA Device Global Mem Size: 11439 MB | CUDA Device Num Multiprocessors: 13 | CUDA Device Core Freq: 0.82 GHz | |--------------------------------------------------------   ","version":"Next","tagName":"h2"},{"title":"Interactive Jobs​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#interactive-jobs","content":" ","version":"Next","tagName":"h2"},{"title":"Bash Sessions​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#bash-sessions","content":" The majority of the jobs on the NYU HPC cluster are submitted with the sbatch command, and executed in the background. These jobs' steps and workflows are predefined by users, and their executions are driven by the scheduler system.  There are cases when users need to run applications interactively ( interactive jobs ). Interactive jobs allow the users to enter commands and data on the command line (or in a graphical interface ), providing an experience similar to working on a desktop or laptop. Examples of common interactive tasks are:  Editing files Compiling and debugging code Exploring data, to insights A graphical window to run visualization etc  To support interactive use in a batch environment, Slurm allows for interactive batch jobs.  ","version":"Next","tagName":"h3"},{"title":"Can you run interactive jobs on the HPC Login nodes?​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#can-you-run-interactive-jobs-on-the-hpc-login-nodes","content":" Since the login nodes of the HPC cluster are shared between many users, running interactive jobs that require significant computing and IO resources on the login nodes will impact many users.  Thus running compute and IO intensive interactive jobs on the HPC login nodes is not allowed.  Such jobs may be removed without notice!  Instead of running interactive jobs on Login nodes, users can run interactive jobs on the HPC Compute nodes using SLURM's srun utility. Running interactive jobs on compute nodes does not impact many users and in addition provides access to resources that are not available on the login nodes, such as interactive access to GPUs, high memory, exclusive access to all the resources of a compute node, etc. Note: There is no partition on the HPC cluster that has been reserved for Interactive jobs.  ","version":"Next","tagName":"h3"},{"title":"Start an Interactive Job​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#start-an-interactive-job","content":" When you start an interactive batch job the command prompt is not immediately returned. Instead, you wait until the resource is available when the prompt is returned and you are on a compute node and in a batch job - much like the process of logging in to a host with ssh. To end the session, type 'exit', again just like the process of logging in and out with ssh.  [wd35@log-0 ~]$ srun --pty /bin/bash [wd35@c17-01 ~]$ hostname c17-01   To use any GUI-based program within the interactive batch session you will need to extend X forwarding with the --x11 option. This of course still relies on you having X forwarding at your login session - try running 'xterm' before starting the interactive to verify that this is working correctly.  ","version":"Next","tagName":"h3"},{"title":"Request Resources​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#request-resources","content":" You can request resources for an interactive batch session just as you would for any other job, for example to request 4 processors with 4GB memory for 2 hours.  If you do not request resources you will get the default settings. If after some directory navigation in your interactive session, you can jump back to the directory you submitted from with:  cd $SLURM_SUBMIT_DIR   ","version":"Next","tagName":"h3"},{"title":"Interactive Job Options​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#interactive-job-options","content":" (Don't just submit the job, but also wait for it to start and connect stdout, stderr and stdin to the current terminal)  -nnum Specify the number of tasks to run, eg. -n4. Default is one CPU core per task -ttime Request job running duration, eg. -t1:30:00 --mem=MB Specify the real memory required per node in MegaBytes, eg. --mem=4000--ptyExecute the first task in pseudo terminal mode, eg. --pty /bin/bash, to start a bash command shell --gres=gpu:N To request N number of GPUs --x11 Enable X forwarding, so programs using a GUI can be used during the session (provided you have X forwarding to your workstation set up)To leave an interactive batch session, type exit at the command prompt  Certain tasks need user iteraction - such as debugging and some GUI-based applications. However the HPC clusters rely on batch job scheduling to efficiently allocate resources. Interactive batch jobs allow these apparently conflicting requirements to be met.  ","version":"Next","tagName":"h3"},{"title":"Interactive Bash Job Examples​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#interactive-bash-job-examples","content":" Example (Without x11 forwarding)  Through srun SLURM provides rich command line options for users to request resources from the cluster, to allow interactive jobs. Please see some examples and short accompanying explanations in the code block below, which should cover many of the use cases.  In the srun examples below, through --pty /bin/bash we request to start bash command shell session in pseudo terminal by default the resource allocated is single CPU core and 2GB memory for 1 hour:  srun --pty /bin/bash   To request 4 CPU cores, 4 GB memory, and 2 hour running duration:  srun -c4 -t2:00:00 --mem=4000 --pty /bin/bash   To request one GPU card, 3 GB memory, and 1.5 hour running duration:  srun -t1:30:00 --mem=3000 --gres=gpu:1 --pty /bin/bash   Example (x11 forwarding)  In srun there is an option &quot;–x11&quot;, which enables X forwarding, so programs using a GUI can be used during an interactive session (provided you have X forwarding to your workstation set up).  To request computing resources, and export x11 display on allocated node(s)  srun --x11 -c4 -t2:00:00 --mem=4000 --pty /bin/bash xterm # check if xterm popping up okay   To request GPU card etc, and export x11 display:  srun --x11 -t1:30:00 --mem=3000 --gres=gpu:1 --pty /bin/bash   ","version":"Next","tagName":"h3"},{"title":"R interactive job​","type":1,"pageTitle":"Slurm - Submitting Jobs","url":"/rts-docs-dev/docs/hpc/submitting_jobs/slurm_submitting_jobs/#r-interactive-job","content":" The following example shows how to work with Interactive R session on a compute node:  [NetID@log-1 ~]$ srun -c 1 --pty /bin/bash [NetID@c17-01 ~]$ module purge [NetID@c17-01 ~]$ module list No modules loaded [NetID@c17-01 ~]$ module load r/intel/4.0.4 [NetID@c17-01 ~]$ module list Currently Loaded Modules: 1) intel/19.1.2 2) r/intel/4.0.4 [NetID@c17-01 ~]$ R R version 4.0.4 (2021-02-15) -- &quot;Lost Library Book&quot; Copyright (C) 2021 The R Foundation for Statistical Computing Platform: x86_64-centos-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. &gt; 5 + 10 [1] 15 &gt; 6 ** 2 [1] 36 &gt; tan(45) [1] 1.619775 &gt; &gt; q() Save workspace image? [y/n/c]: n [NetID@c17-01 ~]$ exit exit [NetID@log-1 ~]$  ","version":"Next","tagName":"h3"},{"title":"R Packages with renv","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/tools_and_software/r_packages_with_renv/","content":"","keywords":"","version":"Next"},{"title":"Setup​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/docs/hpc/tools_and_software/r_packages_with_renv/#setup","content":" Say your R code is in directory /scratch/$USER/projects/project1  cd /scratch/$USER/projects/project1 module purge module load r/gcc/4.4.0 R   ","version":"Next","tagName":"h2"},{"title":"Automatic deletion of your files​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/docs/hpc/tools_and_software/r_packages_with_renv/#automatic-deletion-of-your-files","content":" This page describes the installation of packages on /scratch. One has to remember, though, that files stored in the HPC scratch file system are subject to the HPC Scratch old file purging policy: Files on the /scratch file system that have not been accessed for 60 or more days will be purged (read more).  Thus you can consider the following options:  Reinstall your packages if some of the files get deleted You can do this manuallyYou can do this automatically. For example, within a workflow of a pipeline software like Nextflow Pay for &quot;Research Project Space&quot; - read more hereUse Singularity and install packages within a corresponding overlay file - read more here  ","version":"Next","tagName":"h3"},{"title":"Cache directory setup​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/docs/hpc/tools_and_software/r_packages_with_renv/#cache-directory-setup","content":" By default, renv will cache package installation files to your home directory (most likely either in ~/.local/share/renv or ~/.cache/R/renv/ or something similar).  To avoid filling up your home directory, we advise to set up path to alternative cache directory (otherwise your home directory may fill up quickly)  Create directory  mkdir -p /scratch/$USER/.cache/R/renv   Create a file in you project directory named .Renviron and put the following in in the file. It is the R project directory (/scratch/$USER/projects/project1) in this example.  RENV_PATHS_ROOT=/scratch/&lt;USER_NETID&gt;/.cache/R/renv   ","version":"Next","tagName":"h3"},{"title":"Init renv​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/docs/hpc/tools_and_software/r_packages_with_renv/#init-renv","content":" The renv package is already installed for module r/gcc/4.4.0. You need to install it yourself if you use other R module version  ## Do this if renv is not available (already installed for r/gcc/4.4.0) # install.packages(&quot;renv&quot;) ## By default this will install renv package into a sub-directory within your home directory ## init renv in project's directory renv::init(&quot;.&quot;)   Restart R for renv to take effect. Once you start R, your renv environment will be loaded automatically.  R version 4.4.0 (2024-04-24) -- &quot;Puppy Cup&quot; ... * Project '/scratch/$USER/projects/project1' loaded. [renv 1.0.7]   ","version":"Next","tagName":"h3"},{"title":"Check​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/docs/hpc/tools_and_software/r_packages_with_renv/#check","content":" You can check your library paths with the .libPaths() command  &gt; .libPaths() [1] &quot;/scratch/$USER/projects/project1/renv/library/R-4.1/x86_64-pc-linux-gnu&quot;   You can check where the cache is set with the following:  renv::paths$cache() #[1] &quot;/home/$USER/.cache/R/renv/cache/v5/R-4.1/x86_64-pc-linux-gnu&quot;   ","version":"Next","tagName":"h3"},{"title":"Add/remove, etc. packages​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/docs/hpc/tools_and_software/r_packages_with_renv/#addremove-etc-packages","content":" Install a package, such as reshape2. Below we can see it is not yet installed and then install it.  R library(reshape2) Error in library(reshape2) : there is no package called ‘reshape2’ install.packages(&quot;reshape2&quot;)   note You must be in the project1 directory for renv to load your project and the appropriate personal environment that you have created. If you want to copy your environment to a new location, use the bundle package, as shown below.  Test R file  print(&quot;hello&quot;) renv::restore() library(reshape2) names(airquality) &lt;- tolower(names(airquality)) head(airquality) aql &lt;- melt(airquality) print(&quot;hello again&quot;)   For testing run it as  srun --pty /bin/bash Rscript test.R   note Your .Rprofile file will include line source(&quot;renv/activate.R&quot;)  The file will output the following:  [1] &quot;hello&quot; * The library is already synchronized with the lockfile. ozone solar.r wind temp month day 1 41 190 7.4 67 5 1 2 36 118 8.0 72 5 2 3 12 149 12.6 74 5 3 4 18 313 11.5 62 5 4 5 NA NA 14.3 56 5 5 6 28 NA 14.9 66 5 6 No id variables; using all as measure variables [1] &quot;hello again&quot;   ","version":"Next","tagName":"h3"},{"title":"Clean up​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/docs/hpc/tools_and_software/r_packages_with_renv/#clean-up","content":" Keep only the packages that you use in this particular project (not all the packages available on the system)  R # launch R renv::clean() # remove packages not recorded in the lockfile from the target library   ","version":"Next","tagName":"h3"},{"title":"Recommended Workflow​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/docs/hpc/tools_and_software/r_packages_with_renv/#recommended-workflow","content":" The general workflow when working with renv is:  Call renv::init() to initialize a new project-local environment with a private R library,Work in the project as normal, installing and removing new R packages as they are needed in the project,Call renv::snapshot() to save the state of the project library to the lockfile (called renv.lock), By default, renv::snapshot() will only capture packages listed in your R scripts within the R Project. For more options read the renv::snapshot() documentation. Continue working on your project, installing and updating R packages as needed.If needed, call renv::restore() to revert to the previous state as encoded in the lockfile if your attempts to update packages introduced some new problems.  The renv::init() function attempts to ensure the newly-created project library includes all R packages currently used by the project. It does this by crawling R files within the project for dependencies with the renv::dependencies() function. The discovered packages are then installed into the project library with the renv::hydrate() function, which will also attempt to save time by copying packages from your user library (rather than reinstalling from CRAN) as appropriate.  Calling renv::init() will also write out the infrastructure necessary to automatically load and use the private library for new R sessions launched from the project root directory. This is accomplished by creating (or amending) a project-local .Rprofile with the necessary code to load the project when the R session is started.  If you’d like to initialize a project without attempting dependency discovery and installation – that is, you’d prefer to manually install the packages your project requires on your own – you can use renv::init(bare = TRUE) to initialize a project with an empty project library.  ","version":"Next","tagName":"h3"},{"title":"Use with sbatch​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/docs/hpc/tools_and_software/r_packages_with_renv/#use-with-sbatch","content":" When you launch a job with sbatch, R will check if there is renv directory, and if renv is on it will pick up packages, installed using renv in the current directory.  Before you launch sbatch job, you need to make sure your project renv environment is ready, as outlined in the previous section.  ","version":"Next","tagName":"h2"},{"title":"Store and Share your R Project's R version and R Package Versions​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/docs/hpc/tools_and_software/r_packages_with_renv/#store-and-share-your-r-projects-r-version-and-r-package-versions","content":" Reproduce Environment​  If you already have file renv.lock or bundle file skip step 1  In the original location (your own laptop for example) go to project directory and execute (Make sure the whole path to project directory and names of your script files don't have empty spaces!)  R # install.packages(&quot;renv&quot;) ## if needed renv::init() renv::snapshot()   Take file renv.lock and copy it to a new location for the projectAt the new location - restore environment: go to directory of the project and execute. (Make sure version of R is the same)  ## Reproduce environment module purge module load r/gcc/4.4.0 R renv::restore() renv::init()   renv will install/compile what is needed on any system (Linux, Windows, etc). You can share your code with other researchers no matter what system they use. However, you should be careful that the same version of R is used between systems.  What to save/publish/commit with Git​  In order to have your work reproducible by you or/and others, save and/or commit your code in git, please including  renv.lock (which lists all packages and versions that you use including the version of R)  Migrating from Packrat​  The renv package has replaced the now deprecated Packrat package. The renv::migrate() function makes it possible to migrate projects from Packrat to renv. See the ?migrate documentation for more details. In essence, calling renv::migrate(&quot;&lt;project path&gt;&quot;) will be enough to migrate the Packrat library and lockfile such that they can then be used by renv.  ","version":"Next","tagName":"h3"},{"title":"Useful links​","type":1,"pageTitle":"R Packages with renv","url":"/rts-docs-dev/docs/hpc/tools_and_software/r_packages_with_renv/#useful-links","content":" https://rstudio.github.io/renv/articles/renv.html ","version":"Next","tagName":"h3"},{"title":"Singularity: Run Custom Applications with Containers","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/tools_and_software/singularity_run_custom_applications_with_containers/","content":"","keywords":"","version":"Next"},{"title":"What is Singularity​","type":1,"pageTitle":"Singularity: Run Custom Applications with Containers","url":"/rts-docs-dev/docs/hpc/tools_and_software/singularity_run_custom_applications_with_containers/#what-is-singularity","content":" Singularity is a container based Linux kernel workspace that works just like docker. You can run pre-built programs in containers without having to worry about the pre-install environment.  For users who are familiar with Docker containers, Singularity works very similarly, and can even run Docker containers.  For a detailed introduction on Singularity, visit their official site here  ","version":"Next","tagName":"h2"},{"title":"why do we use Singularity​","type":1,"pageTitle":"Singularity: Run Custom Applications with Containers","url":"/rts-docs-dev/docs/hpc/tools_and_software/singularity_run_custom_applications_with_containers/#why-do-we-use-singularity","content":" There are multiple reasons to use Singularity on the HPC clusters:  Security: Singularity provides a layer of security as it does not require any root access on our clusters. This makes it safer against malware and bad scripts that might jeopardize the outer system. Thus we only support Singularity on our clusters(there are not other options such as Kubernetes or Docker on our clusters right now)Containerization: Singularity will run all your images(packaged and pre-built programs) inside of its containers, each container works like a small vm. They contain all the required environment and files of a single Linux kernel and you don't have to worry about any pre-installation nonsenseInter-connectivity: Containers are able to talk to each other, as well as the home system, so while each container has its own small space, they are still a part of a big interconnected structure. Thus enabling you to connect your programs.Accessibility: Probably the most important feature of all, Singularity allows you to run your program in 2 to 3 simple steps, as shown in the topic how to run a singularity container.  ","version":"Next","tagName":"h2"},{"title":"how to run a singularity container​","type":1,"pageTitle":"Singularity: Run Custom Applications with Containers","url":"/rts-docs-dev/docs/hpc/tools_and_software/singularity_run_custom_applications_with_containers/#how-to-run-a-singularity-container","content":" There are 3 steps to run a Singularity container on our clusters:  pulling a image from Singularity hub or Docker hub  $ singularity pull &lt;image name&gt; # image name can be for example shub://vsoch/hello-world or docker://godlovedc/lolcow     build the image  $ singularity build &lt;a name of your choosing&gt;.simg:rw &lt;image name&gt; # the image name can be a local image or an image from a hub   We add the :rw tag at the end of the .simg to explicitly give it &quot;read and write&quot; permissions while building.    You can now run your container using the built image:  run container  # this is one way of running a container $ singularity run &lt;image name&gt;.simg:ro # this is another way to run a container $ ./&lt;image name&gt;.simg:ro   Unlike in the build phase, we add the :ro tag which means &quot;read only&quot; - as we are now just executing the image, not building it, and thus do not need it to be written. Writing access causes the Singularity image to be locked and it can become inaccessible while it is in read/write mode, so read only mode is best for executing commands.  running this would yield a menu for output:    go into container  singularity shell &lt;image name&gt;.simg:ro # after this step, you will be going into the container and start your programming     you can run commands for the container using exec arguments without actually going into the container  $ singularity exec &lt;image name&gt;.simg:ro &lt;commands&gt; # adding commands to the back will return the display result of these commands in the container without actually going into the container   Example:    That's it! Now you're good to go and can just use these simple steps to run singularity images and run your programs  For full information and documentation on Singularity, visit their site here  ","version":"Next","tagName":"h2"},{"title":"How to Create a Singularity Container​","type":1,"pageTitle":"Singularity: Run Custom Applications with Containers","url":"/rts-docs-dev/docs/hpc/tools_and_software/singularity_run_custom_applications_with_containers/#how-to-create-a-singularity-container","content":" So what if you want to create an image from your container and save it for a rainy day?  The instructions are here for your convenience, read through them to create your own Singularity container and package it into an image!  For those that know how docker containers are built, you can build docker containers using the information here and upload them onto docker hub and pulling them using Singularity. Singularity supports all docker images!  ","version":"Next","tagName":"h2"},{"title":"Singularity vs Docker​","type":1,"pageTitle":"Singularity: Run Custom Applications with Containers","url":"/rts-docs-dev/docs/hpc/tools_and_software/singularity_run_custom_applications_with_containers/#singularity-vs-docker","content":" Why are there so many mentions of Docker? The reason is that Singularity is essentially the same as Docker and you don't need to relearn Singularity if you already have experience with Docker. Now let's get into some pros and cons between the two programs.  Docker is more accepted commercially than Singularity. You can download and run Docker on your own computer with any operating system and build containers with ease while Singularity is used in a more academic setting. Singularity only supports Linux operating systems and cannot run on a windows linux kernel(your windows ubuntu), so it is much more limited.However, Docker requires root or admin access for the operating system it deploys on, and our clusters do not offer that access to any software that requires this criteria. Thus Docker is not available on the clusters and Singularity is.A silver lining in all of this is that Singularity fully supports docker images and you can do everything in docker and push your image to docker hub and pull them on the clusters. Thus making sure that you don't need to relearn Singularity all over again and can just use it through the simplest of commands in this wiki.  Good luck with Singularity, and have fun! ","version":"Next","tagName":"h2"},{"title":"Support","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/training_and_support/support/","content":"Support Some of your questions may be already answered here Introductory HPC Video PlaylistCluster Getting Started DocumentationConsider to sign up for Training and Workshops NYU HPC offers personalized help through personal consultations for simple and advanced cases: Do you have trouble with something that seems trivial?Would you like to discuss how to better apply Deep Learning to your case?Something else?Contact us directly at hpc@nyu.edu","keywords":"","version":"Next"},{"title":"HPC training at NYU","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/training_and_support/training_at_nyu/","content":"","keywords":"","version":"Next"},{"title":"Workshops offered by the HPC team at the library:​","type":1,"pageTitle":"HPC training at NYU","url":"/rts-docs-dev/docs/hpc/training_and_support/training_at_nyu/#workshops-offered-by-the-hpc-team-at-the-library","content":" The HPC team offers multiple workshops at the library covering various aspects of HPC usage from novice to advanced level. You can find the list of available HPC couses can be viewed at nyu.libcal.com.  ","version":"Next","tagName":"h3"},{"title":"HPC Caprentry tutorials for Greene​","type":1,"pageTitle":"HPC training at NYU","url":"/rts-docs-dev/docs/hpc/training_and_support/training_at_nyu/#hpc-caprentry-tutorials-for-greene","content":" Introduction to using the shell in a High-Performance Computing context: This lesson provides an introduction to the bash shell aimed at researchers who will be using the command line to use remote, high-performance computing (HPC) systems. The material is also suitable for teaching the use of the shell for any remote, advanced computing resources. An Introduction to High Performance Computing: This lesson teaches the basics of interacting with high-performance computing (HPC) clusters through the command line.  ","version":"Next","tagName":"h3"},{"title":"Introductory Videos​","type":1,"pageTitle":"HPC training at NYU","url":"/rts-docs-dev/docs/hpc/training_and_support/training_at_nyu/#introductory-videos","content":" We have a playlist of how-to videos covering the basics of accessing and using Greene here (www.youtube.com). ","version":"Next","tagName":"h3"},{"title":"HPC training outside NYU","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/training_and_support/training_outside/","content":"","keywords":"","version":"Next"},{"title":"ACCESS workshops​","type":1,"pageTitle":"HPC training outside NYU","url":"/rts-docs-dev/docs/hpc/training_and_support/training_outside/#access-workshops","content":" As part of the Advanced Cyberinfrastructure Coordination Ecosystem: Services &amp; Support program, NSF provides tutorials for HPC, OpenOnDemand, etc. ","version":"Next","tagName":"h3"},{"title":"High Speed Research Network","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hsrn/intro/","content":"High Speed Research Network","keywords":"","version":"Next"},{"title":"Software on Greene","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/tools_and_software/software_on_greene/","content":"","keywords":"","version":"Next"},{"title":"Software Overview​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/docs/hpc/tools_and_software/software_on_greene/#software-overview","content":" There are different types of software packages available  Use module avail command to see preinstalled software. This includes the licensed software listed below Singularity Containers You can find those already built and ready to use, at location /scratch/work/public/singularity/For more information on running software with Singularity, click here. Python/R/Julia packages can be installed by a user  If you need another linux program installed, please contact us at hpc@nyu.edu  ","version":"Next","tagName":"h2"},{"title":"Software and Environment Modules​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/docs/hpc/tools_and_software/software_on_greene/#software-and-environment-modules","content":" Lmod, an Environment Module system, is a tool for managing multiple versions and configurations of software packages and is used by many HPC centers around the world. With Environment Modules, software packages are installed away from the base system directories, and for each package, an associated modulefile describes what must be altered in a user's shell environment - such as the $PATH environment variable - in order to use the software package. The modulefile also describes dependencies and conflicts between this software package and other packages and versions.  To use a given software package, you load the corresponding module. Unloading the module afterwards cleanly undoes the changes that loading the module made to your environment, thus freeing you to use other software packages that might have conflicted with the first one.  Below is a list of modules and their associated functions:  Command\tFunctionmodule unload &lt;module-name&gt;\tunload a module module show &lt;module-name&gt;\tsee exactly what effect loading the module will have with module purge\tremove all loaded modules from your environment module load &lt;module-name&gt;\tload a module module whatis &lt;module-name&gt;\tfind out more about a software package module list\tcheck which modules are currently loaded in your environment module avail\tcheck what software packages are available module help &lt;module-name&gt;\tA module file may include more detailed help for the software package  ","version":"Next","tagName":"h2"},{"title":"Package Management for R, Python, & Julia, and Conda in general​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/docs/hpc/tools_and_software/software_on_greene/#package-management-for-r-python--julia-and-conda-in-general","content":" Conda environments (Python, R)Using virtual environments for PythonManaging R packages with renvSingularity with Miniconda  ","version":"Next","tagName":"h2"},{"title":"Examples of software usage on Greene​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/docs/hpc/tools_and_software/software_on_greene/#examples-of-software-usage-on-greene","content":" Examples can be found under /scratch/work/public/examples/ and include the following   alphafold\tknitro\tSingularity amd GPUs\tlammps\tslurm comsol\tmatlab\tspark c-sharp\tmathematica\tstata crystal17\tnamd\tsquashfs fluent\torca\ttrinity gaussian\tquantum-espresso\tvnc hadoop-streaming\tR\tvscode julia\tsas\txvfb jupyter notebooks\tschrodinger\t  ","version":"Next","tagName":"h2"},{"title":"Accessing Datasets with Singularity​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/docs/hpc/tools_and_software/software_on_greene/#accessing-datasets-with-singularity","content":" Singularity for Datasets  ","version":"Next","tagName":"h2"},{"title":"Licensed Software​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/docs/hpc/tools_and_software/software_on_greene/#licensed-software","content":" ","version":"Next","tagName":"h2"},{"title":"SCHRODINGER​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/docs/hpc/tools_and_software/software_on_greene/#schrodinger","content":" Schrödinger provides a complete suite of software solutions with the latest advances in pharmaceutical research and computational chemistry. The NYU New York campus has a limited number of licenses for the Biologics Suite (ConfGen, Epik, Jaguar, Jaguar pKa, MacroModel, Prime, QSite, SiteMap), BioLuminate and the Basic Docking Suite.  note Schrödinger can be used for non-commercial, academic purposes ONLY.  Using SCHRODINGER on HPC Cluster​  To load Schrodinger module execute  $ module load schrodinger/2021-1   Using SCHRODINGER on NYU Lab Computers​  Request your account at: https://www.schrodinger.com/request-accountDownload the software at: https://www.schrodinger.com/downloads/releasesContact NYU-HPC team to request your license file.  These license servers are accessible from NYU subnet.  Please see the following links for installation of the license file:  https://www.schrodinger.com/kb/377238https://www.schrodinger.com/license-installation-instructions  To check licenses status  # module load schrodinger/2021-1 # load schrodinger if not already loaded # licadmin STAT # licutil -jobs ## For example: [wang@cs001 ~]$ licutil -jobs ######## Server /share/apps/schrodinger/schrodinger.lic Product &amp; job type Jobs BIOLUMINATE 10 BIOLUMINATE, Docking 1 BIOLUMINATE, Shared 10 CANVAS 50 COMBIGLIDE, Grid Generation 11 COMBIGLIDE, Library Generation 50 COMBIGLIDE, Protein Prep 11 COMBIGLIDE, Reagent Prep 1 EPIK 11 GLIDE, Grid Generation 11 GLIDE, Protein Prep 11 GLIDE, SP Docking 1 GLIDE, XP Descriptors 1 GLIDE, XP Docking 1 IMPACT 11 JAGUAR 5 JAGUAR, PKA 5 KNIME 50 LIGPREP, Desalter 1 LIGPREP, Ionizer 3511 LIGPREP, Ligparse 1 LIGPREP, Neutralizer 1 LIGPREP, Premin Bmin 1 LIGPREP, Ring Conf 1 LIGPREP, Stereoizer 1 LIGPREP, Tautomerizer 1 MACROMODEL 5 MACROMODEL, Autoref 5 MACROMODEL, Confgen 5 MACROMODEL, Csearch Mbae 5 MAESTRO, Unix 1000 MMLIBS 3511 PHASE, CL Phasedb Confsites 1 PHASE, CL Phasedb Convert 1 PHASE, CL Phasedb Manage 1 PHASE, DPM Ligprep Clean Structures 1 PHASE, DPM Ligprep Generate Conformers 5 PHASE, MD Create sites 1 PRIME, CM Build Membrane 2 PRIME, CM Build Structure 2 PRIME, CM Edit Alignment 2 PRIME, CM Struct Align 18 PRIME, Threading Search 2 QSITE 5 SITEMAP 10   Schrodinger Example Files​  Example SBATCH jobs and outputs are available to review here:  /scratch/work/public/examples/schrodinger/   ","version":"Next","tagName":"h3"},{"title":"COMSOL​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/docs/hpc/tools_and_software/software_on_greene/#comsol","content":" COMSOL is a problem-solving simulation environment, enforcing compatibility guarantees consistent multiphysics models. COMSOL Multiphysics is a general-purpose software platform, based on advanced numerical methods, for modeling and simulating physics-based problems. The package is cross-platform (Windows, Mac, Linux). The COMSOL Desktop helps you organize your simulation by presenting a clear overview of your model at any point. It uses functional form, structure, and aesthetics as the means to achieve simplicity for modeling complex realities.  note This license is for academic use only with Floating Network Licensing in nature i.e., authorized users are allowed to use the software on desktops. Please contact hpc@nyu.edu for the license. However, COMSOL is also available on NYU HPC cluster Greene.  In order to check what Comsol licenses are available on Greene use comsol_licenses command in your terminal session.  Several versions of COMSOL are available on the HPC cluster. To use COMSOL on the Greene HPC cluster, please load the relevant module in your batch job submission script:  module load comsol/5.6.0.280   To submit a COMSOL job in a parallel fashion, running on multiple processing cores, follow the steps below:  Create a directory on &quot;scratch&quot; as given below.  mkdir /scratch/&lt;net_id&gt;/example cd /scratch/&lt;net_id&gt;/example   Copy example files to your newly created directory  cp /scratch/work/public/examples/comsol/run-comsol.sbatch /scratch/&lt;net_id&gt;/example/ cp /scratch/work/public/examples/comsol/test-input.mph /scratch/&lt;net_id&gt;/example/   Edit the slurm batch script file (run-comsol.sbatch) to match your case (for example chance location of the run directory).Once the slurm batch script file is ready, it can be submitted to the job scheduler using sbatch. After successful completion of job, verify output log file for detail output information.  sbatch run-comsol.sbatch   ","version":"Next","tagName":"h3"},{"title":"MATHEMATICA​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/docs/hpc/tools_and_software/software_on_greene/#mathematica","content":" Mathematica is a general computing environment with organizing algorithmic, visualization, and user interface capabilities. The many mathematical algorithms included in Mathematica make computation easy and fast.  To run Mathematica on the Greene HPC cluster, please load the relevant module in your batch job submission script:  module load mathematica/12.1.1   note In the example below the module is loaded already in the sbatch script.  To submit a batch Mathematica job for running in a parallel mode on multiple processing cores, follow below steps:  Create a directory on &quot;scratch&quot; as given below.  mkdir /scratch/&lt;net_id&gt;/example cd /scratch/&lt;net_id&gt;/example   Copy example files to your newly created directory.  cp /scratch/work/public/examples/mathematica/basic/example.m /scratch/&lt;net_id&gt;/example/ cp /scratch/work/public/examples/mathematica/basic/run-mathematica.sbatch /scratch/&lt;net_id&gt;/example   Edit the slurm batch script file (run-mathematica.sbatch) to match your case (for example chance location of the run directory).Once the sbatch script file is ready, it can be submitted to the job scheduler using sbatch. After successful completion of job, verify output log file generated.  sbatch run-mathematica.sbatch   ","version":"Next","tagName":"h3"},{"title":"SAS​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/docs/hpc/tools_and_software/software_on_greene/#sas","content":" SAS is a software package which enables programmers to perform many tasks, including:  Information retrievalData managementReport writing &amp; graphicsStatistical analysis and data miningBusiness planningForecasting and decision supportOperations research and project managementQuality improvementApplications developmentData warehousing (extract, transform, load)Platform independent and remote computing.  There are licenses for 2 CPUs on the HPC Cluster.  Running a parallel SAS job on HPC cluster (Greene):​  To submit a SAS job for running on multiple processing elements, follow below steps:  Create a directory on &quot;scratch&quot;:  mkdir /scratch/&lt;net_id&gt;/example cd /scratch/&lt;net_id&gt;/example   Copy example files to your newly created directory.  cp /scratch/work/public/examples/sas/test.sas /scratch/&lt;net_id&gt;/example/ cp /scratch/work/public/examples/sas/test2.sas /scratch/&lt;net_id&gt;/example/ cp /scratch/work/public/examples/sas/run-sas.sbatch /scratch/&lt;net_id&gt;/example/   Submit as shown below. After successful completion of job, verify output log file generated.  sbatch run-sas.sbatch   ","version":"Next","tagName":"h3"},{"title":"MATLAB​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/docs/hpc/tools_and_software/software_on_greene/#matlab","content":" MATLAB is a technical computing environment for high performance numeric computation and visualization. MATLAB integrates numerical analysis, matrix computation, signal processing, and graphics in an easy to use environment without using traditional programming.  MATLAB on personal computers and laptops​  NYU has a Total Academic Headcount (TAH) license which provides campus-wide access to MATLAB, Simulink, and a variety of add-on products. All faculty, researchers, and students (on any NYU campus) can use MATLAB on their personal computers and laptops and may go to the following site to download the NYU site license software free of charge.  https://www.mathworks.com/academia/tah-portal/new-york-university-618777.html  MATLAB can be used for non-commercial, academic purposes.  There are several versions of Matlab available on the cluster and the relevant version can be loaded.  module load matlab/2020b module load matlab/2021a   In order to run MATLAB interactively on the cluster, start an interactive slurm job, load the matlab module and launch an interactive matlab session in the terminal.  Mathworks has provided a Greene Matlab User Guide that presents useful tips and practices for using Matlab on the cluster.  ","version":"Next","tagName":"h3"},{"title":"STATA​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/docs/hpc/tools_and_software/software_on_greene/#stata","content":" Stata is a command and menu-driven software package for statistical analysis. It is available for Windows, Mac, and Linux operating systems. Most of its users work in research. Stata's capabilities include data management, statistical analysis, graphics, simulations, regression and custom programming.  Running a parallel STATA job on HPC cluster (Greene):​  To submit a STATA job for running on multiple processing elements, follow below steps.  Create a directory on &quot;scratch&quot;:  mkdir /scratch/&lt;net_id&gt;/example cd /scratch/&lt;net_id&gt;/example   Copy example files to your newly created directory.  cp /scratch/work/public/examples/stata/run-stata.sbatch /scratch/&lt;net_id&gt;/example/ cp /scratch/work/public/examples/stata/stata-test.do /scratch/&lt;net_id&gt;/example/   Submit using sbatch. After successful completion of job, verify output log file generated.  sbatch run-stata.sbatch   ","version":"Next","tagName":"h3"},{"title":"GAUSSIAN​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/docs/hpc/tools_and_software/software_on_greene/#gaussian","content":" Gaussian uses basic quantum mechanic electronic structure programs. This software is capable of handling proteins and large molecules using semi-empirical, ab initio molecular orbital (MO), density functional, and molecular mechanics calculations.  The NYU Gaussian license only covers PIs at the Washington Square Park campus. We will grant access to you after verifying your WSP affiliation. For access, please email hpc@nyu.edu.  Running a parallel Gaussian job on HPC cluster (Greene):​  To submit a Gaussian job for running on multiple processing elements, follow below steps.  Create a directory on &quot;scratch&quot;:  mkdir /scratch/&lt;net_id&gt;/example cd /scratch/&lt;net_id&gt;/example #Copy example files to your newly created directory. cp /scratch/work/public/examples/gaussian/basic/test435.com /scratch/&lt;net_id&gt;/example/ cp /scratch/work/public/examples/gaussian/basic/run-gaussian.sbatch /scratch/&lt;net_id&gt;/example/   Once the sbatch script file is ready, it can be submitted to the job scheduler using sbatch. After successful completion of job, verify output log file generated.  sbatch run-gaussian.sbatch   ","version":"Next","tagName":"h3"},{"title":"Knitro​","type":1,"pageTitle":"Software on Greene","url":"/rts-docs-dev/docs/hpc/tools_and_software/software_on_greene/#knitro","content":" Knitro is a commercial software package for solving large scale mathematical optimization problems. Knitro is specialized for nonlinear optimization, but also solves linear programming problems, quadratic programming problems, systems of nonlinear equations, and problems with equilibrium constraints. The unknowns in these problems must be continuous variables in continuous functions; however, functions can be convex or nonconvex. Knitro computes a numerical solution to the problem—it does not find a symbolic mathematical solution. Knitro versions 9.0.1 and 10.1.1 are available.  Running a parallel Knitro job on HPC cluster (Greene):​  To submit a Knitro job for running on multiple processing elements, follow below steps.  Create a directory on &quot;scratch&quot;:  mkdir /scratch/&lt;net_id&gt;/example cd /scratch/&lt;net_id&gt;/example   Copy example files to your newly created directory.  cp /scratch/work/public/examples/knitro/knitro.py /scratch/&lt;net_id&gt;/example/   There is no sample sbatch script available for knitro.After creating your own sbatch script you can execute it as follows:  sbatch &lt;script&gt;.sbatch  ","version":"Next","tagName":"h3"},{"title":"Research Technology Cloud","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/rtc/intro/","content":"Research Technology Cloud","keywords":"","version":"Next"},{"title":"Linux Tutorial","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/tutorials/linux_tutorial/","content":"","keywords":"","version":"Next"},{"title":"Available file systems on Greene​","type":1,"pageTitle":"Linux Tutorial","url":"/rts-docs-dev/docs/hpc/tutorials/linux_tutorial/#available-file-systems-on-greene","content":" Files Systems for usage:  The NYU HPC clusters have multiple file systems for user's file storage needs. Each file system is configured differently to serve a different purpose.  Space\tEnvironment Variable\tPurpose\tFlushed?\tAllocation (per user)/home\t$HOME\tProgram Development space; For storing small files, source code, scripts etc that are backed up\tNO\t20GB /scratch\t$SCRATCH\tComputational Workspace; For storing large files/data, infrequent reads and writes\tYES Files not accessed for 60 days are deleted\t5TB /archive\t$ARCHIVE\tLong Term Storage ( Cold storage )\tNO\t2TB  ","version":"Next","tagName":"h2"},{"title":"Basic Linux Commands​","type":1,"pageTitle":"Linux Tutorial","url":"/rts-docs-dev/docs/hpc/tutorials/linux_tutorial/#basic-linux-commands","content":" ","version":"Next","tagName":"h2"},{"title":"Navigating the directory structure​","type":1,"pageTitle":"Linux Tutorial","url":"/rts-docs-dev/docs/hpc/tutorials/linux_tutorial/#navigating-the-directory-structure","content":" We've already seen ssh, which takes us from the host we are on to a different host, and hostname, which tells us which host we are on now. Mostly you'll move around filesystems and directories, which resemble inverted tree structures as shown below schematically:  ls - To list files in the current directory  If this is your first time using the HPC Cluster, ls probably won't return anything, because you have no files to list.  There are a couple of useful options for ls:  ls -l lists the directory contents in long format, one file or directory per line, with extra information about who own the file, how big it is, and what permissions are set. ls -a lists hidden files. In Unix, files whose names begin with dot (.) are hidden. This does not stop anything from using those files, it simply instructs ls not to show the files unless the -a option is used.  A command typed at the Unix command prompt, looks something like this:  pwd - print working directory, or &quot;where am i now ?&quot; in the directory space.  In Unix, filesystems and directories are arranged in a hierarchy. **A forward slash &quot;/&quot; is the directory separator, and the topmost directory visible to a host is called &quot;/&quot;. Filesystems are also mounted into this directory structure, so you can access everything that is visible on this host by moving around in the directory hierarchy.  You should see something like /home/NetID  cd - To change to a different directory,  use &quot;cd&quot; (&quot;change directory&quot;). You'll need to give it the path to the directory you wish to change into, eg &quot;cd /scratch/NetID&quot;. You can go up one directory with &quot;cd ..&quot;.  If you run &quot;cd&quot; with no arguments, you will be returned to your home directory and if you run &quot;cd -&quot;, you will be returned to the directory you were in most recently.  mkdir - To create a new location, use &quot;mkdir new_directory_name&quot;.  rmdir - To remove a directory, use &quot;rmdir new_directory_name&quot;.  man - Manual page. This command provides more information about a command eg., &quot;man ls&quot;  cat - Prints the content of the file eg., &quot;cat filename&quot;  ","version":"Next","tagName":"h3"},{"title":"Copying, moving or deleting files locally​","type":1,"pageTitle":"Linux Tutorial","url":"/rts-docs-dev/docs/hpc/tutorials/linux_tutorial/#copying-moving-or-deleting-files-locally","content":" Copying: The &quot;cp&quot; command makes a duplicate copy of files and directories within a cluster or machine. The general usage is &quot;cp source destination&quot;:  command\tExplanationcp test_file.txt test_file2.txt\tMakes a duplicate copy of test_file.txt with a new name test_file2.txt cp -r subdir subdir2\tThat is, a new directory &quot;subdir2&quot; is created and each file under subdir is copied recursively to the new subdir2  Moving: The &quot;mv&quot; command renames files and directories within a cluster or machine. The general usage is &quot;mv source_dir destination_dir&quot;  command\tExplanationmv dummy_file.txt test_file.txt\tRenames dummy_file.txt as test_file.txt mv subdir new_subdir\tRenames the directory &quot;subdir&quot; to a new directory &quot;new_subdir&quot;  Deleting files: The &quot;rm&quot; ( remove ) command deletes files and optionally directories within a cluster or machine.  danger There is no undelete in Unix. Once it is removed, it's gone !  command\tExplanationrm dummy_file.txt\tRemove a file rm -i dummy_file.txt\tIf you use -i you will be prompted for confirmation before each file is deleted rm -f serious_file.txt\tForcibly remove a file rmdir subdir/\tRemove subdir only if it's empty rm -r subdir/\tRecursively delete the directory subdir and everything else in it. Use it with care !  ","version":"Next","tagName":"h2"},{"title":"Text Editor​","type":1,"pageTitle":"Linux Tutorial","url":"/rts-docs-dev/docs/hpc/tutorials/linux_tutorial/#text-editor","content":" nano is a friendly text editor that can be used to edit the content of an existing file or create a new file. Here are some options used in nano editor.  Options\tExplanationCtrl + O\tSave Changes Ctrl + X\tExit nano Ctrl + K\tCut single line Ctrl + U\tPaste the text  ","version":"Next","tagName":"h2"},{"title":"Writing Scripts​","type":1,"pageTitle":"Linux Tutorial","url":"/rts-docs-dev/docs/hpc/tutorials/linux_tutorial/#writing-scripts","content":" An entire sequence of commands can be captured in a script for repeated or later execution. This is the mechanism by which batch jobs are run on the HPC clusters. The essential elements of a script are illustrated in the example below:  #!/bin/bash # the first line should begin with #! and the path to the interpreter under which the script should run # do stuff as if it were an interactive session: cd $HOME/some_place date ls -l pwd # scripts can use loops and conditionals. See 'man bash' for syntax for f in `ls`; do echo &quot;found a file called $f&quot; done   There are two ways to run scripts:  Give the script execute permission. and run it as a command:  chmod u+x my_script ./my_script   Run a shell, and pass the script as an argument  bash my_script   Notice in the first example that to run the script, we prefixed it with &quot;./&quot;. If the script is not somewhere in the shell's $PATH, it won't find it to run unless it's location is explicitly specified. This is even true when the script is in the normally in the $PATH. Therefore, we specify that the script is in the current directory with ./.  ","version":"Next","tagName":"h2"},{"title":"Setting execute permission with chmod​","type":1,"pageTitle":"Linux Tutorial","url":"/rts-docs-dev/docs/hpc/tutorials/linux_tutorial/#setting-execute-permission-with-chmod","content":" In Unix, a file has three basic permissions, each of which can be set for three levels of user. The permissions are:  Read permission (&quot; r &quot;) - numeric value 4 Write permission (&quot; w &quot;) - numeric value 2 Execute permission (&quot; x &quot;) - numeric value 1.  info When applied to a directory, execute permission refers to whether the directory can be entered with 'cd'  The three levels of user are:  The user who owns the file ( the &quot;user&quot;, referred to with &quot;u&quot;) The group to which the file belongs - referred to with &quot;g&quot;. Each user has a primary group and is optionally a member of other groups, when a user creates a file it is normally associated with the user's primary group. At NYU HPC all users are in a group named &quot; users &quot;, so group permissions has little meaning All other users are referred to with &quot; o &quot;  You grant permissions with &quot;chmod who+what file&quot; and revoke them with &quot;chmod who-what file&quot;.  info The first has &quot;+&quot; and the second &quot;-&quot;  Here &quot;who&quot; some combination of &quot;u&quot;, &quot;g&quot; and &quot;o&quot; and what is some combination of &quot;r&quot;, &quot;w&quot; and &quot;x&quot;. So to set execute permission, as in the example above, we use:  chmod u+x my_script  ","version":"Next","tagName":"h2"},{"title":"About SRDE, projects and getting started","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/srde/faq/basics/","content":"","keywords":"","version":"Next"},{"title":"What is the SRDE?​","type":1,"pageTitle":"About SRDE, projects and getting started","url":"/rts-docs-dev/docs/srde/faq/basics/#what-is-the-srde","content":" NYU Secure Research Data Environment (SRDE) is a centralized secure computing platform designed to support research projects that require storage, sharing and analysis of high risk datasets. The team provides researchers with consultations and resources to comply with security requirements of research grants and Data Use Agreements. SRDE resources intend to meet the security controls outlined in the NIST 800-171 to safeguard Controlled Unclassified Information (CUI).  Technical description Please refer to our technical description here  ","version":"Next","tagName":"h2"},{"title":"Who can have an SRDE project?​","type":1,"pageTitle":"About SRDE, projects and getting started","url":"/rts-docs-dev/docs/srde/faq/basics/#who-can-have-an-srde-project","content":" Access to SRDE is available to NYU researchers and sponsored members of their research team (i.e. co-investigators, research assistants, external collaborators).  IRB Waiver Requirement A project must be reviewed and approved (or receive a waiver) by the University's Institutional Review Board (IRB) in order to have an SRDE.  ","version":"Next","tagName":"h2"},{"title":"How do I sign up for an SRDE?​","type":1,"pageTitle":"About SRDE, projects and getting started","url":"/rts-docs-dev/docs/srde/faq/basics/#how-do-i-sign-up-for-an-srde","content":" Fill out our intake form to provide more information about your project: Secure Research Data Environment intake form. Once we have received your form, the team will review the information and will contact you to schedule the consultation.  ","version":"Next","tagName":"h2"},{"title":"How much will the SRDE cost?​","type":1,"pageTitle":"About SRDE, projects and getting started","url":"/rts-docs-dev/docs/srde/faq/basics/#how-much-will-the-srde-cost","content":" The cost is dependent on the needs of the project, such as size of the data and the type of machine needed for the analysis. Google Cloud has a calculator that will help estimate the costs: https://cloud.google.com/products/calculator  ","version":"Next","tagName":"h2"},{"title":"My data is not high risk, are there other options available?​","type":1,"pageTitle":"About SRDE, projects and getting started","url":"/rts-docs-dev/docs/srde/faq/basics/#my-data-is-not-high-risk-are-there-other-options-available","content":" There are several options available depending on the data risk classification and the needs of the project. There are resources provided by the university such as research project space, cloud computing, etc. You can check out many of these services on the HPC Support Site. If you are unsure on how to proceed, a consultation with the SRDE team will help determine the best path forward.  ","version":"Next","tagName":"h2"},{"title":"What does the SRDE team need from me for the consultation?​","type":1,"pageTitle":"About SRDE, projects and getting started","url":"/rts-docs-dev/docs/srde/faq/basics/#what-does-the-srde-team-need-from-me-for-the-consultation","content":" To help get things started it would be beneficial to submit an intake form with any related data governance documentation (files can be attached to the form), including, but not limited to, data use agreement, OSP/IRB documents, and project information. ","version":"Next","tagName":"h2"},{"title":"Environment and Roles","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/srde/faq/env_roles/","content":"","keywords":"","version":"Next"},{"title":"Who is the Data Steward?​","type":1,"pageTitle":"Environment and Roles","url":"/rts-docs-dev/docs/srde/faq/env_roles/#who-is-the-data-steward","content":" The data steward is an individual who is responsible for the ingress and egress of the data. The data steward is usually an IT administrator of the school the PI is associated with, they should be familiar with data sets and classification and may even cosign the Data Use Agreement (DUA) associated with the project.  Data Steward role If there is no such person, the project PI will need to assign the role to someone who will NOT be analyzing the data in the SRDE since this role does not have access to the research workspace in the SRDE (to enforce separation of duties). The SRDE team will provide role-based training for the data steward.  ","version":"Next","tagName":"h2"},{"title":"Will other project users have access to my files?​","type":1,"pageTitle":"Environment and Roles","url":"/rts-docs-dev/docs/srde/faq/env_roles/#will-other-project-users-have-access-to-my-files","content":" Each user will have access to two drives within the SRDE workspace: home and scratch. The home drive is private and the scratch drive is shared. Any files for collaboration with other project team members on the workspace should be placed or copied over to the scratch drive.  ","version":"Next","tagName":"h2"},{"title":"What kind of software is available on the SRDE?​","type":1,"pageTitle":"Environment and Roles","url":"/rts-docs-dev/docs/srde/faq/env_roles/#what-kind-of-software-is-available-on-the-srde","content":" Some statistical analysis software packages are preinstalled on the SRDE, such as Stata and MATLAB, other software is added on a case-by-case basis, dependent on the security and compatibility of the software with the SRDE.  ","version":"Next","tagName":"h2"},{"title":"How long will the data stay in the SRDE?​","type":1,"pageTitle":"Environment and Roles","url":"/rts-docs-dev/docs/srde/faq/env_roles/#how-long-will-the-data-stay-in-the-srde","content":" Project lifecycle will be determined between the PI and the SRDE Team during the intake interview. ","version":"Next","tagName":"h2"},{"title":"DLP Interpretation Guide","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/srde/dlp/dlp/","content":"","keywords":"","version":"Next"},{"title":"Viewing results from the DLP report​","type":1,"pageTitle":"DLP Interpretation Guide","url":"/rts-docs-dev/docs/srde/dlp/dlp/#viewing-results-from-the-dlp-report","content":" This query, the most basic, fetches the first 100 flagged items in the report.  SELECT quote, info_type.name, info_type.sensitivity_score.score, likelihood FROM `your_table_name` LIMIT 100   Each row of the report contains a great deal of metadata on where the potentially sensitive metadata was found, as well as metadata on the DLP scan itself, but here we select only the following four columns:  quote: the span of text that was flagged as sensitive infoinfo_type.name: the type of sensitive infoInfo_type.sensitivity_score.score: the sensitivity level (LOW, MODERATE, or HIGH)likelihood: the confidence with which the DLP tool has flagged the item (POSSIBLE, LIKELY, or VERY_LIKELY)  The results should look something like this. As you can see, the same piece of text may be flagged multiple times with different types, depending on the results of DLP’s auto-detection algorithms.  To see more results, you can adjust the value of the LIMIT clause or remove it entirely. Alternatively, use some of the sample queries below to view targeted subsets of the data.  ","version":"Next","tagName":"h2"},{"title":"Sample Queries: selecting a subset of flagged items​","type":1,"pageTitle":"DLP Interpretation Guide","url":"/rts-docs-dev/docs/srde/dlp/dlp/#sample-queries-selecting-a-subset-of-flagged-items","content":" Select only high-sensitivity items  SELECT quote, info_type.name, info_type.sensitivity_score.score, likelihood FROM `your_table_name` WHERE info_type.sensitivity_score.score = &quot;SENSITIVITY_HIGH&quot; LIMIT 100   Select only items that are high-sensitivity and have a likelihood higher than “possible”  SELECT quote, info_type.name, info_type.sensitivity_score.score, likelihood FROM `your_table_name` WHERE info_type.sensitivity_score.score = &quot;SENSITIVITY_HIGH&quot; AND likelihood != &quot;POSSIBLE&quot; LIMIT 100   Select all items, sorted by type  SELECT quote, info_type.name, info_type.sensitivity_score.score, likelihood FROM `your_table_name` ORDER BY info_type.name   Select all items of type PERSON_NAME, ordered alphabetically  SELECT quote, info_type.name, info_type.sensitivity_score.score, likelihood FROM `your_table_name` WHERE info_type.name = &quot;PERSON_NAME&quot; ORDER BY quote  ","version":"Next","tagName":"h2"},{"title":"Using the SRDE","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/srde/faq/using_srde/","content":"","keywords":"","version":"Next"},{"title":"Will I receive training on how to use the SRDE?​","type":1,"pageTitle":"Using the SRDE","url":"/rts-docs-dev/docs/srde/faq/using_srde/#will-i-receive-training-on-how-to-use-the-srde","content":" Absolutely! Once your SRDE is set up the SRDE team will schedule onboarding sessions for the research workspace users and a separate one for the data steward. In addition, all users will have access to the User Guide and SRDE support team for troubleshooting and guidance.  ","version":"Next","tagName":"h2"},{"title":"Why is the SRDE in a terminal, is there a screen layout?​","type":1,"pageTitle":"Using the SRDE","url":"/rts-docs-dev/docs/srde/faq/using_srde/#why-is-the-srde-in-a-terminal-is-there-a-screen-layout","content":" At this time the SRDE is command-line only. We are working on an updated version with a graphical user interface (GUI) for projects with the need for it.  ","version":"Next","tagName":"h2"},{"title":"How do I export a file?​","type":1,"pageTitle":"Using the SRDE","url":"/rts-docs-dev/docs/srde/faq/using_srde/#how-do-i-export-a-file","content":" All egress is done by the Data Steward. In order to have a file exported, please follow instructions in the SRDE User Guide to place the file in the export folder in the research workspace, then alert your Data Steward that there is a file ready for export.  ","version":"Next","tagName":"h2"},{"title":"Can I upload my own (non-sensitive) files to the SRDE?​","type":1,"pageTitle":"Using the SRDE","url":"/rts-docs-dev/docs/srde/faq/using_srde/#can-i-upload-my-own-non-sensitive-files-to-the-srde","content":" In order to maintain the integrity of the data management flow into and out of the SRDE all data going into the environment needs to go through the Data Steward. This ensures both that the Data Steward is aware of all external data entering or leaving the environment and can confirm compliance with the DUA, as well as providing a single path for audit logging. To have your files uploaded to the environment, you can provide them to the Data Steward who can then upload them to the ingress bucket for your retrieval. ","version":"Next","tagName":"h2"},{"title":"Secure Research Data Environment (SRDE)","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/srde/getting_started/eligibility_accounts/","content":"","keywords":"","version":"Next"},{"title":"SRDE Eligibility​","type":1,"pageTitle":"Secure Research Data Environment (SRDE)","url":"/rts-docs-dev/docs/srde/getting_started/eligibility_accounts/#srde-eligibility","content":" A project must be reviewed and approved by the University's Institutional Review Board (IRB) (or receive a waiver) in order to have an SRDE. Access to SRDE is available to NYU researchers and sponsored members of their research team (i.e. co-investigators, research assistants, external collaborators). SRDE project owners and requesters must be in one of the eligible positions to act as a project Primary Investigator (PI), as outlined by the NYU IRB policy. This includes Tenured/Tenure Track Faculty, Continuing Contract faculty, Honorific Research Faculty, Professional Research Personnel, and Emeriti and retired faculty. The SRDE project owner should be the same as the PI of the IRB protocol or waiver.  Users on a research project must have valid and active NYU NetID credentials, including external, non-NYU collaborators, and be listed on the IRB protocol. Non-NYU Researchers/Collaborators need to obtain an affiliate status to obtain access. A full-time NYU faculty member must sponsor a non-NYU collaborator for affiliate status. Please see instructions for affiliate management (NYU NetID login is required to follow the link). A Data Steward must be a faculty member or university employee.  A Data Steward must be designated for each research project using SRDE and will be solely responsible for the transport of data for the project’s SRDE. Only users approved according to the Data Provider’s requirements are permitted to access project workspaces and related resources. These individuals are required to go through our on-boarding process, which includes signing a User Agreement and agreeing to our Terms of Use.  ","version":"Next","tagName":"h2"},{"title":"Requesting an SRDE Project​","type":1,"pageTitle":"Secure Research Data Environment (SRDE)","url":"/rts-docs-dev/docs/srde/getting_started/eligibility_accounts/#requesting-an-srde-project","content":" The SRDE form contains details about the project, such as if the project requires IRB (Institutional Review Board) approval, technical requirements (such as data storage and software). The form will be submitted to the SRDE team for review.  Link to the Secure Research Data Environment Intake Form.  After you submit the intake form, the SRDE team will review the submitted documents and will respond to schedule the consultation. ","version":"Next","tagName":"h2"},{"title":"Support","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/srde/support/support/","content":"Support Please email your questions to: srde-support@nyu.edu","keywords":"","version":"Next"},{"title":"Best Practices","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/srde/user_guide/best_practices/","content":"","keywords":"","version":"Next"},{"title":"Shared Files​","type":1,"pageTitle":"Best Practices","url":"/rts-docs-dev/docs/srde/user_guide/best_practices/#shared-files","content":" Each user on the research workspace has their own home directory, as well as access to the top-level /shared partition.  ","version":"Next","tagName":"h2"},{"title":"Shared data files​","type":1,"pageTitle":"Best Practices","url":"/rts-docs-dev/docs/srde/user_guide/best_practices/#shared-data-files","content":" It is recommended to keep datasets under the /shared partition, especially if they are large. This is more efficient than each researcher making their own copy from the ingress bucket, and ensures all experiments are consistent with each other.  ","version":"Next","tagName":"h3"},{"title":"Shared code files​","type":1,"pageTitle":"Best Practices","url":"/rts-docs-dev/docs/srde/user_guide/best_practices/#shared-code-files","content":" Code files should also be stored under the /shared partition whenever possible. You can use a local git repo to keep a version history of your codebase, and to avoid conflicts from multiple developers working on the same file at once. To create a repo,  cd /shared/code git init   And then, after adding or modifying files,  git add * git commit -m “log message describing your change”   The git repo, with its full version history, can be exported alongside your results for transparency and reproducibility. ","version":"Next","tagName":"h3"},{"title":"Start here!","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/srde/getting_started/intro/","content":"Start here! Welcome to the Secure Research Data Environment documentation! If you do not have an active project, please proceed to the next section that explains the eligibility criteria and how you may request one. If you are an active user, you can proceed to one of the categories on the left.","keywords":"","version":"Next"},{"title":"Data Access","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/srde/user_guide/data_access/","content":"Data Access After you are connected to the Workspace Host using SSH per the instructions in the previous section, you can access data that has been placed in the workspace ingress bucket by your Data Steward. You can use the gsutil ls and cp commands to copy the data into your home directory using the steps described below. Use the following command to see list of folders in your workspace: gsutil ls As shown above, there are several folders. Data that has been transferred into the workspace is in the Ingress folder. Use the following command to list the objects in the ingress folder, replacing the path with your project’s path: gsutil ls gs://your-workspace-ingress-path List the contents of the folder with the timestamp corresponding to the date the data was transferred into the workspace, and you will see the files that were uploaded: gsutil ls gs://your-workspace-ingress-path/data-timestamp-folder To copy the files into your home directory use gsutil cp command (use period at end to copy to your home directory): gsutil cp gs://your-workspace-ingress-path/data-timestamp-folder/filename . ","keywords":"","version":"Next"},{"title":"Troubleshooting","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/srde/user_guide/troubleshooting/","content":"Troubleshooting Coming soon!","keywords":"","version":"Next"},{"title":"Connecting to SRDE","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/srde/user_guide/connecting/","content":"","keywords":"","version":"Next"},{"title":"Connecting through Google Cloud Console​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/docs/srde/user_guide/connecting/#connecting-through-google-cloud-console","content":" Navigate to Google Cloud Console https://console.cloud.google.com/welcome and login with your NetID. Click the Select a project drop-down list at the top left corner of the page. In the Select a project window that appears, search and select the bastion project using the provided project ID (ex. test-dev1-bastion-1234).    Once selected, navigate to the VM Instances page via the Navigation menu (Menu in the top left corner of the page ) &gt; Compute Engine &gt; VM Instances. A running Bastion instance will be visible in the page as shown below:    ssh to the Bastion instance by clicking on the SSH button, a new SSH-in-browser tab will appear with a restricted CLI ( Command line interface ) connected to the instance. We are now inside the Bastion Host.    Now we can ssh to our workspace host by using the workspace internal IP address 10.0.0.2:  ssh 10.0.0.2   This will open the workspace CLI, with access to the workspace host having the computing needs to work on our data.  ","version":"Next","tagName":"h2"},{"title":"Managing Data Transfer","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/srde/user_guide/data_transfers/","content":"","keywords":"","version":"Next"},{"title":"Data Ingestion process​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/docs/srde/user_guide/data_transfers/#data-ingestion-process","content":" Ingesting data into the secure environment is a two-step process; First the Data Steward must upload the data onto the staging GCP Storage Bucket and then “push” the data into the secure Workspace environment.  ","version":"Next","tagName":"h2"},{"title":"Uploading Data to the Staging Area​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/docs/srde/user_guide/data_transfers/#uploading-data-to-the-staging-area","content":" Option1: Using the Web Console Interface​  Log into GCP console, set project to your staging project (i.e. srde-staging-dev), and navigate on the side panel to Cloud Storage -&gt; Buckets:  Navigate to your research workspace’s corresponding Staging Ingress bucket:  Copy data to the Staging Ingress bucket:  Option2: Using the CLI​  Follow the instructions in section 2 to install and configure gcloud on your workstation. Once this is done, run the following command to find your workspace’s bucket:  gsutil ls | fgrep [Workspace Name]   The workspace name will be given to you by the SRDE team after your workspace has been provisioned. The command above should output two buckets– one will be for data ingest (ingress) and the other will be for data egress:  nyu10003@cloudshell:~ (srde-staging-dev-cedd)$ gsutil ls | fgrep example gs://nyu-us-east4-example-staging-egress-9d94/ gs://nyu-us-east4-example-staging-ingress-4bd9/   To ingest data into the SRDE, run the following command to copy individual files into the ingress bucket:  gsutil cp [FILENAME] gs://[INGRESS BUCKET]   So for instance, the following command would copy an individual text file (1661-0.txt) into the example ingress bucket:  gsutil cp 1661-0.txt gs://nyu-us-east4-example-staging-ingress-4bd9/   To copy a folder, you need to add -r after cp:  gsutil cp -r [FOLDER] gs://[INGRESS BUCKET]   We would use the following command to copy a folder named dataset into the example ingress bucket:  gsutil cp -r dataset gs://nyu-us-east4-example-staging-ingress-4bd9/   ","version":"Next","tagName":"h3"},{"title":"Push Data to the Research Workspace Using Airflow​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/docs/srde/user_guide/data_transfers/#push-data-to-the-research-workspace-using-airflow","content":" Once the data is in the Staging Ingress bucket, navigate to Cloud Composer and click on Airflow:  In Airflow you will see the DAG workflows for your project. If you do not see any DAGs, contact srde-support@nyu.edu with subject line “Missing Airflow Permissions”  Once you see the workflows for your project, pick the one named [project-id]_Ingress_1_Staging_to_Workspace, which will bring you to the DAG page. On the DAG page, click on the “play” button at the top right to trigger the DAG:  The DAG may take a few minutes to run. You can see its progress in the status display on the bottom left.  The display shows a list of tasks executed by the DAG. A light green square will appear next to the task when it is running, and turn dark green when it is complete. When all tasks have finished successfully, the DAG is done.  Researchers will now be able to see the data in the ingress bucket in the research project workspace.  Access policy for Data Stewards Data stewards do not have access to the research project workspace.  Instructions for researchers who need to access the ingested data in the research workspace are found in the Data Access section of this document.  ","version":"Next","tagName":"h3"},{"title":"Data Egress Process​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/docs/srde/user_guide/data_transfers/#data-egress-process","content":" To transport data out of the SRDE project workspace, research team members copy files to be exported to the 'export' folder in the Researcher Workspace Egress bucket, sample command below:  After the files have been copied to the export folder in the egress bucket within the workspace, researchers will notify the Data Steward that they are ready to export. The Data Steward will first move the files to the Staging Egress folder and scan them using the Data Loss Prevention API, a tool for automatically detecting sensitive data types. Next, they will check the generated report and either pass the inspection or fail it. Passing the inspection moves the data onwards to the Research Data Egress project for external sharing. Failing the inspection blocks the export.  ","version":"Next","tagName":"h2"},{"title":"Push the data from the Research Workspace to Staging​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/docs/srde/user_guide/data_transfers/#push-the-data-from-the-research-workspace-to-staging","content":" First, run Egress DAG #1 to move files to the Staging Egress folder. Follow the same instructions as above to navigate to the Airflow page.  Once on the Airflow page, find the DAG named [project-id]_Egress_1_Workspace_to_Staging_Inspection.  Once on the DAG page, follow the steps to trigger the DAG, as instructed above. This DAG executes several tasks:  An archive copy of the export files is created within the workspace.The export files are moved to the staging environment.A DLP inspection is run to scan the exported files for sensitive data. The DLP scan may take some time to run, so wait for all tasks to be marked as successful (dark green) before proceeding.  ","version":"Next","tagName":"h3"},{"title":"Check the DLP inspection report​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/docs/srde/user_guide/data_transfers/#check-the-dlp-inspection-report","content":" After Stage 1 is successfully completed, the DLP inspection findings are written to BigQuery. To examine results, navigate to BigQuery by going to Google console webpage, typing BigQuery on the search bar, and selecting it from the list.  Once in BigQuery, on the Explorer tab on the left, click on the corresponding project, then on the table that corresponds to the scan that was done. The name will contain the UTC date and time of the scan, using the format dlp_YYYY-MM-DD-HHMMSS. You can verify the report’s creation time under the “Details” tab.  Select “Query &gt; In new tab” to examine the results. The following default query will return a sample of 1000 results:  SELECT * FROM “table_id” LIMIT 1000   For more information on querying the DLP report, see the DLP Interpretation Guide (TODO Add section on DLP Interp. guide!)  Click on Run to run the query and review the results of the scan. After running the query you will see the results on the lower half of the window:  ","version":"Next","tagName":"h3"},{"title":"Pass or fail the inspection​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/docs/srde/user_guide/data_transfers/#pass-or-fail-the-inspection","content":" Once the results are reviewed, the Data Steward approves or denies movement to the external egress bucket. They navigate back to the Airflow page and choose one of the following options:  If DLP scan results are NOT approved, Data Steward fails the data export by running Egress_2_Staging_Fail_inspection. Once on the DAG page, follow the steps to trigger the DAG, as instructed above. The data will be fully deleted from staging, and only the archived copy will remain in the workspace.If DLP scan results ARE approved, Data Steward passes the data export by running Egress_3_Staging_Pass_Inspection. Once on the DAG page, follow the steps to trigger the DAG, as instructed above. The data will be transferred to the project’s external egress bucket, where the researchers will be able to access and share it. After the final egress DAG completes successfully, the Data Steward should notify the researchers either a) that their data is available in the external egress bucket or b) that their data export was denied and why.  ","version":"Next","tagName":"h3"},{"title":"Moving Files to Export​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/docs/srde/user_guide/data_transfers/#moving-files-to-export","content":" You can use the gsutil cp command to copy data from your home directory to the Egress export folder in the workspace using the following steps. Use the gsutil ls command to see the list of folders in your workspace. Copy your file into the Egress folder, adding /export/yourfilename to the Egress folder path:  gsutil cp data_file.txt gs://egress_bucket_path/export/data_file.txt     ","version":"Next","tagName":"h2"},{"title":"Auto-Inspection​","type":1,"pageTitle":"Managing Data Transfer","url":"/rts-docs-dev/docs/srde/user_guide/data_transfers/#auto-inspection","content":" When files are added to the export folder, they are automatically scanned for sensitive data using the Data Loss Prevention API. This is the same tool that the Data Steward will use to examine your exported data and approve or deny the export, so you should review the results of auto-inspection carefully. Before notifying the Data Steward that an export is ready, make sure that the DLP inspection does not detect sensitive info, or that if it does, you are aware of the items it flags and can explain why they are false alarms.  The DLP scan is automatically triggered by any new file in the export folder. It may take several minutes to run. When it is complete, a summary file will be written back to the “dlp” folder in the egress bucket.  gsutil ls gs://egress_bucket_path/dlp   Within this folder, a folder is created for each exported file, and within that are dated summary reports for each version.  gsutil ls gs://egress_bucket_path/dlp/data_file.txt/   You should see a file of the format dlp_results_YYYY-MM-DD-HHMMSS corresponding to approximately when you added the file to the export folder. Note that the scan takes about a minute to pick up new files, and may behave oddly if you upload several versions very close together.  To see the summary file contents, use the command:  gsutil cat gs://egress_bucket_path/dlp/data_file.txt/dlp_results_YYYY-MM-DD-HHMMSS   If sensitive information is detected, you will see it listed by type and count  If no sensitive information is detected, you will see a clean scan report. Double-check that the “Processed bytes” and “Total estimated bytes” approximately line up with the size of your file–if both values are 0 it is likely that there was an error in the scan. ","version":"Next","tagName":"h2"},{"title":"Connecting through Google Cloud Shell​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/docs/srde/user_guide/connecting/#connecting-through-google-cloud-shell","content":" Navigate to https://shell.cloud.google.com/ while logged in using your NetID.  ","version":"Next","tagName":"h2"},{"title":"Setting project and zone​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/docs/srde/user_guide/connecting/#setting-project-and-zone","content":" Note - Ask your SRDE administrator for the appropriate GCP PROJECT_ID and ZONE_NAME. Replace the values in the two commands below and run them  gcloud config set project PROJECT_ID gcloud config set compute/zone ZONE_NAME   ","version":"Next","tagName":"h3"},{"title":"Confirm settings​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/docs/srde/user_guide/connecting/#confirm-settings","content":" Before proceeding, confirm that the project and zone match your GCP project ID and zone:  gcloud config list [compute] region = us-east4 zone = us-east4-a [core] account = netid@nyu.edu disable_usage_reporting = False project = test-dev1-bastion-1234 Your active configuration is: [default]   ","version":"Next","tagName":"h3"},{"title":"Generate SSH keys​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/docs/srde/user_guide/connecting/#generate-ssh-keys","content":" Unused keys expire! Google Cloud Shell will delete your files, including generated SSH keys, if they are not accessed for 120 days. If this happens you will need to generate them again.  The simplest way to generate SSH keys is to delegate the key generation to gcloud. In order to trigger key creation, run the following command.  note Ignore the result of this command. It will most likely print errors to the output console.  gcloud compute ssh bastion-vm   You will be prompted to enter an SSH passphrase. This is optional, however it is recommended for additional user security.  The above command should log you into the bastion VM. You will see a prompt like:  -bash-4.4$”   Before proceeding, exit back to your local machine  exit   Then make sure the above step created two keys in your ssh home directory (~/.ssh) as shown below:  ls ~/.ssh     Start the ssh-agent on your local machine  eval `ssh-agent -s`   Add the google_compute_engine key to your ssh session  ssh-add ~/.ssh/google_compute_engine   Connect to the instance with gcloud using the –ssh-flag-”-A” flag  note This command uses the default project and zone set above.  gcloud compute ssh bastion-vm --ssh-flag=&quot;-A&quot; --tunnel-through-iap   ","version":"Next","tagName":"h3"},{"title":"Add SSH key to session​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/docs/srde/user_guide/connecting/#add-ssh-key-to-session","content":" Run the following command to add the google_compute_engine key to the current session:ssh  ssh-add -L   Connect to the workstation-vm  ssh 10.0.0.2   ","version":"Next","tagName":"h3"},{"title":"Future logins​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/docs/srde/user_guide/connecting/#future-logins","content":" After the initial login, you will not need to regenerate the SSH keys, but you will need the rest of the command sequence from “Start the SSH agent”. On your local machine:  eval `ssh-agent -s` ssh-add ~/.ssh/google_compute_engine gcloud compute ssh bastion-vm --ssh-flag=&quot;-A&quot; --tunnel-through-iap --project=PROJECT_ID   And then on the bastion VM:  ssh 10.0.0.2   ","version":"Next","tagName":"h3"},{"title":"Connecting on MacOS/Linux​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/docs/srde/user_guide/connecting/#connecting-on-macoslinux","content":" ","version":"Next","tagName":"h2"},{"title":"Install gcloud CLI​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/docs/srde/user_guide/connecting/#install-gcloud-cli","content":" Follow the official guidelines to install the latest version of gcloud CLI locally on your computer.  note After completing the gcloud installation, verify that the gcloud binary is in your $PATH environment variable.  ","version":"Next","tagName":"h3"},{"title":"Configure local gcloud settings​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/docs/srde/user_guide/connecting/#configure-local-gcloud-settings","content":" Run the following command. It generates a link as shown below  gcloud auth login --no-launch-browser     Copy the link and open your chrome browser in incognito mode to perform user sign in.Username is your NYU NetID email address. For e.g. netid@nyu.edu  You will be redirected to the NYU SSO page and MFA verification through Duo Push. After successfully logging in, you will be asked to allow google SDK to access your account as shown below    Pressing the “Allow” button on this page will present the authorization code. Copy the code and paste it in the terminal. If this step is successful, you should see this text printed to the console. You are now logged in as [netid@nyu.edu].  ","version":"Next","tagName":"h3"},{"title":"Connect to the workspace​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/docs/srde/user_guide/connecting/#connect-to-the-workspace","content":" Follow the same instructions for connecting with Google Cloud Shell above, starting from section on setting project and zone above.  ","version":"Next","tagName":"h3"},{"title":"Connecting on Windows 10/11​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/docs/srde/user_guide/connecting/#connecting-on-windows-1011","content":" ","version":"Next","tagName":"h2"},{"title":"Start and Configure SSH-Agent Service​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/docs/srde/user_guide/connecting/#start-and-configure-ssh-agent-service","content":" Using an elevated PowerShell window (run as admin), execute the following command to install the SSH-Agent service and configure it to start automatically when you log into your machine:  Get-Service ssh-agent | Set-Service -StartupType Automatic -PassThru | Start-Service     ","version":"Next","tagName":"h3"},{"title":"Install gcloud CLI​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/docs/srde/user_guide/connecting/#install-gcloud-cli-1","content":" Download the [Google Cloud CLI installer] (https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe) and run the installer    Alternatively, run the following command to download and install:  (New-Object Net.WebClient).DownloadFile(&quot;https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe&quot;, &quot;$env:Temp\\GoogleCloudSDKInstaller.exe&quot;) &amp; $env:Temp\\GoogleCloudSDKInstaller.exe   ","version":"Next","tagName":"h3"},{"title":"Install Git​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/docs/srde/user_guide/connecting/#install-git","content":" Download the Git Bash setup from the official website: https://git-scm.com/ and run the installer  ","version":"Next","tagName":"h3"},{"title":"Install Putty​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/docs/srde/user_guide/connecting/#install-putty","content":" Download and install Putty from this link https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html  Post installation verify that the Putty authentication agent is installed and available  For 64-bit installer, you will find this executable at C:/Program Files/PuTTY/pageant.exe  ","version":"Next","tagName":"h3"},{"title":"Install Python (>version 3.0)​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/docs/srde/user_guide/connecting/#install-python-version-30","content":" Install Python from the official website:https://www.python.org/downloads/  Remember to check “Add python to the environment path.” ***add screenshot  Make sure it's installed and available on PATH. On many systems Python comes pre-installed, you can try running the python command to start the Python interpreter to check and see if it is already installed.    On windows you can also try the py command which is a launcher which is more likely to work. If it is installed you will see a response which will include the version number, for example:    ","version":"Next","tagName":"h3"},{"title":"Logging in:​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/docs/srde/user_guide/connecting/#logging-in","content":" Authenticate gcloud by starting a new session of command line or powershell. initialize and login to gcloud with your account (you will be redirected to the browser for authentication)  gcloud auth login       Run Git Bash and start the ssh-agent on your local machine  eval `ssh-agent -s`     Add the SSH key to agent by running  pageant.exe     The app runs in the background. you can find it in the tray.  Right click the icon and select &quot;Add Key&quot;. Add the google_compute_engine key with the PPK extension (~/.ssh/google_compute_engine) to your agent:  :::Skip this step in the future Go to the Pageant shortcut icon from the Windows Start Menu or your desktop.  Right click on the icon, and click on Properties. (If Properties is not an option on the menu, click on Open file location, then right click on the Pageant icon, and click on Properties)  :::    From the Shortcut tab, edit the Target field. Leave the path to pageant.exe intact. After that path, add the path to your Google .ppk key file.  Critical The key path should be outside the quotation marks. i  Here’s an example:  &quot;C:\\Program Files\\PuTTY\\pageant.exe&quot; C:\\Users\\Sam\\.ssh\\google_compute_engine.ppk     ","version":"Next","tagName":"h3"},{"title":"SSH into the bastion VM from Git Bash​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/docs/srde/user_guide/connecting/#ssh-into-the-bastion-vm-from-git-bash","content":" tip Ask your SRDE administrator for the appropriate GCP project ID.  Replace gcp-project-id with that information in the below command:  export PROJECT_ID=gcp-project-id; gcloud compute ssh bastion-vm --ssh-flag=&quot;-A&quot; --zone=us-east4-a --tunnel-through-iap --project=${PROJECT_ID}     When SSHing to bastion in the git bash window, a new terminal in putty appears with the bastion connection  A PuTTY security alert window may pop up to accept the host key, click on Accept  ","version":"Next","tagName":"h3"},{"title":"Add SSH key to session​","type":1,"pageTitle":"Connecting to SRDE","url":"/rts-docs-dev/docs/srde/user_guide/connecting/#add-ssh-key-to-session-1","content":" Run ssh-add to add the google_compute_engine key to the current session  ssh-add -L   Connect to the workstation-vm  ssh 10.0.0.2    ","version":"Next","tagName":"h3"},{"title":"HPC Foundations","type":0,"sectionRef":"#","url":"/rts-docs-dev/docs/hpc/tutorials/hpc_foundations/","content":"","keywords":"","version":"Next"},{"title":"Other File Systems​","type":1,"pageTitle":"HPC Foundations","url":"/rts-docs-dev/docs/hpc/tutorials/hpc_foundations/#other-file-systems","content":" Similar to /home, users have access to multiple filesystems that are :  Filesystem\tUser(s) space\tPurpose\tEnv Variable/home\t/home/Net_ID/\tWorkspace\t$HOME /scratch\t/scratch/Net_ID/\tGeneral Storage\t$SCRATCH /archive\t/archive/Net_ID/\tCold Storage\t$ARCHIVE  You will find more details about these filesystems at Greene Storage Types page.  You can jump to your /scratch directory at /scratch/Net_ID/ with the cd command as cd /scratch/Net_ID, Or you could simple use the $SCRATCH environment variable as:  [pp2959@log-1 ~]$ pwd /home/pp2959/ [pp2959@log-1 ~]$ cd $SCRATCH [pp2959@log-1 ~]$ pwd /scratch/pp2959/ [pp2959@log-1 ~]$   Also you can view other user(s) /scratch space on the cluster with ls /scratch.  ls /scratch   The /scratch Space:  This is a special type of filesystem called General Parallel File System (GPFS) designed for large storage and high IO (Input/Output) throughput, supporting parallel reads and writes for the best performance ! An appropriate data space where parallel compute resources ingest their datasets (and even write back to it) during very large workloads, such as distributed Deep Learning at scale All nodes in the cluster, that includes login, compute, and data transfer nodes share this filesystem This is a temporary space for loading and unloading large datasets, that is files are purged with a prior notice, to maintain performance, hence the name Scratch  The /archive Space:  An archival space for your projects, a cold storage option where you stash your work long term Cannot be accessed by compute resources Never purged  ","version":"Next","tagName":"h3"},{"title":"Running programs on a login node​","type":1,"pageTitle":"HPC Foundations","url":"/rts-docs-dev/docs/hpc/tutorials/hpc_foundations/#running-programs-on-a-login-node","content":" Login nodes. As the name implies are used for interacting with the cluster only. They are not equiped with compute heavy hardware or much memory, and hence you may run simple programs ( that can lag a bit ) but not compute heavy workloads.  Let us take a look at an example of running a simple lua script on this type of node, create a lua script file named hello.lua using vim, a powerful terminal based text editor:  [pp2959@log-3 ~]$ vim hello.lua   Running the vim command followed by a file_name as an argument creates a new text file and opens the editor within the terminal  Press i once open, this will switch the editor to insert mode.In insert mode you can start typing to file (it's a temporary buffer) like anyother text editor  Copy the below lua code and paste it in the editor with Ctrl-v ( on windows ) or Cmd-v on ( MacOS ):  os.execute(&quot;hostname&quot;) print(&quot;hello, world&quot;)   Now, Press Esc key to escape from insert mode  Notice how you cannot type anything else after escaping from insert mode, however you can go back to insert mode by clicking on i ( short for insert mode )  Then, Press colon : (don't press anything else after), you should notice the : appear near bottom left corner of the editorThis is where you type your editor commands like save file, discard changes, open a new file, etcContinue typing wq, as in the editor command should look like :wqPress Enter key to execute this commandThis saves the file to your current directory and exits the editor, you should be back on your console now  Again the : here is to start typing an editor command, followed by the command(s) themselves. Like w is to write changes to the file hello.lua followed by q to quite from the editor.  In case if you would like to force quite and start again, then press Esc first to exit from insert mode, or anyother mode you may have accidentally enabled. This ensures you are completely exited from any modes, then execute the editor commands - :q! to force quite discrading changes, here q for quite and ! for force  Once done, check the contents of your file with the cat command:  [pp2959@log-3 ~]$ ls new_file.txt hello.lua [pp2959@log-3 ~]$ cat hello.lua os.execute(&quot;hostname&quot;) print(&quot;hello, world&quot;) [pp2959@log-3 ~]$   Here os.execute() executes a shell command, in this example the command hostname to print the name of the host on which the script is being executed. Followed by printing the message hello, world  Now if you try to run the script as lua hello.lua, you may get an error like:  [pp2959@log-3 ~]$ lua hello.lua -bash: lua: command not found [pp2959@log-3 ~]$   By default software packages are not installed in our working environment.  Now, how do we run this lua script ?Since we would require a lua installation to do so  So let us try and install lua with linux's apt-get package manager:  apt-get install lua   As you can see, we encounter an error like the one below:  [pp2959@log-3 ~]$ apt-get install lua -bash: apt-get: command not found [pp2959@log-3 ~]$   apt-get is not available to users as it requires root priviliges which the users do not get.You will need to load pre-installed software pacakges with a command called module.  First, let's search for any versions of lua available by running the command module spider &lt;Software_Package&gt;:  module spider lua   This will list all lua packages Or modules available for use, as shown below:  [pp2959@log-3 ~]$ module spider lua -------------------------------------------------------------- lua: lua/5.3.6 -------------------------------------------------------------- This module can be loaded directly: module load lua/5.3.6 [pp2959@log-3 ~]$   Read the output carefully, we can see a lua package is available that is lua/5.3.6 in this example.  If the system administrators add new lua packages sometime in the future then, they appear in the above list, from which you could choose any one of them.  Pick a version from this list, in this example we select the version lua/5.3.6.  You can also check what modules are loaded in your current shell session with the command module list:  module list   [pp2959@log-3 ~]$ module list No modules loaded [pp2959@log-3 ~]$   To load the lua module, we use the module load command as module load &lt;Software_Package&gt;:  module load lua/5.3.6   Now, check and verify that the module has been loaded to your current shell environment with:  module list   Read the output carefully, you may notice that sometimes dependencies are also loaded along with a module.  Verify that we can invoke lua by running lua -v, the option -v is to print version details:  lua -v   Now, run the lua script hello.lua.  lua hello.lua   [pp2959@log-3 ~]$ lua hello.lua log-3 hello, world [pp2959@log-3 ~]$   NOTICE: First line of this output is the name of the host where the script ran, followed by the message hello, world  This way we can search for available modules with the command module spider &lt;Software_Package&gt; using keywords.  To list ALL modules try module spider without providing any keywords:  module spider   This will open up an interactive list of all modules, in linux this is called paging.  To navigate this list (paging) try the following steps carefully:  Press and hold j key to go downPress and hold k key to go upJust Click / once (don't click anything else after): You will notice the / character at the bottom left corner just like in vimContinue typing the keywords for your module name, for example just type pythoPress EnterThis will bring up matching module names based on those keywordsClick n, to jump to a next matchSimilarly, Click N to jump to a previous match And finally, to exit from the list just like in vim, use the quit command :qRetry, practise.  To unload a module try module unload &lt;Module_Name&gt;:  module unload lua/5.3.6   [pp2959@log-3 ~]$ module list Currently Loaded Modules: 1) lua/5.3.6 [pp2959@log-3 ~]$ module unload lua/5.3.6 [pp2959@log-3 ~]$ module list No modules loaded [pp2959@log-3 ~]$   To get rid of all module and start a new, try:  module purge   And for more options, try:  module --help   RECAP login nodes are .../home filesystem and it's purposeLoad necessary modules to run our programs  ","version":"Next","tagName":"h2"},{"title":"Running Programs on a compute node​","type":1,"pageTitle":"HPC Foundations","url":"/rts-docs-dev/docs/hpc/tutorials/hpc_foundations/#running-programs-on-a-compute-node","content":" The Greene cluster has over 100s of compute nodes equiped with all kinds of High Performance hardware such as x86 Intel, AMD server CPUs, and Nvidia, AMD server GPUs ( such as the H100s ).  Some of these nodes are categorized as shown below with examples:  Node Category\tDescriptionCPU Nodes\tCPU only nodes with sufficient memory. For example 48 core Intel Cascade lake CPU with 384 GB memory, per node Nvidia GPU Nodes\tNodes that are equiped with Nvidia GPUs. For example 48 Core Intel server CPU with 384 GB and 4 H100s, per node AMD GPU Nodes\tEquiped with AMD GPUs. For example 128 core CPU with 512 GB ram and 8 MI250s, per node  And these nodes are interconnected with low latency, high throughput interconnects that follow a specific network topology, for example infiniband or ethernet cables. And hence it is called as a Cluster.  Communication between these nodes takes place with the help of message passing protocols implemented as a software library. For example Open Source MPI - Message Passing Interface library for inter node communications, or Proprietary NCCL library for communication between Nvidia GPUs across nodes.  Usually these nodes are busy running programs at high workloads, in order to run your hello.lua script on one of these (or across many) nodes, you will have to submit a job request which gets queued and scheduled on the compute node(s) based on priority and availability of resources.  To do so, we use a Job Scheduler, Or also called a workload manager, that manages submitted jobs by user(s).  Greene makes use of an Open Source workload manager called SLURM which stands for &quot;Simple Linux Utility for Resource Management&quot;.  Make sure that you have loaded the lua module before proceeding:  module load lua/5.3.6   To run your hello.lua on a compute node we use the srun command as shown below:  srun lua hello.lua   [pp2959@log-2 ~]$ srun lua hello.lua srun: job 55744835 queued and waiting for resources srun: job 55744835 has been allocated resources cm001.hpc.nyu.edu hello, world [pp2959@log-2 ~]$   info Read the Output carefully This job is given an id that is 55744835, this is called a job id.The job job 55744835 is queued and waiting to be scheduled on a compute node, since these nodes are expected to be busy based on demand, it may take some time for your job to be scheduledOnce the job gets scheduled, your program lua hello.lua gets run on a chosen compute node(s) and the program's output is printed back to your consoleBased on your output, you may notice the name of the compute node that this program runs on, the node cm001.hpc.nyu.ed in this example is a CPU only node, you may notice a different node. You can find more details about the [specific nodes here].  Now how do we determine Or specify the amount of resources needed to run our hello.lua script ?  By defualt slurm schedules just 1 CPU and 1 GB memory to run your programs.  In order to get sufficient resources, you will need to request them to SLURM by passing the appropriate options with srun command as shown below:  srun --cpus-per-task=4 --mem=8GB lua hello.lua   [pp2959@log-2 ~]$ srun --cpus-per-task=4 --mem=8GB lua hello.lua srun: job 55744916 queued and waiting for resources srun: job 55744916 has been allocated resources hello, world [pp2959@log-2 ~]$   This will send in a job request for 4 cores and 8 GB memory to SLURMSlurm will queue this job request along with many other job requests submitted by users across the clusterThen it will lookup for a compute node that has sufficient resources pertaining to your jobOnce found, it reserves the resources and schedules your job on this particular compute nodeYour job, in this case the command lua hello.lua runs independently on the compute node, unless either explicitly canceled by invoking scancel command (which you will learn next) OR your program errors out  We can check the status of our submitted jobs by using the squeue command.  To do so open a new second terminal and ssh to greene.hpc.nyu.edu.  In the first terminal Submit a job that executes linux sleep command as shown below, ( make sure you have logged in to greene.hpc.nyu.edu at your second terminal before running the below command ):  srun --cpus-per-task=4 --mem=8GB /bin/bash -c &quot;echo 'sleep 120s' ; sleep 120&quot;   In this Script: We are executing a bash script echo 'sleep 120s' ; sleep 120 where echo prints the strings sleep 120s followed by ;, indicating a next command, a second command : sleep 120 to sleep for 120 seconds. All executed within a bash shell  Then in the second terminal, execute squeue command as:  squeue --me   You should see an output like the one below:  [pp2959@log-2 ~]$ squeue --me JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 55747638 short bash pp2959 R 0:02 1 cm002 [pp2959@log-2 ~]$   Running squeue will print the statuses of all jobs submitted by all users on the clusterRunning squeue --me will print only the jobs submitted by youRunning squeue -u &lt;Net_ID&gt; will print out the jobs submitted by a particular userAnd running squeue --job &lt;Job_ID&gt; will print the status of a particular job given it's job idTry squeue --help for more options  Again, submit a new job this time to execute the sleep for 5 mins or 300 seconds:  srun --cpus-per-task=4 --mem=8GB /bin/bash -c &quot;echo sleep 300s; sleep 300&quot;   Copy the job id that you get. And check it's status with squeue as:  squeue --job &lt;Job_ID&gt;   Then in the second terminal execute scancel with your job id as:  scancel &lt;Job_ID&gt;   Replacing &lt;Job_ID&gt; above with the actual job id  This cancels your job either queued or already scheduled on a compute node.  scancel &lt;Job_ID&gt; cancels a particular job based on the job idscancel --me cancels all of your jobs  To run jobs with a gpu use the gres option:  srun --cpus-per-task=4 --mem=8GB --gres=gpu:1 lua hello.lua   --gres=gpu:1 to request one gpu of any type--gres=gpu:v100:1 to request one v100 gpu specifically  Now note the following carefully:  Most of the time your jobs are queued and may never be scheduled because of demand and competition for resources. Therefore, it is crucial in understanding how SLURM schedules jobs so that you may properly craft job requests that get scheduled faster, for this you will need to consider two things:  FIRST: Jobs are scheduled based on priority, higher priority jobs are scheduled first before lower priority jobs.  SECOND: However, Backfill Scheduling overrides priority:  Backfill scheduling is a technique that considers 2 things, a job's resource requirements and it's expected lifetime. Based on these 2 factors, a low priority job that would require less compute and is expected to run for a short time may get scheduled before a high priority job waiting in queue inorder to backfill gaps in compute pools on a regular basis.  Therefore, it is crucial to be thoughtful, by requesting only the necessary compute resources to run your programs and specifying a reasonable lifetime that your job is expected to last.  Thus, it is important to include the --time option for every job that you submit, for example --time=00:40:00 specifies that your job may last for 40 minutes max. --time follows the format HH:MM:SS :  srun --cpus-per-task=4 --mem=8GB --gres=gpu:1 --time=00:02:00 lua hello.lua   check srun --help for more options:  srun --help   So far we have seen on &quot;how to submit jobs&quot; for a single node, we can even submit jobs for multiple nodes, or also called tasks.  We can ask slurm to schedule multiple tasks to run our programs concurrently. For example consider we require 2 tasks: 'Task A that does work A' and 'Task B that does work B' where, both of these tasks can be done independently and simultaneously. They do not depend on eachother. For example, consider a simple modification for our hello.lua script below:  local hostname = io.popen('hostname'):read() local task = 0 if task == 0 then print(hostname .. &quot; (Task A): hello, world&quot;) end if task == 1 then print(hostname .. &quot; (Task B): hello, world&quot;) end   Modify your current hello.lua as above and run it as:  lua hello.lua   Observe the code, we extract name of the host which this program runs on by executing the command hostname within the lua script, using lua's io.popen() method which returns the exectued command's outputs as a file ( stdout file in linux ). We read this file with :read() method to get the contents as string in this case the host's name as a string.  The output should look like the one below:  [pp2959@log-1 ~]$ lua hello.lua log-1 (Task A): hello, world [pp2959@log-1 ~]$   We observe that, with the task variable set to 0 in the script, we end up executing the task of printing the message (Task A): hello, world on log-1, in this example.  Similarly if we set the task variable to 1 then we end up printing the message (Task B): hello, world as shown below:  [pp2959@log-2 ~]$ cat hello.lua local hostname = io.popen('hostname'):read() local task = 1 if task == 0 then print(hostname .. &quot; (Task A): hello, world&quot;) end if task == 1 then print(hostname .. &quot; (Task B): hello, world&quot;) end [pp2959@log-2 ~]$ lua hello.lua log-2 (Task B): hello, world [pp2959@log-2 ~]$   Carefully notice how both of the tasks are independent and simultaneously executable.  So the question would be &quot;how can we execute both of these tasks simultaneously&quot; in a single job submission making use of sufficient resources  To do so we can specify the option --tasks in our srun command like:  srun --tasks=2 --cpus-per-task=4 --mem=4GB --time=05:00 lua hello.lua   And the output may look like below:  [pp2959@log-2 ~]$ cat hello.lua local hostname = io.popen('hostname'):read() local task = 1 if task == 0 then print(hostname .. &quot; (Task A): hello, world&quot;) end if task == 1 then print(hostname .. &quot; (Task B): hello, world&quot;) end [pp2959@log-2 ~]$ srun --tasks=2 --cpus-per-task=4 --mem=4GB --time=05:00 lua hello.lua srun: job 55792604 queued and waiting for resources srun: job 55792604 has been allocated resources cm004.hpc.nyu.edu (Task B): hello, world cm004.hpc.nyu.edu (Task B): hello, world [pp2959@log-2 ~]$   Notice that the task variable is set to 1 in the above example  Based on the outputs you can observe that we ran our program twice, this is because we specified for 2 tasks, where slurm schedules 4 CPUs and 4GB of memory in total and distributes 2 CPUs per task (tasks share the memory pool).  Usually tasks are run either on the same node, or on different nodes depending on the availability of resources. In this example, both the tasks ran on a same compute node that is cm004.hpc.nyu.edu.  If you would like to explicitly run tasks on different nodes then you may use the --nodes option as:  srun --nodes=2 --cpus-per-task=4 --mem=4GB --time=05:00 lua hello.lua   [pp2959@log-2 ~]$ srun --nodes=2 --cpus-per-task=4 --mem=4GB --time=05:00 lua hello.lua srun: job 55792637 queued and waiting for resources srun: job 55792637 has been allocated resources cm010.hpc.nyu.edu (Task B): hello, world cm011.hpc.nyu.edu (Task B): hello, world [pp2959@log-2 ~]$   Note: Two different nodes are utilized in the example above, cm010.hpc.nyu.edu and cm011.hpc.nyu.edu.  Now, we know that our lua script can be executed simultaneously, then how do we execute two different independent tasks like the tasks of printing 2 different 'hello, world' messages ?  We can do so with the help of slurm environment variables, specifically the variable SLURM_PROCID, short of slurm process id.  For example execute the tasks again, this time print the SLURM_PROCID env variable:  srun --tasks=2 --cpus-per-task=4 --mem=4GB --time=05:00 printenv SLURM_PROCID   In this job we are executing printenv command to print the value of SLURM_PROCID environment variable  And the output should look something like this:  [pp2959@log-1 ~]$ srun --tasks=2 --cpus-per-task=4 --mem=4GB --time=05:00 printenv SLURM_PROCID srun: job 55768908 queued and waiting for resources srun: job 55768908 has been allocated resources 1 0 [pp2959@log-1 ~]$   Observe how the env variable SLURM_PROCID is different for both the tasks  This way you can distinguish tasks within a task. And therefore let us modify the hello.lua script to read from env variables as shown below :  local hostname = io.popen('hostname'):read() local task = tonumber(os.getenv(&quot;SLURM_PROCID&quot;)) if task == 0 then print(hostname .. &quot; (Task A): hello, world&quot;) end if task == 1 then print(hostname .. &quot; (Task B): hello, world&quot;) end   In this modified script we read the env variable SLURM_PROCID as a string (by default) and convert it to a number with tonumber() method  And test it without setting any env variables:  lua hello.lua   We should get the expected behavior as shown:  [pp2959@log-1 ~]$ lua hello.lua [pp2959@log-1 ~]$   No message is printed above because we have not set the SLURM_PROCID env variable yet and so the task variable within the lua script is nil (NULL value), SLURM sets this variable accordingly once we submit our job.  Now, let us submit a job with 2 tasks:  srun --tasks=2 --cpus-per-task=4 --mem=4GB --time=05:00 lua hello.lua   You should get an output like below:  [pp2959@log-2 ~]$ srun --tasks=2 --cpus-per-task=4 --mem=4GB --time=05:00 lua hello.lua srun: job 55792659 queued and waiting for resources srun: job 55792659 has been allocated resources cm004.hpc.nyu.edu (Task A): hello, world cm004.hpc.nyu.edu (Task B): hello, world [pp2959@log-2 ~]$   We successfully ran the same script twice simultaneously that performs two different independent tasks based on a task id or in this case SLURM_PROCID.  Slurm offers many environment variables to work with, you can find the full list of slurm environment variables at the slurm documentation page.  To explicitly perform tasks across two different nodes replace --tasks with --nodes as:  srun --nodes=2 --cpus-per-task=4 --mem=4GB --time=05:00 lua hello.lua   [pp2959@log-2 ~]$ srun --nodes=2 --cpus-per-task=4 --mem=4GB --time=05:00 lua hello.lua srun: job 55792666 queued and waiting for resources srun: job 55792666 has been allocated resources cm028.hpc.nyu.edu (Task A): hello, world cm029.hpc.nyu.edu (Task B): hello, world [pp2959@log-2 ~]$   And notice how the tasks are performed on two separate nodes from the hostnames  You can even perform multiple tasks per node with the option --tasks-per-node along with --nodes for example:  srun --nodes=2 --tasks-per-node=1 --cpus-per-task=4 --mem=4GB --time=05:00 lua hello.lua   [pp2959@log-2 ~]$ srun --nodes=2 --tasks-per-node=1 --cpus-per-task=4 --mem=4GB --time=05:00 lua hello.lua srun: job 55792708 queued and waiting for resources srun: job 55792708 has been allocated resources cm043.hpc.nyu.edu (Task B): hello, world cm042.hpc.nyu.edu (Task A): hello, world [pp2959@log-2 ~]$   Also, for debugging purposes it is recommended to use the --label option as:  srun --label --nodes=1 --tasks-per-node=2 --cpus-per-task=4 --mem=4GB --time=05:00 lua hello.lua   This will prepend the task id label with your program's outputs as shown below:  [pp2959@log-2 ~]$ srun --label --nodes=1 --tasks-per-node=2 --cpus-per-task=4 --mem=4GB --time=05:00 lua hello.lua srun: job 56142474 queued and waiting for resources srun: job 56142474 has been allocated resources 1: cm025.hpc.nyu.edu (Task B): hello, world 0: cm025.hpc.nyu.edu (Task A): hello, world [pp2959@log-2 ~]$   NOTE: In the above example, both the tasks 0 and 1 ran simultaneously and their outputs are line buffered, meaning whichever task prints a line first it's output is displayed first. For example outputs from task 1 may get printed first before task 0 during their concurrent execution and hence we see task 1's output first in the above example. You cannot expect 'output lines' from 'concurrently executing tasks' to be printed in any order. Lines are printed in any arbitary order depending on whichever task prints first. Therefore, it is recommended to use the --label option for keeping track of which lines in the output belongs to which tasks during their concurrent execution. --label labels standard output of tasks based on task ID from 0 to N.  So far we understood that slurm chooses 'on which nodes our programs should run on' based on it's scheduling decisions, however it also provides more control like specifying explicitly on which partition we can run our programs on.  Here partitions are similar nodes grouped together as a list. For example H100 nodes are grouped together as a partition named H100_Partition. Whenever we submit a job request for H100s then nodes sequentially along this partition are reserved and our job is scheduled on them.  You can check the list of all partitions and their compute node list with the sinfo command. This will provide you with more information about the partitions, and their statuses:  sinfo   To specify a particular partition, you can use the --partition option as shown below:  srun --partition=cs --nodes=2 --tasks-per-node=1 --cpus-per-task=4 --mem=4GB --time=05:00 lua hello.lua   (A) SLURM OVERVIEW Users submit jobs on the cluster. Slurm ( or also called slurm controller ) that runs exclusively on it's own node, queues up these submitted jobs based on priority and schedules them across compute nodes based on the jobs' compute requirements and expected execution time (Priority, and Backfill scheduling). Once a job has been scheduled on a compute node(s) it runs without interruption. The slurm controller continously monitors the job's status throughout it's life cycle and manages a database ( i.e. MySQL ) where it temporarily maintains the status of all running jobs across the cluster. Whenever users make a slurm query such as the squeue command, to check on the status of their jobs ( or anyother slurm commands ). Such commands invoke a Remote Procedure Call (RPC for short) to the slurm controller, that fetches the job's status from it's database for the user. Too many RPCs to the slurm controller in a short span of time may result in overloading of operations on the slurm's database. Resulting in slurm's poor performance ( RPCs are usually not rate limited for various reasons ) Hence it is recommended to takecare Or limit invoking slurm commands very frequently in case of invoking them within a bash script or a python script Failing to follow may result in the user account being suspended  (B) IMPORTANT NOTE ! It is crucial to understand everything until this point, this builds your foundations in understanding further topics covered from this point onwards. Please make sure to cover all the topics until this point in case you may have missed anything. It gets easier from here.  RECAP: So far we have learn what compute clusters are ...How srun works ...squeue ...scancel ...And more ...  ","version":"Next","tagName":"h2"},{"title":"Submitting Batch jobs​","type":1,"pageTitle":"HPC Foundations","url":"/rts-docs-dev/docs/hpc/tutorials/hpc_foundations/#submitting-batch-jobs","content":" Previously we have seen how we could submit individual interactive jobs mostly to run individual programs, however there is an issue with this method :  What happens if we get disconnected from our ssh session while running our jobs ? To understand this we need to understand how ssh sessions and bash shells are setup in our case First, when we ssh to greene.hpc.nyu.edu, we land on a login node running a bash shell, the console is our shell where we execute the linux commands Then when we submit a job with srun, our program runs within a new bash sub-shell belonging to this particular srun within which slurm sets the necessary environment variables accordingly, like the SLURM_PROCID environment variable as we have seen before Therefore, &quot;hello, world&quot; output(s) printed by this program executing on compute node(s) are buffered all the way from their sub-shell(s) to our bash shell running on the login node, and are displayed line after line on console Hence, if our ssh gets disconnected for any reason, the current bash shell is destroyed, and the job currently being executed within this sub-shell is cancelled  Therefore, we make use of slurm batch scripts also called sbatch instead of interactive jobs. Basically they are simple bash scripts with special directives that we submit to slurm instead of running them interactively.  Within a sbatch script we either specify a single job by invoking srun or batch multiple jobs by invoking multiple srun and submit it to slurm under a single job id hence called a batch job.  Once we submit a batch job, they are independently scheduled regardless of what happens to our shell. We can safely disconnect from our ssh session, and return later on to check on the status of our submitted batch job.  NOTE: Submitting Batch jobs is the preferred way of submitting jobs to slurm  A simple batch job can be written as :  #!/bin/bash #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=1GB #SBATCH --time=00:05:00 srun /bin/bash -c &quot;sleep 60; echo 'hello, world' &quot;   As you can see, we provide the familiar slurm options in a format that is #SBATCH, these are called slurm directives in our bash script.  Create a batch script like the above named hello.sbatch and submit it using the sbatch command:  sbatch hello.sbatch   Check the status of this job with:  squeue --me   Once done, notice that in the same directory from where you submitted this job, there is a new file created slurm-55815161.out, where the number 55815161 is the job id in this example.  Check the contents of this file:  cat slurm-&lt;Job_ID&gt;.out   [pp2959@log-1 slurm_hello_world]$ cat slurm-55815161.out hello, world [pp2959@log-1 slurm_hello_world]$   This is the output of your job. A new file is created by default named slurm-&lt;Job_ID&gt;.out and the outputs are written to it.  You can write the outputs to a custom file name for example hello.out using the directive #SBATCH --output=hello.out. Add this directive to your hello.sbatch file as shown below:  #!/bin/bash #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=1GB #SBATCH --time=00:05:00 #SBATCH --output=hello.out srun /bin/bash -c &quot;echo 'hello, world' &quot;   And re-submit your batch job:  sbatch hello.sbatch   You should notice a new file hello.out gets created, and your hello, world message output is redirected to this file.  [pp2959@log-1 slurm_hello_world]$ cat hello.out hello, world [pp2959@log-1 slurm_hello_world]$   By default error messages that gets generated by your programs are redirected to the same output file, but you can also specify an exclusive file just for writing error messages at, using the directive #SBATCH --error=hello.err in this example.  Modify hello.sbatch to include this directive and also a modified program that prints hello, world then throws an error with exit code 1:  #!/bin/bash #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=1GB #SBATCH --output=hello.out #SBATCH --error=hello.err srun /bin/bash -c &quot;echo 'hello, world'; exit 1&quot;   Submit this job:  sbatch hello.sbatch   Once done check both, output and error outputs of your program:  [pp2959@log-3 slurm_hello_world]$ cat hello.err srun: error: cm013: task 0: Exited with exit code 1 srun: Terminating StepId=55815589.0 [pp2959@log-3 slurm_hello_world]$ cat hello.out hello, world [pp2959@log-3 slurm_hello_world]$   The error messages are redirected to a seperate file hello.err  In this example the error message states as follows,  In the first line, slurm tells us that the program running on host cm013, which is a compute node, with task 0, for this particular srun exited with an error message of exit code 1, since we used exit 1 in our bash script. You may use any error codes from 1 to 255 for debugging purposes, where code 0 is to exit with no errors.Also we have a StepId in this error message as StepId=55815589.0. Here this particular srun is assigned a step id of 0.Invoking a srun is also called a job step in a batch job.  We can invoke multiple job steps within our batch job as shown below:  #!/bin/bash #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=1GB #SBATCH --output=hello.out #SBATCH --error=hello.err srun --time=02:00 /bin/bash -c &quot;echo '(step 0): hello, world'; &quot; srun --time=02:00 /bin/bash -c &quot;echo '(step 1): hello, world'; &quot;   Every srun declared in the batch script is called a job step that will get it's own step id from 0 to N.  Modify hello.sbatch file with the above code and submit the batch job:  sbatch hello.sbatch   Now instead of squeue, you can check the status of your batch jobs with the command sacct or also known as slurm accounting, this is a much easier method of observing your batch jobs than using squeue.  Once the job is done, check it's history with slurm accounting as:  sacct --jobs &lt;Batch_Job_ID&gt;   And the output should look like this:  [pp2959@log-1 ~]$ sacct -jobs 55879998 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 55879998 hello.sba+ short users 1 COMPLETED 0:0 55879998.ba+ batch users 1 COMPLETED 0:0 55879998.ex+ extern users 1 COMPLETED 0:0 55879998.0 bash users 1 COMPLETED 0:0 55879998.1 bash users 1 COMPLETED 0:0 [pp2959@log-1 ~]$   Let's disect the output,  Every row is a timeline of your batch job's execution each step at a time.Observe that the first step is the submission of your batch script named hello.sbatch to slurm as indicated in JobName column as hello.sba+, here + indicates more letters.Also notice that the short partition is selected for this job as indicated at Partition column.The second row, or next step is the resource allocation step for this particular batch job also called batch step given a Job ID of 55879998.batch, seen as 55879998.ba+ at JobID column.Third row is an external step that accounts for all resource usage by this job given a Job ID of 55879998.extern or 55879998.ex+.And the subsequent steps are the normal steps created when srun is invoked within the script in the format as &lt;Job_ID&gt;.&lt;Step_ID&gt;, in this example 55879998.0 and 55879998.1.Do observe how normal steps have their own State and ExitCodes columns. The State of these two steps is COMPLETED and their exit codes are 0:0 which means they completed without any errors. So for example if one of the steps say step 0 exits because of an error, then it's State will change to Failed and it's ExitCode will be displayed there.  Let's consider the below example:  #!/bin/bash #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 #SBATCH --mem=1GB #SBATCH --output=hello.out #SBATCH --error=hello.err srun --time=02:00 /bin/bash -c &quot;echo '(step 0): hello, world'; exit 1 &quot; srun --time=02:00 /bin/bash -c &quot;echo '(step 1): hello, world'; &quot;   Modify the hello.sbatch script with the above code and submit the job, once done check it's accounting with sacct:  [pp2959@log-1 slurm_hello_world]$ sacct -j 55880839 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 55880839 hello.sba+ short users 1 COMPLETED 0:0 55880839.ba+ batch users 1 COMPLETED 0:0 55880839.ex+ extern users 1 COMPLETED 0:0 55880839.0 bash users 1 FAILED 1:0 55880839.1 bash users 1 COMPLETED 0:0 [pp2959@log-1 slurm_hello_world]$   Observe the state of each step. In this example the following &quot;Batch job submission step&quot; (55880839), &quot;resource allocation step&quot; (55880839.ba+), and &quot;batch script execution step&quot; (55880839.ex+) are COMPLETED successfully based on their State columns, but the first &quot;normal step&quot; (55880839.0) FAILED with exit code 1, whereas the second &quot;normal step&quot; (55880839.1) COMPLETED successfully.  You can verify this from your output files hello.out and hello.err:  cat hello.out hello.err   [pp2959@log-1 slurm_hello_world]$ cat hello.out hello.err (step 0): hello, world (step 1): hello, world srun: error: cm009: task 0: Exited with exit code 1 srun: Terminating StepId=55880839.0 [pp2959@log-1 slurm_hello_world]$   We can even control how we distribute resources among these steps by passing options to srun as usual.  For example let's allocate a pool of 4 CPUs and 8 GB memory for a batch job, then distribute just 2 CPUs and 4 GB memory from this pool to our first step, step 0:  #!/bin/bash #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=4 #SBATCH --mem=8GB #SBATCH --time=02:00 #SBATCH --output=hello.out #SBATCH --error=hello.err echo &quot;number of CPUs: $(nproc)&quot; srun --cpus-per-task=2 --mem=4GB --time=02:00 /bin/bash -c ' echo &quot;(step 0) number of CPUs: $(nproc)&quot;; sleep 60' srun /bin/bash -c ' echo &quot;(step 1) number of CPUs: $(nproc)&quot;; sleep 60'   Modify hello.sbatch script with the above code and submit a batch job. Once the job is done, check the outputs:  [pp2959@log-3 slurm_hello_world]$ cat hello.out number of CPUs: 4 (step 0) number of CPUs: 2 (step 1) number of CPUs: 4 [pp2959@log-3 slurm_hello_world]$   As you can see, we are able to control the resources allocated for a step. In this case we distributed just 2 CPUs from the overall pool of 4 CPUs to our first step that is step 0.We did not mention any options to srun in our second step therefore by default step 1 inherits all resources during it's execution.Since we used the sleep command to simulate the execution time for each step, let us check their execution times with sacct by using the --format option, run:  sacct --job &lt;Job_ID&gt; --format=JobID,JobName,State,AllocCPUS,Elapsed   Here --format=JobID,JobName,State,AllocCPUS,Elapsed will show only these columns in the sacct output for the job --job &lt;Job_ID&gt;  [pp2959@log-3 slurm_hello_world]$ sacct --format=JobID,JobName,State,AllocCPUS,Elapsed,Start,End --job 56061185 JobID JobName State AllocCPUS Elapsed Start End ------------ ---------- ---------- ---------- ---------- ------------------- ------------------- 56061185 hello.sba+ TIMEOUT 4 00:02:03 2025-01-18T14:40:26 2025-01-18T14:42:29 56061185.ba+ batch COMPLETED 4 00:02:03 2025-01-18T14:40:26 2025-01-18T14:42:29 56061185.ex+ extern COMPLETED 4 00:02:03 2025-01-18T14:40:26 2025-01-18T14:42:29 56061185.0 bash COMPLETED 2 00:01:00 2025-01-18T14:40:27 2025-01-18T14:41:27 56061185.1 bash COMPLETED 4 00:01:02 2025-01-18T14:41:27 2025-01-18T14:42:29 [pp2959@log-3 slurm_hello_world]$   From the Elapsed times (5th column), it took 2:03 minutes for the job to be scheduled after waiting in queue.Then step 0 takes 1:00 minute to execute and complete because of the use of sleep 60 command.And step 1 also takes roughly 1:02 minute to execute and finish because of sleep 60.Notice the Start and End columns of step 0 and step 1. In this example step 1 starts at 14:41:27 only after step 0 completes it's execution at 14:41:27.From this we learn that step 1 starts once the step 0 completes its execution.  We can distribute tasks among these steps within our batch job to execute them simultaneously for example modify hello.sbatch with the below code:  #!/bin/bash #SBATCH --nodes=1 #SBATCH --tasks-per-node=2 #SBATCH --cpus-per-task=2 #SBATCH --mem=8GB #SBATCH --time=04:00 #SBATCH --output=hello.out #SBATCH --error=hello.err srun --ntasks=1 --mem=4GB /bin/bash -c 'echo &quot;(step 0): hello, world&quot;; sleep 60' &amp; srun --ntasks=1 --mem=4GB /bin/bash -c 'echo &quot;(step 1): hello, world&quot;; sleep 60' &amp; wait   Once the job finishes executing, check on it's accounting information with:  sacct --format=JobID,JobName,State,AllocCPUS,Elapsed,Start,End --job &lt;Job_ID&gt;   [pp2959@log-3 slurm_hello_world]$ sacct --format=JobID,JobName,State,AllocCPUS,Elapsed,Start,End --job 56062529 JobID JobName State AllocCPUS Elapsed Start End ------------ ---------- ---------- ---------- ---------- ------------------- ------------------- 56062529 hello.sba+ COMPLETED 4 00:01:02 2025-01-18T15:41:47 2025-01-18T15:42:49 56062529.ba+ batch COMPLETED 4 00:01:02 2025-01-18T15:41:47 2025-01-18T15:42:49 56062529.ex+ extern COMPLETED 4 00:01:02 2025-01-18T15:41:47 2025-01-18T15:42:49 56062529.0 bash COMPLETED 2 00:01:02 2025-01-18T15:41:47 2025-01-18T15:42:49 56062529.1 bash COMPLETED 2 00:01:02 2025-01-18T15:41:47 2025-01-18T15:42:49 [pp2959@log-3 slurm_hello_world]$   From this example above, observe the Start and End times for step 0 and step 1. We see that both steps run concurrently as we asked for 2 tasks using the directive #SBATCH --tasks-per-node=2 and ended up distributing them among our steps with srun option --ntasks=1.  DO NOTE: You need to distribute the compute resource properly by specifying exactly how much tasks, CPUs, memory and GPUs are to be inherited by a job step in order to execute all your steps simultaneously. Since a job step (srun) by default inherits all resource if not specified. Then that step may end up consuming more resource ( like mem ) than required otherwise could have been allocated for other steps. This can cause other job steps to wait until the one that that is currently consuming the resource to finish it's execution and free up those resource (e.g. mem).  Let's run our hello.lua example by submitting a batch job, modify the contents of your previous lua script as:  local hostname = io.popen('hostname'):read() local task = tonumber(os.getenv(&quot;SLURM_PROCID&quot;)) local stepid = os.getenv(&quot;SLURM_STEP_ID&quot;) if task == 0 then print(hostname .. &quot; (Step ID): &quot; .. stepid .. &quot; ;(Task A): hello, world&quot;) end if task == 1 then print(hostname .. &quot; (Step ID): &quot; .. stepid .. &quot; ;(Task B): hello, world&quot;) end   This lua script executes 2 tasks (Task A and B) simultaneously and also prints the job's step ID  Now modify the hello.sbatch script as below:  #!/bin/bash #SBATCH --nodes=2 #SBATCH --tasks-per-node=2 #SBATCH --cpus-per-task=1 #SBATCH --mem=8GB #SBATCH --time=04:00 #SBATCH --output=hello.out #SBATCH --error=hello.err module purge module load lua/5.3.6 srun --ntasks=2 --mem=2GB lua hello.lua &amp; srun --ntasks=2 --mem=2GB lua hello.lua &amp; wait   Once the job is done, check your program's outputs, it should look like the one below:  [pp2959@log-3 slurm_hello_world]$ cat hello.out cm005.hpc.nyu.edu (Step ID): 1 ;(Task A): hello, world cm006.hpc.nyu.edu (Step ID): 1 ;(Task B): hello, world cm006.hpc.nyu.edu (Step ID): 0 ;(Task B): hello, world cm005.hpc.nyu.edu (Step ID): 0 ;(Task A): hello, world [pp2959@log-3 slurm_hello_world]$   From the output, we were able to execute 4 tasks simultaneously, 2 tasks on both different nodes based on the directives #SBATCH --nodes=2; #SBATCH --tasks-per-node=2 for our batch job.Then we were able to utilize these 2 tasks within a job step itself to perform the tasks A and B, of printing &quot;hello, world&quot; twice simultaneously.Hence, we were able to execute in total 4 tasks simultaneously across 2 nodes, each executing a single job step that performs 2 independent tasks A and B simultaneously.  You may verify this from the job's accounting information:  sacct --format=JobID,JobName,State,AllocCPUS,Elapsed,Start,End --job &lt;Job_ID&gt;   [pp2959@log-3 slurm_hello_world]$ sacct --format=JobID,JobName,State,AllocCPUS,Elapsed,Start,End --job 56063037 JobID JobName State AllocCPUS Elapsed Start End ------------ ---------- ---------- ---------- ---------- ------------------- ------------------- 56063037 hello.sba+ COMPLETED 4 00:00:00 2025-01-18T16:43:52 2025-01-18T16:43:52 56063037.ba+ batch COMPLETED 2 00:00:00 2025-01-18T16:43:52 2025-01-18T16:43:52 56063037.ex+ extern COMPLETED 4 00:00:00 2025-01-18T16:43:52 2025-01-18T16:43:52 56063037.0 lua COMPLETED 2 00:00:00 2025-01-18T16:43:52 2025-01-18T16:43:52 56063037.1 lua COMPLETED 2 00:00:00 2025-01-18T16:43:52 2025-01-18T16:43:52 [pp2959@log-3 slurm_hello_world]$   Check Start and End times verify that the job steps have indeed ran concurrently.  ","version":"Next","tagName":"h2"},{"title":"Run jobs interactively with Compute node(s)​","type":1,"pageTitle":"HPC Foundations","url":"/rts-docs-dev/docs/hpc/tutorials/hpc_foundations/#run-jobs-interactively-with-compute-nodes","content":" So far we have seen how one could:  Submit single interactive jobs to slurm using srun aloneSubmit batch jobs to slurm with sbatchNow we will learn on &quot;how to reserve compute resources for interactive workflows&quot; with salloc  Recall how we used options with srun in requesting compute resources to run our programs, we can do the same with salloc command as shown:  salloc --nodes=1 --tasks=2 --cpus-per-task=1 --mem=4GB --time=10:00 /bin/bash   But without providing any programs to run in the arguments, only requesting resources and running a new bash shell as /bin/bash.  The output should look like this:  [pp2959@log-2 ~]$ salloc --nodes=1 --tasks=2 --cpus-per-task=1 --mem=4GB --time=10:00 salloc: Pending job allocation 56149258 salloc: job 56149258 queued and waiting for resources salloc: job 56149258 has been allocated resources salloc: Granted job allocation 56149258 salloc: Nodes cm[036-037] are ready for job bash-5.1$   Read the output carefully, we submitted a salloc job request that generated a job id 56149258, here we just made an allocation request to slurm for the resources.  The request waits in queue and once the resources are available, in this example nodes cm[036-037] with requested CPUs and memory, are allocated and we enter a new console bash-5.1 which is nothing but a new bash sub-shell on our login node.  Verify that we are still on our login node by running:  bash-5.1$ hostname log-2 bash-5.1$   But now, we can interactively submit job steps exactly like we did with our batch scripts that utilizes the currently allocated pool of compute resources, for example run:  srun hostname   bash-5.1$ srun hostname cm037.hpc.nyu.edu cm036.hpc.nyu.edu bash-5.1$   We can limit resources to our job steps exactly like how we did within our batch scripts:  srun --ntasks=1 --cpus-per-task=1 --mem=2GB hostname   bash-5.1$ srun --ntasks=1 --cpus-per-task=1 --mem=2GB hostname cm006.hpc.nyu.edu bash-5.1$   We can even load a lua module and run the lua script as:  module load lua/5.3.6 srun lua hello.lua   bash-5.1$ srun lua hello.lua cm028.hpc.nyu.edu (Task A): hello, world cm028.hpc.nyu.edu (Task B): hello, world bash-5.1$   And finally, you can keep track of all your interactive job steps in real time within this allocation using sacct.  sacct --job &lt;Current_Job_id&gt;   bash-5.1$ sacct --job 56149430 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 56149430 interacti+ short users 2 RUNNING 0:0 56149430.ex+ extern users 2 RUNNING 0:0 56149430.0 lua users 2 COMPLETED 0:0 56149430.1 hostname users 2 COMPLETED 0:0 bash-5.1$   This way salloc can be used to work interactively with compute nodes for development and debugging purposes.  Once done, you can exit and relenquish the resources by running:  exit   Output bash-5.1$ exit exit salloc: Relinquishing job allocation 56149430 salloc: Job allocation 56149430 has been revoked. [pp2959@log-2 ~]$   ","version":"Next","tagName":"h2"},{"title":"Transfer Data with Data Transfer nodes​","type":1,"pageTitle":"HPC Foundations","url":"/rts-docs-dev/docs/hpc/tutorials/hpc_foundations/#transfer-data-with-data-transfer-nodes","content":" ","version":"Next","tagName":"h2"},{"title":"Package Software with Containers​","type":1,"pageTitle":"HPC Foundations","url":"/rts-docs-dev/docs/hpc/tutorials/hpc_foundations/#package-software-with-containers","content":" ","version":"Next","tagName":"h2"},{"title":"Burst priority jobs to Cloud with Burst nodes​","type":1,"pageTitle":"HPC Foundations","url":"/rts-docs-dev/docs/hpc/tutorials/hpc_foundations/#burst-priority-jobs-to-cloud-with-burst-nodes","content":"","version":"Next","tagName":"h2"}],"options":{"id":"default"}}